{"sentences": ["DistMult DistMult @cite_15 is a special case of RESCAL with a diagonal matrix per relation, so the number of parameters of DistMult grows linearly with respect to the embedding dimension, reducing overfitting.", "However, the linear transformation performed on subject entity embedding vectors in DistMult is limited to a stretch.", "Given the equivalence of subject and object entity embeddings for the same entity, third-order binary tensor learned by DistMult is symmetric in the subject and object entity mode and thus DistMult cannot model asymmetric relations."], "label": ["Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively simple but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms all previous state-of-the-art models across standard link prediction datasets. We prove that TuckER is a fully expressive model, deriving the bound on its entity and relation embedding dimensionality for full expressiveness which is several orders of magnitude smaller than the bound of previous state-of-the-art models ComplEx and SimplE. We further show that several previously introduced linear models can be viewed as special cases of TuckER.", "reference": {"@cite_15": {"mid": "1533230146", "abstract": "Abstract: We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (, 2013) and TransE (, 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2 vs. 54.7 by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.", "ref_function": ["background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["ComplEx ComplEx @cite_1 extends DistMult to the complex domain.", "Even though each relation matrix of ComplEx is still diagonal, subject and object entity embeddings for the same entity are no longer equivalent, but complex conjugates, which introduces asymmetry into the tensor decomposition and thus enables ComplEx to model asymmetric relations."], "label": ["Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively simple but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms all previous state-of-the-art models across standard link prediction datasets. We prove that TuckER is a fully expressive model, deriving the bound on its entity and relation embedding dimensionality for full expressiveness which is several orders of magnitude smaller than the bound of previous state-of-the-art models ComplEx and SimplE. We further show that several previously introduced linear models can be viewed as special cases of TuckER.", "reference": {"@cite_1": {"mid": "2963432357", "abstract": "In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.", "ref_function": ["background", "objective", "method", "background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["ConvE ConvE @cite_32 is the first non-linear model that significantly outperformed the preceding linear models.", "In ConvE, a global 2D convolution operation is performed on the subject entity and relation embedding vectors, after they are reshaped to matrices and concatenated.", "The obtained feature maps are flattened, transformed through a fully connected layer, and the inner product is taken with all object entity vectors to generate a score for each triple.", "Whilst results achieved by ConvE are impressive, its reshaping and concatenating of vectors as well as using 2D convolution on word embeddings is unintuitive."], "label": ["Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively simple but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms all previous state-of-the-art models across standard link prediction datasets. We prove that TuckER is a fully expressive model, deriving the bound on its entity and relation embedding dimensionality for full expressiveness which is several orders of magnitude smaller than the bound of previous state-of-the-art models ComplEx and SimplE. We further show that several previously introduced linear models can be viewed as special cases of TuckER.", "reference": {"@cite_32": {"mid": "2964116313", "abstract": "Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models - which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree - which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set - however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets - deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result", "result", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["HypER HypER @cite_9 is a simplified convolutional model, that uses a hypernetwork to generate 1D convolutional filters for each relation, extracting relation-specific features from subject entity embeddings.", "The authors show that convolution is a way of introducing sparsity and parameter tying and that HypER can be understood in terms of tensor factorization up to a non-linearity, thus placing HypER closer to the well established family of factorization models.", "The drawback of HypER is that it sets most elements of the core weight tensor to 0, which amounts to hard regularization, rather than letting the model learn which parameters to use via a soft regularization approach."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Explaining the inadequacies of previous studies"], "target_paper": "Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively simple but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms all previous state-of-the-art models across standard link prediction datasets. We prove that TuckER is a fully expressive model, deriving the bound on its entity and relation embedding dimensionality for full expressiveness which is several orders of magnitude smaller than the bound of previous state-of-the-art models ComplEx and SimplE. We further show that several previously introduced linear models can be viewed as special cases of TuckER.", "reference": {"@cite_9": {"mid": "2888572441", "abstract": "Knowledge graphs are graphical representations of large databases of facts, which typically suffer from incompleteness. Inferring missing relations (links) between entities (nodes) is the task of link prediction. A recent state-of-the-art approach to link prediction, ConvE, implements a convolutional neural network to extract features from concatenated subject and relation vectors. Whilst results are impressive, the method is unintuitive and poorly understood. We propose a hypernetwork architecture that generates simplified relation-specific convolutional filters that (i) outperforms ConvE and all previous approaches across standard datasets; and (ii) can be framed as tensor factorization and thus set within a well established family of factorization models for link prediction. We thus demonstrate that convolution simply offers a convenient computational means of introducing sparsity and parameter tying to find an effective trade-off between non-linear expressiveness and the number of parameters to learn.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Random data augmentation suffers from low efficiency and generating much uncontrolled noise data.", "To overcome these issue, a few methods have been proposed to take dataset distribution into consideration and augment data according to the feedback of the training dataset, which is more effective than a random distribution.", "Cubuk al proposed AutoAugmentation @cite_16 to create a search space of data augmentation policies.", "It can automatically design a specific policy so as to obtain state-of-the-art validation accuracy for target dataset.", "Peng al proposed Adversarial Data Augmentation @cite_43 to jointly optimize data augmentation and deep model.", "They designed an augmentation network to online generate data and improve the robustness of the deep model.", "However, their data specific augmentation designing process is significantly complicated than random augmentation.", "Our attention guided data augmentation is more simple and direct, which can be easily trained end-to-end."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Explaining the method relationship between own work and references"], "target_paper": "Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.", "reference": {"@cite_43": {"mid": "2963468256", "abstract": "Random data augmentation is a critical technique to avoid overfitting in training deep models. Yet, data augmentation and network training are often two isolated processes in most settings, yielding to a suboptimal training. Why not jointly optimize the two? We propose adversarial data augmentation to address this limitation. The key idea is to design a generator (e.g. an augmentation network) that competes against a discriminator (e.g. a target network) by generating hard examples online. The generator explores weaknesses of the discriminator, while the discriminator learns from hard augmentations to achieve better performance. A reward penalty strategy is also proposed for efficient joint training. We investigate human pose estimation and carry out comprehensive ablation studies to validate our method. The results prove that our method can effectively improve state-of-the-art models without additional data effort.", "ref_function": ["background", "background", "background", "objective", "method", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2804047946", "abstract": "Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5 which is 0.4 better than the previous record of 83.1 . On CIFAR-10, we achieve an error rate of 1.5 , which is 0.6 better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.", "ref_function": ["background", "background", "objective", "method", "method", "background", "method", "result", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["To focus on the local features, many methods rely on the annotations of parts location or attribute.", "Part R-CNN @cite_44 extended R-CNN @cite_32 to detect objects and localize their parts under a geometric prior, then predicted a fine-grained category from a pose-normalized representation.", "@cite_33 proposed a feedback-control framework Deep LAC to back-propagate alignment and classification errors to localization; they also proposed a valve linkage function (VLF) to connect the localization and classification modules."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.", "reference": {"@cite_44": {"mid": "56385144", "abstract": "Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2891951760", "abstract": "Fine-grained classification is challenging due to the difficulty of finding discriminative features. Finding those subtle traits that fully characterize the object is not straightforward. To handle this circumstance, we propose a novel self-supervision mechanism to effectively localize informative regions without the need of bounding-box part annotations. Our model, termed NTS-Net for Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a Teacher agent and a Scrutinizer agent. In consideration of intrinsic consistency between informativeness of the regions and their probability being ground-truth class, we design a novel training paradigm, which enables Navigator to detect most informative regions under the guidance from Teacher. After that, the Scrutinizer scrutinizes the proposed regions from Navigator and makes predictions. Our model can be viewed as a multi-agent cooperation, wherein agents benefit from each other, and make progress together. NTS-Net can be trained end-to-end, while provides accurate fine-grained classification predictions as well as highly informative regions during inference. We achieve state-of-the-art performance in extensive benchmark datasets.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Weakly supervised learning is an umbrella term that covers a variety of studies that attempt to construct predictive models by learning with weak supervision @cite_0 , which mainly consists of incomplete, inexact and inaccurate supervision.", "Localizing object or its parts only by image-level annotation belongs to the inexact supervision."], "label": ["General descriptions of the topic", "Reference to current state of knowledge"], "target_paper": "Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.", "reference": {"@cite_0": {"mid": "2746791238", "abstract": "Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.", "ref_function": ["background", "background", "background", "objective"], "cite_purpose": ["background"]}}}
{"sentences": ["Accurately locating the object or its parts only by image-level supervision is very challenging.", "Early work @cite_7 @cite_13 usually generate class-specific localization maps by Global Average Pooling (GAP) @cite_3 .", "The activation area can reflect the location of an object.", "However, training by softmax cross entropy loss usually leads the model to pay attention to the most discriminative location, whose output bounding box just covers part of the object.", "To locate the whole object.", "Singh al @cite_22 randomly hides the patches of input images so as to force the network to find other discriminative parts.", "However, the process is inefficient for the lack of high-level guidance.", "Zhang al proposed Adversarial Complementary Learning (ACoL) @cite_4 approach to discover entire objects by training two adversary complementary classifiers, which can locate different object parts and discover the complementary regions that belong to the same object.", "Nevertheless, there are only two complementary regions in their implementation, which limits accuracy.", "Our attention-guided data augmentation encourages the model to pay attention to multiple object parts, extract more discriminative features and achieve better performance in object localizing."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge", "Explaining the inadequacies of previous studies", "Other functional sentences", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Describing used methods"], "target_paper": "Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.", "reference": {"@cite_4": {"mid": "2964274719", "abstract": "In this work, we propose Adversarial Complementary Learning (ACoL) to automatically localize integral objects of semantic interest with weak supervision. We first mathematically prove that class localization maps can be obtained by directly selecting the class-specific feature maps of the last convolutional layer, which paves a simple way to identify object regions. We then present a simple network architecture including two parallel-classifiers for object localization. Specifically, we leverage one classification branch to dynamically localize some discriminative object regions during the forward pass. Although it is usually responsive to sparse parts of the target objects, this classifier can drive the counterpart classifier to discover new and complementary object regions by erasing its discovered regions from the feature maps. With such an adversarial learning, the two parallel-classifiers are forced to leverage complementary object regions for classification and can finally generate integral object localization together. The merits of ACoL are mainly two-fold: 1) it can be trained in an end-to-end manner; 2) dynamically erasing enables the counterpart classifier to discover complementary object regions more effectively. We demonstrate the superiority of our ACoL approach in a variety of experiments. In particular, the Top-1 localization error rate on the ILSVRC dataset is 45.14 , which is the new state-of-the-art.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2963045696", "abstract": "We propose \u2018Hide-and-Seek\u2019, a weakly-supervised framework that aims to improve object localization in images and action localization in videos. Most existing weakly-supervised methods localize only the most discriminative parts of an object rather than all relevant parts, which leads to suboptimal performance. Our key idea is to hide patches in a training image randomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. Our approach only needs to modify the input image and can work with any network designed for object localization. During testing, we do not need to hide any patches. Our Hide-and-Seek approach obtains superior performance compared to previous methods for weakly-supervised object localization on the ILSVRC dataset. We also demonstrate that our framework can be easily extended to weakly-supervised action localization.", "ref_function": ["objective", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["differences"]}, "@cite_7": {"mid": "2295107390", "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1 top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2962761264", "abstract": "Global covariance pooling in convolutional neural networks has achieved impressive improvement over the classical first-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD), suffering from inefficient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for fast end-to-end training of global covariance pooling networks. At the core of our method is a meta-layer designed with loop-embedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU. Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing counterparts. By finetuning our models pre-trained on ImageNet, we establish state-of-the-art results on three challenging fine-grained benchmarks. The source code and network models will be available at http: www.peihuali.org iSQRT-COV.", "ref_function": ["background", "background", "background", "method", "method", "background", "method", "method", "result", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["This work lies at the intersection of three research topics: (i) matrix factorization, (ii) parallel & distributed and, (iii) federated learning.", "Recently, Alternating Least Squares (ALS) and Stochastic Gradient Descent (SGD) have gained much interest and have become the most popular algorithms for matrix factorization in recommender systems @cite_25 .", "The ALS algorithm allows learning the latent factor matrices by alternating between updates to one factor matrix while holding the other factor matrix fixed.", "Each iteration of an update to the latent factor matrices is referred to as an .", "Although the time complexity per epoch is cubic in the number of factors, numerous studies show the ALS is well suited for parallelization @cite_13 @cite_12 @cite_9 @cite_10 @cite_4 .", "It is not merely a coincidence that ALS is the premiere parallel matrix factorization implementation for CF in Apache Spark ( https: spark.apache.org docs latest mllib-collaborative-filtering.html)."], "label": ["Describing the objective", "General descriptions of the topic", "Reference to current state of knowledge", "Reference to current state of knowledge", "General reference to previous research or scholarship: about results", "Other functional sentences"], "target_paper": "The increasing interest in user privacy is leading to new privacy preserving machine learning paradigms. In the Federated Learning paradigm, a master machine learning model is distributed to user clients, the clients use their locally stored data and model for both inference and calculating model updates. The model updates are sent back and aggregated on the server to update the master model then redistributed to the clients. In this paradigm, the user data never leaves the client, greatly enhancing the user' privacy, in contrast to the traditional paradigm of collecting, storing and processing user data on a backend server beyond the user's control. In this paper we introduce, as far as we are aware, the first federated implementation of a Collaborative Filter. The federated updates to the model are based on a stochastic gradient approach. As a classical case study in machine learning, we explore a personalized recommendation system based on users' implicit feedback and demonstrate the method's applicability to both the MovieLens and an in-house dataset. Empirical validation confirms a collaborative filter can be federated without a loss of accuracy compared to a standard implementation, hence enhancing the user's privacy in a widely used recommender application while maintaining recommender performance.", "reference": {"@cite_13": {"mid": "1511814458", "abstract": "Many recommendation systems suggest items to users by utilizing the techniques of collaborative filtering(CF) based on historical records of items that the users have viewed, purchased, or rated. Two major problems that most CF approaches have to contend with are scalability and sparseness of the user profiles. To tackle these issues, in this paper, we describe a CF algorithm alternating-least-squares with weighted-?-regularization(ALS-WR), which is implemented on a parallel Matlab platform. We show empirically that the performance of ALS-WR (in terms of root mean squared error(RMSE)) monotonically improves with both the number of features and the number of ALS iterations. We applied the ALS-WR algorithm on a large-scale CF problem, the Netflix Challenge, with 1000 hidden features and obtained a RMSE score of 0.8985, which is one of the best results based on a pure method. In addition, combining with the parallel version of other known methods, we achieved a performance improvement of 5.91 over Netflix's own CineMatch recommendation system. Our method is simple and scales well to very large datasets.", "ref_function": ["background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2142466236", "abstract": "Matrix factorization, when the matrix has missing values, has become one of the leading techniques for recommender systems. To handle web-scale datasets with millions of users and billions of ratings, scalability becomes an important issue. Alternating least squares (ALS) and stochastic gradient descent (SGD) are two popular approaches to compute matrix factorization, and there has been a recent flurry of activity to parallelize these algorithms. However, due to the cubic time complexity in the target rank, ALS is not scalable to large-scale datasets. On the other hand, SGD conducts efficient updates but usually suffers from slow convergence that is sensitive to the parameters. Coordinate descent, a classical optimization approach, has been used for many other large-scale problems, but its application to matrix factorization for recommender systems has not been thoroughly explored. In this paper, we show that coordinate descent-based methods have a more efficient update rule compared to ALS and have faster and more stable convergence than SGD. We study different update sequences and propose the CCD++ algorithm, which updates rank-one factors one by one. In addition, CCD++ can be easily parallelized on both multi-core and distributed systems. We empirically show that CCD++ is much faster than ALS and SGD in both settings. As an example, with a synthetic dataset containing 14.6 billion ratings, on a distributed memory cluster with 64 processors, to deliver the desired test RMSE, CCD++ is 49 times faster than SGD and 20 times faster than ALS. When the number of processors is increased to 256, CCD++ takes only 16 s and is still 40 times faster than SGD and 20 times faster than ALS.", "ref_function": ["background", "background", "background", "background", "method", "result", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2029463952", "abstract": "The efficient, distributed factorization of large matrices on clusters of commodity machines is crucial to applying latent factor models in industrial-scale recommender systems. We propose an efficient, data-parallel low-rank matrix factorization with Alternating Least Squares which uses a series of broadcast-joins that can be efficiently executed with MapReduce. We empirically show that the performance of our solution is suitable for real-world use cases. We present experiments on two publicly available datasets and on a synthetic dataset termed Bigflix, generated from the Netflix dataset. Bigflix contains 25 million users and more than 5 billion ratings, mimicking data sizes recently reported as Netflix' production workload. We demonstrate that our approach is able to run an iteration of Alternating Least Squares in six minutes on this dataset. Our implementation has been contributed to the open source machine learning library Apache Mahout.", "ref_function": ["background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2054141820", "abstract": "As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.", "ref_function": ["background"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2113802117", "abstract": "The collaborative filtering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subfield of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netflix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netflix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efficiently. In the experimentation section, we first report on some implementation issues, and we suggest on how parameter optimization can be performed efficiently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets.", "ref_function": ["background", "background", "background", "background", "result", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Federated Learning, on the other hand, a distributed learning paradigm essentially assumes user data is not available on central servers and is private and confidential.", "A prominent direction of research in this domain is based on the weighted averaging of the model parameters @cite_17 @cite_24 .", "In practice, a master machine learning model is distributed to user clients.", "Each client updates the local copy of the model weights using the user's personal data and sends updated weights to the server which uses the weighted average of the clients local model weights to update the master model.", "This federated averaging approach has recently attracted much attention for deep neural networks, however, the same approach may not be applicable to a wide class of other machine learning models such as matrix factorization.", "Classical studies based on federated averaging used CNNs to train on benchmark image recognition tasks @cite_17 , and LSTM on a language modeling tasks @cite_2 @cite_15 .", "As a follow-up analysis on federating deep learning models, numerous studies have been proposed addressing the optimization of the communication payloads, noisy, unbalanced @cite_23 , non-iid and massively distributed data @cite_33 ."], "label": ["Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge", "Reference to current state of knowledge", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: research objective"], "target_paper": "The increasing interest in user privacy is leading to new privacy preserving machine learning paradigms. In the Federated Learning paradigm, a master machine learning model is distributed to user clients, the clients use their locally stored data and model for both inference and calculating model updates. The model updates are sent back and aggregated on the server to update the master model then redistributed to the clients. In this paradigm, the user data never leaves the client, greatly enhancing the user' privacy, in contrast to the traditional paradigm of collecting, storing and processing user data on a backend server beyond the user's control. In this paper we introduce, as far as we are aware, the first federated implementation of a Collaborative Filter. The federated updates to the model are based on a stochastic gradient approach. As a classical case study in machine learning, we explore a personalized recommendation system based on users' implicit feedback and demonstrate the method's applicability to both the MovieLens and an in-house dataset. Empirical validation confirms a collaborative filter can be federated without a loss of accuracy compared to a standard implementation, hence enhancing the user's privacy in a widely used recommender application while maintaining recommender performance.", "reference": {"@cite_33": {"mid": "2807006176", "abstract": "Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55 for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30 for the CIFAR-10 dataset with only 5 globally shared data.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_24": {"mid": "2900594532", "abstract": "Federated learning is an approach to distributed machine learning where a global model is learned by aggregating models that have been trained locally on data-generating clients. Contrary to centralized optimization, clients can be very large in number and face challenges of data and network heterogeneity. Examples of clients include smartphones and connected vehicles, which highlights the practical relevance of federated learning. We benchmark three federated learning algorithms and compare their performance against a centralized approach where data resides on the server. The algorithms Federated Averaging (FedAvg), Federated Stochastic Variance Reduced Gradient, and CO-OP are evaluated on the MNIST dataset, using both i.i.d. and non-i.i.d. partitionings of the data. Our results show that FedAvg achieves the highest accuracy among the federated algorithms, regardless of how data was partitioned. Our comparison between FedAvg and centralized learning shows that they are practically equivalent when i.i.d. data is used. However, the centralized approach outperforms FedAvg with non-i.i.d. data.", "ref_function": ["background", "background", "background", "method", "result", "result", "background", "background", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2900120080", "abstract": "We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the Federated Averaging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2904190483", "abstract": "Federated learning is a distributed form of machine learning where both the training data and model training are decentralized. In this paper, we use federated learning in a commercial, global-scale setting to train, evaluate and deploy a model to improve virtual keyboard search suggestion quality without direct access to the underlying user data. We describe our observations in federated training, compare metrics to live deployments, and present resulting quality increases. In whole, we demonstrate how federated learning can be applied end-to-end to both improve user experiences and enhance user privacy.", "ref_function": ["background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}}}
{"sentences": ["Based on CGANs, @cite_46 have developed a generic framework Pix2pix'', which is suitable for different generative tasks.", "In Pix2pix, one conditional image is adopted as a reference during the training time.", "The generator in Pix2pix is a U-net, which tries to synthesize a fake image conditioned on the given conditional image in order to fool the discriminator, while the discriminator tries to identify the fake image by comparing it with the corresponding target image.", "Under these settings, the discriminator takes the pairs of images as input.", "The U-net is actually an Encoder-Decoder network with skip connection, in which the encoder consists of multiple convolution layers and the decoder consists of multiple deconvolution layers.", "@cite_46 added skip connections between each layer @math and layer @math which allows feature sharing between the encoder and decoder, where @math is the total number of layers.", "All channels at layer @math are simply concatenated with those at layer @math by the skip connections.", "@cite_46 shares a similar goal with us, but it cannot solve the face-to-attributed-sketch translation task since it cannot convert a face image to a sketch conditioned on external facial attributes while our ASGAN is specifically designed to tackle this task."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Facial attributes are important since they provide a detailed description and determine the visual appearance of human faces. In this paper, we aim at converting a face image to a sketch while simultaneously generating facial attributes. To this end, we propose a novel Attribute-Guided Sketch Generative Adversarial Network (ASGAN) which is an end-to-end framework and contains two pairs of generators and discriminators, one of which is used to generate faces with attributes while the other one is employed for image-to-sketch translation. The two generators form a W-shaped network (W-net) and they are trained jointly with a weight-sharing constraint. Additionally, we also propose two novel discriminators, the residual one focusing on attribute generation and the triplex one helping to generate realistic looking sketches. To validate our model, we have created a new large dataset with 8,804 images, named the Attribute Face Photo & Sketch (AFPS) dataset which is the first dataset containing attributes associated to face sketch images. The experimental results demonstrate that the proposed network (i) generates more photo-realistic faces with sharper facial attributes than baselines and (ii) has good generalization capability on different generative tasks.", "reference": {"@cite_46": {"mid": "2963073614", "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["background", "background", "differences"]}}}
{"sentences": ["Some recent research aimed to defend against the attack of adversarial samples and proposed approaches such as defensive distillatione @cite_20 @cite_10 @cite_31 @cite_28 .", "However, experiment results show that these approaches do not perform well in particular situations due to not being able to defend against adversarial samples of high quality @cite_3 ."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about result"], "target_paper": "Neural networks play an increasingly important role in the field of machine learning and are included in many applications in society. Unfortunately, neural networks suffer from adversarial samples generated to attack them. However, most of the generation approaches either assume that the attacker has full knowledge of the neural network model or are limited by the type of attacked model. In this paper, we propose a new approach that generates a black-box attack to neural networks based on the swarm evolutionary algorithm. Benefiting from the improvements in the technology and theoretical characteristics of evolutionary algorithms, our approach has the advantages of effectiveness, black-box attack, generality, and randomness. Our experimental results show that both the MNIST images and the CIFAR-10 images can be perturbed to successful generate a black-box attack with 100 probability on average. In addition, the proposed attack, which is successful on distilled neural networks with almost 100 probability, is resistant to defensive distillation. The experimental results also indicate that the robustness of the artificial intelligence algorithm is related to the complexity of the model and the data set. In addition, we find that the adversarial samples to some extent reproduce the characteristics of the sample data learned by the neural network model.", "reference": {"@cite_28": {"mid": "2950468330", "abstract": "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small \"detector\" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2625220439", "abstract": "Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["motivation"]}, "@cite_31": {"mid": "2594867206", "abstract": "Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93 ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples.", "ref_function": ["background", "background", "objective", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2174868984", "abstract": "Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95 to less than 0.5 on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10^30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800 on one of the DNNs we tested.", "ref_function": ["background", "background", "background", "background", "objective", "objective", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2606529538", "abstract": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The solution of an inverse problem using sample-based priors has a rich history (see @cite_6 @cite_33 for example).", "As does the idea of reducing the dimension of the parameter space by mapping it to a lower-dimensional space @cite_17 @cite_27 .", "However, the use of GANs in these tasks is novel."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies"], "target_paper": "Bayesian inference is used extensively to infer and to quantify the uncertainty in a field of interest from a measurement of a related field when the two are linked by a physical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and or have prior distributions that are difficult to represent mathematically. In this manuscript we consider the use of Generative Adversarial Networks (GANs) in addressing these challenges. A GAN is a type of deep neural network equipped with the ability to learn the distribution implied by multiple samples of a given field. Once trained on these samples, the generator component of a GAN maps the iid components of a low-dimensional latent vector to an approximation of the distribution of the field of interest. In this work we demonstrate how this approximate distribution may be used as a prior in a Bayesian update, and how it addresses the challenges associated with characterizing complex prior distributions and the large dimension of the inferred field. We demonstrate the efficacy of this approach by applying it to the problem of inferring and quantifying uncertainty in the initial temperature field in a heat conduction problem from a noisy measurement of the temperature at later time.", "reference": {"@cite_27": {"mid": "2075385817", "abstract": "A greedy algorithm for the construction of a reduced model with reduction in both parameter and state is developed for an efficient solution of statistical inverse problems governed by partial differential equations with distributed parameters. Large-scale models are too costly to evaluate repeatedly, as is required in the statistical setting. Furthermore, these models often have high-dimensional parametric input spaces, which compounds the difficulty of effectively exploring the uncertainty space. We simultaneously address both challenges by constructing a projection-based reduced model that accepts low-dimensional parameter inputs and whose model evaluations are inexpensive. The associated parameter and state bases are obtained through a greedy procedure that targets the governing equations, model outputs, and prior information. The methodology and results are presented for groundwater inverse problems in one and two dimensions.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2019588162", "abstract": "The construction of suitable preconditioners for the solution of linear systems by iterative methods continues to receive a lot of interest. Traditionally, preconditioners are designed to accelerate convergence of iterative methods to the solution of the linear system. However, when truncated iterative methods are used as regularized solvers of ill-posed problems, the rate of convergence is seldom an issue, and traditional preconditioners are of little use. Here, we present a new approach to the design of preconditioners for ill-posed linear systems, suitable when statistical information about the desired solution or a collection of typical solutions is available. The preconditioners are constructed from the covariance matrix of the solution viewed as a random variable. Since the construction is based on available prior information, these preconditioners are called priorconditioners. A statistical truncation index selection is also presented. Computed examples illustrate how effective such priorconditioners can be.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "1983546683", "abstract": "In this paper, we consider the impedance tomography problem of estimating the conductivity distribution within the body from static current voltage measurements on the body's surface. We present a new method of implementing prior information of the conductivities in the optimization algorithm. The method is based on the approximation of the prior covariance matrix by simulated samples of feasible conductivities. The reduction of the dimensionality of the optimization problem is performed by principal component analysis (PCA).", "ref_function": ["background", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2074686342", "abstract": "We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a spatial or temporal field, endowed with a hierarchical Gaussian process prior. Computational challenges in this construction arise from the need for repeated evaluations of the forward model (e.g., in the context of Markov chain Monte Carlo) and are compounded by high dimensionality of the posterior. We address these challenges by introducing truncated Karhunen-Loeve expansions, based on the prior distribution, to efficiently parameterize the unknown field and to specify a stochastic forward problem whose solution captures that of the deterministic forward model over the support of the prior. We seek a solution of this problem using Galerkin projection on a polynomial chaos basis, and use the solution to construct a reduced-dimensionality surrogate posterior density that is inexpensive to evaluate. We demonstrate the formulation on a transient diffusion equation with prescribed source terms, inferring the spatially-varying diffusivity of the medium from limited and noisy data.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Recently, a number of authors have considered the use machine learning-based methods for solving inverse problems.", "These include the use of convolutional neural networks (CNNs) to solve physics-driven inverse problems @cite_18 @cite_23 @cite_46 , and GANs to solve problems in computer vision @cite_57 @cite_65 @cite_29 @cite_54 @cite_49 @cite_15 @cite_51 @cite_47 .", "There is also a growing body of work dedicated to using GANs to learn regularizers in solving inverse problems @cite_31 and in compressed sensing @cite_32 @cite_58 @cite_19 @cite_13 @cite_30 .", "However, these approaches differs from ours in at least two significant ways.", "First, they solve the inverse problem as an optimization problem and do not rely on Bayesian inference; as a result, regularization is added in an ad-hoc manner, and no attempt is made to quantify the uncertainty in the inferred field.", "Second, the forward map is assumed to satisfy an extension of the restricted isometry property, which may not be the case for forward maps induced by physics-based operators."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the method relationship between own work and references", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "Bayesian inference is used extensively to infer and to quantify the uncertainty in a field of interest from a measurement of a related field when the two are linked by a physical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and or have prior distributions that are difficult to represent mathematically. In this manuscript we consider the use of Generative Adversarial Networks (GANs) in addressing these challenges. A GAN is a type of deep neural network equipped with the ability to learn the distribution implied by multiple samples of a given field. Once trained on these samples, the generator component of a GAN maps the iid components of a low-dimensional latent vector to an approximation of the distribution of the field of interest. In this work we demonstrate how this approximate distribution may be used as a prior in a Bayesian update, and how it addresses the challenges associated with characterizing complex prior distributions and the large dimension of the inferred field. We demonstrate the efficacy of this approach by applying it to the problem of inferring and quantifying uncertainty in the initial temperature field in a heat conduction problem from a noisy measurement of the temperature at later time.", "reference": {"@cite_13": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_30": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2607406448", "abstract": "We propose a partially learned approach for the solution of ill-posed inverse problems with not necessarily linear forward operators. The method builds on ideas from classical regularisation theory and recent advances in deep learning to perform learning while making use of prior information about the inverse problem encoded in the forward operator, noise model and a regularising functional. The method results in a gradient-like iterative scheme, where the 'gradient' component is learned using a convolutional network that includes the gradients of the data discrepancy and regulariser as input in each iteration. We present results of such a partially learned gradient scheme on a non-linear tomographic inversion problem with simulated data from both the Sheep-Logan phantom as well as a head CT. The outcome is compared against filtered backprojection and total variation reconstruction and the proposed method provides a 5.4 dB PSNR improvement over the total variation reconstruction while being significantly faster, giving reconstructions of pixel images in about 0.4 s using a single graphics processing unit (GPU).", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_47": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2963226480", "abstract": "Inverse Problems in medical imaging and computer vision are traditionally solved using purely model-based methods. Among those variational regularization models are one of the most popular approaches. We propose a new framework for applying data-driven approaches to inverse problems, using a neural network as a regularization functional. The network learns to discriminate between the distribution of ground truth images and the distribution of unregularized reconstructions. Once trained, the network is applied to the inverse problem by solving the corresponding variational problem. Unlike other data-based approaches for inverse problems, the algorithm can be applied even if only unsupervised training data is available. Experiments demonstrate the potential of the framework for denoising on the BSDS dataset and for computer tomography reconstruction on the LIDC dataset.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_29": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_54": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_65": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2595294663", "abstract": "The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model G : \u211dk \u2192 \u211dn. Our main theorem is that, if G is L-Lipschitz, then roughly O(k log L) random Gaussian measurements suffice for an l2 l2 recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use 5-10x fewer measurements than Lasso for the same accuracy.", "ref_function": ["objective", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_57": {"mid": "2604885021", "abstract": "While deep learning methods have achieved state-of-theart performance in many challenging inverse problems like image inpainting and super-resolution, they invariably involve problem-specific training of the networks. Under this approach, each inverse problem requires its own dedicated network. In scenarios where we need to solve a wide variety of problems, e.g., on a mobile camera, it is inefficient and expensive to use these problem-specific networks. On the other hand, traditional methods using analytic signal priors can be used to solve any linear inverse problem; this often comes with a performance that is worse than learning-based methods. In this work, we provide a middle ground between the two kinds of methods \u2014 we propose a general framework to train a single deep neural network that solves arbitrary linear inverse problems. We achieve this by training a network that acts as a quasi-projection operator for the set of natural images and show that any linear inverse problem involving natural images can be solved using iterative methods. We empirically show that the proposed framework demonstrates superior performance over traditional methods using wavelet sparsity prior while achieving performance comparable to specially-trained networks on tasks including compressive sensing and pixel-wise inpainting.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2574952845", "abstract": "In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise non-linearity) when the normal operator ( @math , where @math is the adjoint of the forward imaging operator, @math ) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a @math image on the GPU.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_49": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_46": {"mid": "2946534073", "abstract": "Abstract The ability to make decisions based on quantities of interest that depend on variables inferred from measurement finds application in different fields of mechanics and physics. The evaluation of the inferred variables, and hence the quantities of interest, from the measurement typically requires the solution of an inverse problem. For example, in medical imaging the elastic heterogeneity of a tumor and its nonlinear elastic response can be used to distinguish benign tumors from their malignant counterparts. These images of linear and nonlinear elastic parameters of tissue are typically obtained by using a measured displacement field and solving a complex inverse elasticity problem. In this paper we consider circumventing the solution of the inverse problem by using measured displacements as input to a deep convolutional neural network (CNN) and training it to classify tumors on the basis of their elastic heterogeneity and nonlinearity. For a simple, 5-layer CNN trained with 8,000 samples for heterogeneity, and a 4-layer CNN trained with 4,000 samples for nonlinear elasticity we report classification accuracies in the range of 99 . 7 \u2212 99 . 9 . The training and testing data are both obtained from the forward solution of finite element models of samples. We also analyze the weights of the trained model to understand the process through which the network extracts features of elastic moduli from the input displacement images. Finally, we apply the nonlinear elasticity classifier, which is trained entirely using simulated data, to displacement images obtained from ten patients with breast lesions and note that it correctly classifies eight out of ten cases. This application illustrates how data from physics-based models can be used in improving the performance of a data-driven algorithm in data-sparse scenarios.", "ref_function": ["background", "background", "background", "background", "method", "result", "other", "other", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_58": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_51": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["More recently, the approach described in @cite_36 utilizes GANs in a Bayesian setting; however the GAN is trained to approximate the posterior distribution (and not the prior, as in our case), and training is done in a supervised fashion.", "That is, paired samples of the measurement @math and the corresponding true solution @math are required.", "In contrast, our approach is unsupervised, where we require only samples of the true solution @math to train the GAN prior."], "label": ["Explaining the method relationship between own work and references", "Explaining the method relationship between own work and references", "Explaining the method relationship between own work and references"], "target_paper": "Bayesian inference is used extensively to infer and to quantify the uncertainty in a field of interest from a measurement of a related field when the two are linked by a physical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and or have prior distributions that are difficult to represent mathematically. In this manuscript we consider the use of Generative Adversarial Networks (GANs) in addressing these challenges. A GAN is a type of deep neural network equipped with the ability to learn the distribution implied by multiple samples of a given field. Once trained on these samples, the generator component of a GAN maps the iid components of a low-dimensional latent vector to an approximation of the distribution of the field of interest. In this work we demonstrate how this approximate distribution may be used as a prior in a Bayesian update, and how it addresses the challenges associated with characterizing complex prior distributions and the large dimension of the inferred field. We demonstrate the efficacy of this approach by applying it to the problem of inferring and quantifying uncertainty in the initial temperature field in a heat conduction problem from a noisy measurement of the temperature at later time.", "reference": {"@cite_36": {"mid": "2901093491", "abstract": "Characterizing statistical properties of solutions of inverse problems is essential for decision making. Bayesian inversion offers a tractable framework for this purpose, but current approaches are computationally unfeasible for most realistic imaging applications in the clinic. We introduce two novel deep learning based methods for solving large-scale inverse problems using Bayesian inversion: a sampling based method using a WGAN with a novel mini-discriminator and a direct approach that trains a neural network using a novel loss function. The performance of both methods is demonstrated on image reconstruction in ultra low dose 3D helical CT. We compute the posterior mean and standard deviation of the 3D images followed by a hypothesis test to assess whether a \"dark spot\" in the liver of a cancer stricken patient is present. Both methods are computationally efficient and our evaluation shows very promising performance that clearly supports the claim that Bayesian inversion is usable for 3D imaging in time critical applications.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["differences"]}}}
{"sentences": ["Deep hierarchical encoder-decoder architectures are widely and successfully used for many image-based tasks, such as human pose estimation @cite_41 @cite_80 , semantic segmentation @cite_66 @cite_3 @cite_11 @cite_60 @cite_48 @cite_73 @cite_64 @cite_33 @cite_46 @cite_72 , optical flow estimation @cite_53 @cite_49 , and object detection @cite_6 @cite_74 @cite_56 .", "The encoder-decoder architecture, stacked hourglass module, is based on the successive steps of pooling and upsampling, which produces impressive results on human pose estimation @cite_41 .", "@cite_6 introduced the feature pyramid network for object detection.", "As for semantic segmentation tasks, U-Net @cite_66 , SegNet @cite_3 and DeconvNet @cite_72 follow the symmetric encoder-decoder architectures, and they refine the segmentation masks by utilizing features in low-level layers.", "DeepLabv3+ @cite_15 takes advantage of both the encoder-decoder architecture and the atrous convolution modules to effectively change the fields-of-view of filters to capture multi-scale contextual information, which provides new state-of-the-art performance on many semantic segmentation benchmarks.", "that progressively reduces the feature resolution, enlarges the receptive fields of filters and captures higher semantic information; (2) that gradually recovers the spatial information @cite_15 ."], "label": ["General reference to previous research or scholarship: research objective", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Motivated by the success of encoding multi-scale contextual information for image analysis, we propose our PointAtrousGraph (PAG) - a deep permutation-invariant hierarchical encoder-decoder for efficiently exploiting multi-scale edge features in point clouds. Our PAG is constructed by several novel modules, such as Point Atrous Convolution (PAC), Edge-preserved Pooling (EP) and Edge-preserved Unpooling (EU). Similar with atrous convolution, our PAC can effectively enlarge receptive fields of filters and thus densely learn multi-scale point features. Following the idea of non-overlapping max-pooling operations, we propose our EP to preserve critical edge features during subsampling. Correspondingly, our EU modules gradually recover spatial information for edge features. In addition, we introduce chained skip subsampling upsampling modules that directly propagate edge features to the final stage. Particularly, our proposed auxiliary loss functions can further improve our performance. Experimental results show that our PAG outperform previous state-of-the-art methods on various 3D semantic perception applications.", "reference": {"@cite_64": {"mid": "2738804062", "abstract": "Many machine vision applications require predictions for every pixel of the input image (for example semantic segmentation, boundary detection). Models for such problems usually consist of encoders which decreases spatial resolution while learning a high-dimensional representation, followed by decoders who recover the original input resolution and result in low-dimensional predictions. While encoders have been studied rigorously, relatively few studies address the decoder side. Therefore this paper presents an extensive comparison of a variety of decoders for a variety of pixel-wise prediction tasks. Our contributions are: (1) Decoders matter: we observe significant variance in results between different types of decoders on various problems. (2) We introduce a novel decoder: bilinear additive upsampling. (3) We introduce new residual-like connections for decoders. (4) We identify two decoder types which give a consistently high performance.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2963728677", "abstract": "Recent progress in semantic segmentation has been driven by improving the spatial resolution under Fully Convolutional Networks (FCNs). To address this problem, we propose a Stacked Deconvolutional Network (SDN) for semantic segmentation. In SDN, multiple shallow deconvolutional networks, which are called as SDN units, are stacked one by one to integrate contextual information and bring the fine recovery of localization information. Meanwhile, inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network. Besides, hierarchical supervision is applied during the upsampling process of each SDN unit, which enhances the discrimination of feature representations and benefits the network optimization. We carry out comprehensive experiments and achieve the new state-ofthe- art results on four datasets, including PASCAL VOC 2012, CamVid, GATECH, COCO Stuff. In particular, our best model without CRF post-processing achieves an intersection-over-union score of 86.6 in the test set.", "ref_function": ["background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_60": {"mid": "2563705555", "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_41": {"mid": "2307770531", "abstract": "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a \u201cstacked hourglass\u201d network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.", "ref_function": ["objective", "background", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_48": {"mid": "2598666589", "abstract": "One of recent trends [31, 32, 14] in network architecture design is stacking small filters (e.g., 1x1 or 3x3) in the entire network because the stacked small filters is more efficient than a large kernel, given the same computational complexity. However, in the field of semantic segmentation, where we need to perform dense per-pixel prediction, we find that the large kernel (and effective receptive field) plays an important role when we have to perform the classification and localization tasks simultaneously. Following our design principle, we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation. We also suggest a residual-based boundary refinement to further refine the object boundaries. Our approach achieves state-of-art performance on two public benchmarks and significantly outperforms previous results, 82.2 (vs 80.2 ) on PASCAL VOC 2012 dataset and 76.9 (vs 71.8 ) on Cityscapes dataset.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_53": {"mid": "764651262", "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2964309882", "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89 and 82.1 without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: github.com tensorflow models tree master research deeplab.", "ref_function": ["background", "background", "objective", "method", "method", "result", "other"], "cite_purpose": ["background", "background"]}, "@cite_3": {"mid": "2963881378", "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http: mi.eng.cam.ac.uk projects segnet .", "ref_function": ["background", "background", "method", "objective", "method", "method", "method", "result", "background", "background", "background", "objective", "method", "method", "result", "other"], "cite_purpose": ["background", "background"]}, "@cite_6": {"mid": "2565639579", "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_56": {"mid": "2579985080", "abstract": "The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with @math input achieves 81.5 mAP on VOC2007 test, 80.0 mAP on VOC2012 test, and 33.2 mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.", "ref_function": ["objective", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_72": {"mid": "1745334888", "abstract": "We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5 ) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.", "ref_function": ["objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_49": {"mid": "2560474170", "abstract": "The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50 . It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_74": {"mid": "2572745118", "abstract": "In recent years, we have seen tremendous progress in the field of object detection. Most of the recent improvements have been achieved by targeting deeper feedforward networks. However, many hard object categories such as bottle, remote, etc. require representation of fine details and not just coarse, semantic representations. But most of these fine details are lost in the early convolutional layers. What we need is a way to incorporate finer details from lower layers into the detection architecture. Skip connections have been proposed to combine high-level and low-level features, but we argue that selecting the right features from low-level requires top-down contextual information. Inspired by the human visual pathway, in this paper we propose top-down modulations as a way to incorporate fine details into the detection framework. Our approach supplements the standard bottom-up, feedforward ConvNet with a top-down modulation (TDM) network, connected using lateral connections. These connections are responsible for the modulation of lower layer filters, and the top-down network handles the selection and integration of contextual information and low-level features. The proposed TDM architecture provides a significant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16, 35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any bells and whistles (e.g., multi-scale, iterative box refinement, etc.).", "ref_function": ["background", "background", "background", "background", "background", "objective", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_80": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_46": {"mid": "2963815618", "abstract": "Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features is more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0 in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9 mean IoU, which outperforms the previous state-of-the-art results.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_73": {"mid": "2753588254", "abstract": "Effective integration of local and global contextual information is crucial for dense labeling problems. Most existing methods based on an encoder-decoder architecture simply concatenate features from earlier layers to obtain higher-frequency details in the refinement stages. However, there are limits to the quality of refinement possible if ambiguous information is passed forward. In this paper we propose Gated Feedback Refinement Network (G-FRNet), an end-to-end deep learning framework for dense labeling tasks that addresses this limitation of existing methods. Initially, G-FRNet makes a coarse prediction and then it progressively refines the details by efficiently integrating local and global contextual information during the refinement stages. We introduce gate units that control the information passed forward in order to filter out ambiguity. Experiments on three challenging dense labeling datasets (CamVid, PASCAL VOC 2012, and Horse-Cow Parsing) show the effectiveness of our method. Our proposed approach achieves state-of-the-art results on the CamVid and Horse-Cow Parsing datasets, and produces competitive results on the PASCAL VOC 2012 dataset.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_66": {"mid": "1901129140", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http: lmb.informatik.uni-freiburg.de people ronneber u-net .", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "other"], "cite_purpose": ["background", "background"]}, "@cite_11": {"mid": "2557406251", "abstract": "Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8 on the Cityscapes dataset.", "ref_function": ["background", "background", "background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Unorganized point cloud is a simple and straight-forward representation of 3D structures.", "@cite_45 .", "The pioneering work PointNet @cite_45 achieves permutation-invariance by applying symmetric functions.", "Inspired by PointNet, many following works @cite_65 @cite_82 @cite_59 @cite_21 propose more complicated symmetric operations to exploit local geometrical details in 3D points.", "Semantic labeling on point cloud is more challenging than classification and object-part segmentation.", "SPG @cite_0 and SGPN @cite_50 both construct super point graphs to refine their semantic labeling results.", "RSNet @cite_32 introduces a slice pooling layer, a recurrent neural network layer and a slice unpooling layer, which projects unordered point features onto an ordered sequence of feature vectors.", "However, unlike many networks for semantic labeling tasks on images, they @cite_0 @cite_76 @cite_70 do not have hierarchical encoder-decoder architectures."], "label": ["General descriptions of the topic", "Not sure", "Reference to single investigations in the past: about result", "General reference to previous research or scholarship: approaches taken", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Motivated by the success of encoding multi-scale contextual information for image analysis, we propose our PointAtrousGraph (PAG) - a deep permutation-invariant hierarchical encoder-decoder for efficiently exploiting multi-scale edge features in point clouds. Our PAG is constructed by several novel modules, such as Point Atrous Convolution (PAC), Edge-preserved Pooling (EP) and Edge-preserved Unpooling (EU). Similar with atrous convolution, our PAC can effectively enlarge receptive fields of filters and thus densely learn multi-scale point features. Following the idea of non-overlapping max-pooling operations, we propose our EP to preserve critical edge features during subsampling. Correspondingly, our EU modules gradually recover spatial information for edge features. In addition, we introduce chained skip subsampling upsampling modules that directly propagate edge features to the final stage. Particularly, our proposed auxiliary loss functions can further improve our performance. Experimental results show that our PAG outperform previous state-of-the-art methods on various 3D semantic perception applications.", "reference": {"@cite_70": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_21": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_65": {"mid": "2963121255", "abstract": "Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2963517242", "abstract": "Point clouds are an efficient data format for 3D data. However, existing 3D segmentation methods for point clouds either do not model local dependencies [21] or require added computations [14, 23]. This work presents a novel 3D segmentation framework, RSNet1, to efficiently model local structures in point clouds. The key component of the RSNet is a lightweight local dependency module. It is a combination of a novel slice pooling layer, Recurrent Neural Network (RNN) layers, and a slice unpooling layer. The slice pooling layer is designed to project features of unordered points onto an ordered sequence of feature vectors so that traditional end-to-end learning algorithms (RNNs) can be applied. The performance of RSNet is validated by comprehensive experiments on the S3DIS[1], ScanNet[3], and ShapeNet [34] datasets. In its simplest form, RSNets surpass all previous state-of-the-art methods on these benchmarks. And comparisons against previous state-of-the-art methods [21, 23] demonstrate the efficiency of RSNets.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2769473888", "abstract": "We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of large-scale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset).", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_45": {"mid": "2560609797", "abstract": "Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_59": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_50": {"mid": "2769312834", "abstract": "We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_76": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_82": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["With the recent advent of neural networks, a number of single object classification and detection methods with high performance have been proposed @cite_31 @cite_36 @cite_39 .", "Beyond obtaining one feature vector from one image for an object, several multi-object detection techniques from single shot have been developed by introducing new network structures @cite_12 @cite_59 @cite_51 @cite_16 @cite_38 .", "In particular, some of these methods can be applied to various real-time tasks since the whole detection network is composed of single network pipeline."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Summarize the above references"], "target_paper": "We present NOLBO, a variational observation model estimation for 3D multi-object from 2D single shot. Previous probabilistic instance-level understandings mainly consider the single-object image, not single shot with multi-object; relations between objects and the entire scene are out of their focus. The objectness of each observation also hardly join their model. Therefore, we propose a method to approximate the Bayesian observation model of scene-level 3D multi-object understanding. By exploiting variational auto-encoder (VAE), we estimate latent variables from the entire scene, which follow tractable distributions and concurrently imply 3D full shape and pose. To perform object-oriented data association and probabilistic simultaneous localization and mapping (SLAM), our observation models can easily be adopted to probabilistic inference by replacing object-oriented features with latent variables.", "reference": {"@cite_38": {"mid": "2884561390", "abstract": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https: github.com facebookresearch Detectron.", "ref_function": ["background", "background", "objective", "background", "objective", "method", "method", "result", "other"], "cite_purpose": ["background"]}, "@cite_36": {"mid": "2194775991", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57 error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "ref_function": ["background", "background", "method", "result", "result", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_39": {"mid": "1686810756", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", "ref_function": ["background", "background", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_59": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2163605009", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5 and 17.0 which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3 , compared to 26.2 achieved by the second-best entry.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_51": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2613718673", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2 mAP) and 2012 (70.4 mAP) using 300 proposals per image. Code is available at https: github.com ShaoqingRen faster_rcnn.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Various studies have also been conducted to understand the instance-level representation from 2D images such as object shape, orientation or bounding box.", "@cite_4 @cite_2 @cite_55 and @cite_48 estimate the orientation of the object by viewpoint classification with discretized bins.", "In addition, 3D bounding box regression has been carried out to obtain the object location and orientation @cite_49 @cite_6 @cite_10 @cite_40 .", "In order to estimate the distinct 3D shape of objects, @cite_20 aligns the prior shape to a single object image through key point matching and estimates its 3D shape and orientation together.", "@cite_0 estimates the 3D mesh with linear combination of parameterized prior shapes.", "In @cite_23 @cite_37 @cite_17 @cite_41 , they have actively utilized non-linear regression and latent variables of neural networks for 3D reconstruction from 2D."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken"], "target_paper": "We present NOLBO, a variational observation model estimation for 3D multi-object from 2D single shot. Previous probabilistic instance-level understandings mainly consider the single-object image, not single shot with multi-object; relations between objects and the entire scene are out of their focus. The objectness of each observation also hardly join their model. Therefore, we propose a method to approximate the Bayesian observation model of scene-level 3D multi-object understanding. By exploiting variational auto-encoder (VAE), we estimate latent variables from the entire scene, which follow tractable distributions and concurrently imply 3D full shape and pose. To perform object-oriented data association and probabilistic simultaneous localization and mapping (SLAM), our observation models can easily be adopted to probabilistic inference by replacing object-oriented features with latent variables.", "reference": {"@cite_37": {"mid": "2964137676", "abstract": "What is a good vector representation of an object? We believe that it should be generative in 3D, in the sense that it can produce new 3D objects; as well as be predictable from 2D, in the sense that it can be perceived from 2D images. We propose a novel architecture, called the TL-embedding network, to learn an embedding space with these properties. The network consists of two components: (a) an autoencoder that ensures the representation is generative; and (b) a convolutional network that ensures the representation is predictable. This enables tackling a number of tasks including voxel prediction from 2D images and 3D model retrieval. Extensive experimental analysis demonstrates the usefulness and versatility of this embedding.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "1591870335", "abstract": "Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs (Convolutional Neural Networks). We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.", "ref_function": ["background", "background", "method", "method", "objective", "result"], "cite_purpose": ["background"]}, "@cite_41": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_48": {"mid": "2799123546", "abstract": "We present a fast inverse-graphics framework for instance-level 3D scene understanding. We train a deep convolutional network that learns to map image regions to the full 3D shape and pose of all object instances in the image. Our method produces a compact 3D representation of the scene, which can be readily used for applications like autonomous driving. Many traditional 2D vision outputs, like instance segmentations and depth-maps, can be obtained by simply rendering our output 3D scene model. We exploit class-specific shape priors by learning a low dimensional shape-space from collections of CAD models. We present novel representations of shape and pose, that strive towards better 3D equivariance and generalization. In order to exploit rich supervisory signals in the form of 2D annotations like segmentation, we propose a differentiable Render-and-Compare loss that allows 3D shape and pose to be learned with 2D supervision. We evaluate our method on the challenging real-world datasets of Pascal3D+ and KITTI, where we achieve state-of-the-art results.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_55": {"mid": "1949483711", "abstract": "We characterize the problem of pose estimation for rigid objects in terms of determining viewpoint to explain coarse pose and keypoint prediction to capture the finer details. We address both these tasks in two different settings - the constrained setting with known bounding boxes and the more challenging detection setting where the aim is to simultaneously detect and correctly estimate pose of objects. We present Convolutional Neural Network based architectures for these and demonstrate that leveraging viewpoint estimates can substantially improve local appearance based keypoint predictions. In addition to achieving significant improvements over state-of-the-art in the above tasks, we analyze the error modes and effect of object characteristics on performance to guide future efforts towards this goal.", "ref_function": ["background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2768879211", "abstract": "We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task [10] that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by [27, 28] that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm. For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our approach substantially outperforms other recent CNN-based approaches [10, 25] when they are all used without postprocessing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method.", "ref_function": ["background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_0": {"mid": "2773522905", "abstract": "One challenge that remains open in 3D deep learning is how to efficiently represent 3D data to feed deep networks. Recent works have relied on volumetric or point cloud representations, but such approaches suffer from a number of issues such as computational complexity, unordered data, and lack of finer geometry. This paper demonstrates that a mesh representation (i.e. vertices and faces to form polygonal surfaces) is able to capture fine-grained geometry for 3D reconstruction tasks. A mesh however is also unstructured data similar to point clouds. We address this problem by proposing a learning framework to infer the parameters of a compact mesh representation rather than learning from the mesh itself. This compact representation encodes a mesh using free-form deformation and a sparse linear combination of models allowing us to reconstruct 3D meshes from single images. In contrast to prior work, we do not rely on silhouettes and landmarks to perform 3D reconstruction. We evaluate our method on synthetic and real-world datasets with very promising results. Our framework efficiently reconstructs 3D objects in a low-dimensional way while preserving its important geometrical aspects.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_40": {"mid": "2560544142", "abstract": "We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].", "ref_function": ["method", "method", "method", "method", "method", "result", "method", "result"], "cite_purpose": ["uses"]}, "@cite_49": {"mid": "2229637417", "abstract": "We focus on the task of amodal 3D object detection in RGB-D images, which aims to produce a 3D bounding box of an object in metric form at its full extent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a 3D volumetric scene from a RGB-D image as input and outputs 3D object bounding boxes. In our approach, we propose the first 3D Region Proposal Network (RPN) to learn objectness from geometric shapes and the first joint Object Recognition Network (ORN) to extract geometric features in 3D and color features in 2D. In particular, we handle objects of various sizes by training an amodal RPN at two different scales and an ORN to regress 3D bounding boxes. Experiments show that our algorithm outperforms the state-of-the-art by 13.8 in mAP and is 200\u00d7 faster than the original Sliding Shapes.", "ref_function": ["objective", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_2": {"mid": "2518803647", "abstract": "Convolutional Neural Networks (CNNs) were recently shown to provide state-of-the- art results for object category viewpoint estimation. However different ways of formulat- ing this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact perfor- mance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of ap- proximately 5 mAVP over previous state-of-the-art results on the Pascal3D+ dataset [28]. In particular for their most challenging 24 view classification task we improve the results from 31.1 to 36.1 mAVP.", "ref_function": ["background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2546066744", "abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2600447016", "abstract": "This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instance- and class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.", "ref_function": ["objective", "method", "method", "method", "result", "result"], "cite_purpose": ["uses"]}, "@cite_20": {"mid": "1946609740", "abstract": "Despite the great progress achieved in recognizing objects as 2D bounding boxes in images, it is still very challenging to detect occluded objects and estimate the 3D properties of multiple objects from a single image. In this paper, we propose a novel object representation, 3D Voxel Pattern (3DVP), that jointly encodes the key properties of objects including appearance, 3D shape, viewpoint, occlusion and truncation. We discover 3DVPs in a data-driven way, and train a bank of specialized detectors for a dictionary of 3DVPs. The 3DVP detectors are capable of detecting objects with specific visibility patterns and transferring the meta-data from the 3DVPs to the detected objects, such as 2D segmentation mask, 3D pose as well as occlusion or truncation boundaries. The transferred meta-data allows us to infer the occlusion relationship among objects, which in turn provides improved object recognition results. Experiments are conducted on the KITTI detection benchmark [17] and the outdoor-scene dataset [41]. We improve state-of-the-art results on car detection and pose estimation with notable margins (6 in difficult data of KITTI). We also verify the ability of our method in accurately segmenting objects from the background and localizing them in 3D.", "ref_function": ["background", "objective", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2963735494", "abstract": "In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at: https: github.com Yang7879 3D-RecGAN.", "ref_function": ["background", "method", "objective", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Through multi-object detection and instance-level understanding altogether, learning the disentangled representation of multi-object becomes achievable.", "@cite_6 exploits the yolov2 structure @cite_51 to estimate the 3D bounding box and center of the multi-object and obtains the orientations.", "In @cite_48 , they estimate the 3D shape rendering and orientation under faster R-CNN structure @cite_12 .", "They obtain the shape rendering via weighted sum of the parameterized prior shape with PCL.", "Orientations are estimated by classifying the bins which indicate the discretized object pose.", "Similarly, in @cite_15 , they design the object observation factor to perform data association for pose SLAM.", "RoIs for multi-object are obtained by @cite_51 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "We present NOLBO, a variational observation model estimation for 3D multi-object from 2D single shot. Previous probabilistic instance-level understandings mainly consider the single-object image, not single shot with multi-object; relations between objects and the entire scene are out of their focus. The objectness of each observation also hardly join their model. Therefore, we propose a method to approximate the Bayesian observation model of scene-level 3D multi-object understanding. By exploiting variational auto-encoder (VAE), we estimate latent variables from the entire scene, which follow tractable distributions and concurrently imply 3D full shape and pose. To perform object-oriented data association and probabilistic simultaneous localization and mapping (SLAM), our observation models can easily be adopted to probabilistic inference by replacing object-oriented features with latent variables.", "reference": {"@cite_48": {"mid": "2799123546", "abstract": "We present a fast inverse-graphics framework for instance-level 3D scene understanding. We train a deep convolutional network that learns to map image regions to the full 3D shape and pose of all object instances in the image. Our method produces a compact 3D representation of the scene, which can be readily used for applications like autonomous driving. Many traditional 2D vision outputs, like instance segmentations and depth-maps, can be obtained by simply rendering our output 3D scene model. We exploit class-specific shape priors by learning a low dimensional shape-space from collections of CAD models. We present novel representations of shape and pose, that strive towards better 3D equivariance and generalization. In order to exploit rich supervisory signals in the form of 2D annotations like segmentation, we propose a differentiable Render-and-Compare loss that allows 3D shape and pose to be learned with 2D supervision. We evaluate our method on the challenging real-world datasets of Pascal3D+ and KITTI, where we achieve state-of-the-art results.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2768879211", "abstract": "We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task [10] that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by [27, 28] that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm. For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our approach substantially outperforms other recent CNN-based approaches [10, 25] when they are all used without postprocessing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method.", "ref_function": ["background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2789111492", "abstract": "We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fly, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show --- to the best of our knowledge --- first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_51": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "uses"]}, "@cite_12": {"mid": "2613718673", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2 mAP) and 2012 (70.4 mAP) using 300 proposals per image. Code is available at https: github.com ShaoqingRen faster_rcnn.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["These studies are efficient because they mainly concern direct and accurate estimation of the object characteristics through network modeling; on the other hand, the probabilistic observation models are relatively less considered.", "Although they exploit the neural network for nonlinear regression, approximating the intractable distribution is rarely concerned.", "Therefore, Bayesian inference with obtained features are challenging; for example, data association for SLAM is considered only in front-end and additional algorithms are necessary to perform loop closing and place recognition @cite_45 @cite_15 ."], "label": ["Summarize the above references", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "We present NOLBO, a variational observation model estimation for 3D multi-object from 2D single shot. Previous probabilistic instance-level understandings mainly consider the single-object image, not single shot with multi-object; relations between objects and the entire scene are out of their focus. The objectness of each observation also hardly join their model. Therefore, we propose a method to approximate the Bayesian observation model of scene-level 3D multi-object understanding. By exploiting variational auto-encoder (VAE), we estimate latent variables from the entire scene, which follow tractable distributions and concurrently imply 3D full shape and pose. To perform object-oriented data association and probabilistic simultaneous localization and mapping (SLAM), our observation models can easily be adopted to probabilistic inference by replacing object-oriented features with latent variables.", "reference": {"@cite_15": {"mid": "2789111492", "abstract": "We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fly, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show --- to the best of our knowledge --- first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_45": {"mid": "2097696373", "abstract": "We present the major advantages of a new 'object oriented' 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, real-time 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph optimisation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruction, but with a huge representation compression. The object graph enables predictions for accurate ICP-based camera to model tracking at each live frame, and efficient active search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relocalisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}}}
{"sentences": ["To handle the intractable target distribution, latent variables can be adopted @cite_46 @cite_18 @cite_34 @cite_13 .", "In order to understand and utilize the latent space, @cite_58 @cite_9 have studied the relations between latent variables and object visualization by using VAE @cite_34 .", "However, it is still challenging to apply the proposed method to probabilistic model approximation, as it mainly concentrates on the interpretable graphic codes.", "To approximate the observation probability, entropy and variational likelihood is exploited in the field of the active vision @cite_22 @cite_14 @cite_7 .", "Using VAE, @cite_60 @cite_8 have proposed methods to approximate the observation model of 3D objects for Bayesian inference.", "Based on the ELBO which approximates the observation model, they have shown that how the probabilistic SLAM with data association can be performed with expectation-maximization (EM) algorithm."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken"], "target_paper": "We present NOLBO, a variational observation model estimation for 3D multi-object from 2D single shot. Previous probabilistic instance-level understandings mainly consider the single-object image, not single shot with multi-object; relations between objects and the entire scene are out of their focus. The objectness of each observation also hardly join their model. Therefore, we propose a method to approximate the Bayesian observation model of scene-level 3D multi-object understanding. By exploiting variational auto-encoder (VAE), we estimate latent variables from the entire scene, which follow tractable distributions and concurrently imply 3D full shape and pose. To perform object-oriented data association and probabilistic simultaneous localization and mapping (SLAM), our observation models can easily be adopted to probabilistic inference by replacing object-oriented features with latent variables.", "reference": {"@cite_18": {"mid": "2130428211", "abstract": "Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efficient, easy to implement and significantly more accurate than standard variational Bayesian inference for LDA.", "ref_function": ["background", "background", "method"], "cite_purpose": ["uses"]}, "@cite_14": {"mid": "638212880", "abstract": "Directing robot attention to recognise activities and to anticipate events like goal-directed actions is a crucial skill for human-robot interaction. Unfortunately, issues like intrinsic time constraints, the spatially distributed nature of the entailed information sources, and the existence of a multitude of unobservable states affecting the system, like latent intentions, have long rendered achievement of such skills a rather elusive goal. The problem tests the limits of current attention control systems. It requires an integrated solution for tracking, exploration and recognition, which traditionally have been seen as separate problems in active vision. We propose a probabilistic generative framework based on information gain maximisation and a mixture of Kalman Filters that uses predictions in both recognition and attention-control. This framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration. Interestingly, the sensors control policy, directly derived from first principles, represents the intuitive trade-off between finding the most discriminative clues and maintaining overall awareness. Experiments on a simulated humanoid robot observing a human executing goal-oriented actions demonstrated improvement on recognition time and precision over baseline systems.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2144691386", "abstract": "We introduce a formalism for optimal sensor parameter selection for iterative state estimation in static systems. Our optimality criterion is the reduction of uncertainty in the state estimation process, rather than an estimator-specific metric (e.g., minimum mean squared estimate error). The claim is that state estimation becomes more reliable if the uncertainty and ambiguity in the estimation process can be reduced. We use Shannon's information theory to select information-gathering actions that maximize mutual information, thus optimizing the information that the data conveys about the true state of the system. The technique explicitly takes into account the a priori probabilities governing the computation of the mutual information. Thus, a sequential decision process can be formed by treating the a priori probability at a certain time step in the decision process as the a posteriori probability of the previous time step. We demonstrate the benefits of our approach in an object recognition application using an active camera for sequential gaze control and viewpoint selection. We describe experiments with discrete and continuous density representations that suggest the effectiveness of the approach.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2766469071", "abstract": "We develop a comprehensive description of the active inference framework, as proposed by Friston (2010), under a machine-learning compliant perspective. Stemming from a biological inspiration and the auto-encoding principles, a sketch of a cognitive architecture is proposed that should provide ways to implement estimation-oriented control policies. Computer simulations illustrate the effectiveness of the approach through a foveated inspection of the input data. The pros and cons of the control policy are analyzed in detail, showing interesting promises in terms of processing compression. Though optimizing future posterior entropy over the actions set is shown enough to attain locally optimal action selection, offline calculation using class-specific saliency maps is shown better for it saves processing costs through saccades pathways pre-processing, with a negligible effect on the recognition compression rates.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_60": {"mid": "2889482274", "abstract": "This paper presents a feature encoding method of complex 3D objects for high-level semantic features. Recent approaches to object recognition methods become important for semantic simultaneous localization and mapping (SLAM). However, there is a lack of consideration of the probabilistic observation model for 3D objects, as the shape of a 3D object basically follows a complex probability distribution. Furthermore, since the mobile robot equipped with a range sensor observes only a single view, much information of the object shape is discarded. These limitations are the major obstacles to semantic SLAM and view-independent loop closure using 3D object shapes as features. In order to enable the numerical analysis for the Bayesian inference, we approximate the true observation model of 3D objects to tractable distributions. Since the observation likelihood can be obtained from the generative model, we formulate the true generative model for 3D object with the Bayesian networks. To capture these complex distributions, we apply a variational auto-encoder. To analyze the approximated distributions and encoded features, we perform classification with maximum likelihood estimation and shape retrieval.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2967631872", "abstract": "We present a Bayesian object observation model for complete probabilistic semantic SLAM. Recent studies on object detection and feature extraction have become important for scene understanding and 3D mapping. However, 3D shape of the object is too complex to formulate the probabilistic observation model; therefore, performing the Bayesian inference of the object-oriented features as well as their pose is less considered. Besides, when the robot equipped with an RGB mono camera only observes the projected single view of an object, a significant amount of the 3D shape information is abandoned. Due to these limitations, semantic SLAM and viewpoint-independent loop closure using volumetric 3D object shape is challenging. In order to enable the complete formulation of probabilistic semantic SLAM, we approximate the observation model of a 3D object with a tractable distribution. We also estimate the variational likelihood from the 2D image of the object to exploit its observed single view. In order to evaluate the proposed method, we perform pose and feature estimation, and demonstrate that the automatic loop closure works seamlessly without additional loop detector in various environments.", "ref_function": ["background", "background", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2238938802", "abstract": "Objects belonging to different categories evoke reliably different fMRI activity patterns in human occipitotemporal cortex, with the most prominent distinction being that between animate and inanimate objects. An unresolved question is whether these categorical distinctions reflect category-associated visual properties of objects or whether they genuinely reflect object category. Here, we addressed this question by measuring fMRI responses to animate and inanimate objects that were closely matched for shape and low-level visual features. Univariate contrasts revealed animate-and inanimate-preferring regions in ventral and lateral temporal cortex even for individually matched object pairs e.g., snake-rope. Using representational similarity analysis, we mapped out brain regions in which the pairwise dissimilarity of multivoxel activity patterns neural dissimilarity was predicted by the objects' pairwise visual dissimilarity and or their categorical dissimilarity. Visual dissimilarity was measured as the time it took participants to find a unique target among identical distractors in three visual search experiments, where we separately quantified overall dissimilarity, outline dissimilarity, and texture dissimilarity. All three visual dissimilarity structures predicted neural dissimilarity in regions of visual cortex. Interestingly, these analyses revealed several clusters in which categorical dissimilarity predicted neural dissimilarity after regressing out visual dissimilarity. Together, these results suggest that the animate-inanimate organization of human visual cortex is not fully explained by differences in the characteristic shape or texture properties of animals and inanimate objects. Instead, representations of visual object properties and object category may coexist in more anterior parts of the visual system.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "2951004968", "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.", "ref_function": ["background", "method", "objective", "method", "method", "method", "result"], "cite_purpose": ["uses", "background"]}, "@cite_46": {"mid": "2107034620", "abstract": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.", "ref_function": ["objective", "background", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_58": {"mid": "1691728462", "abstract": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs.", "ref_function": ["objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2099471712", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["uses"]}}}
{"sentences": ["A method for producing courteous behavior is to use a game-theoretic interaction model @cite_15 .", "The problem is modeled in a way that all agents try to optimize their own behavior.", "By predicting and considering the reaction to the ego trajectory, courteous behavior can be generated.", "It is shown that this courtesy leads to better imitation of human behavior @cite_15 ."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Efficient behavior and trajectory planning is one of the major challenges for automated driving. Especially intersection scenarios are very demanding due to their complexity arising from the variety of maneuver possibilities and other traffic participants. A key challenge is to generate behaviors which optimize the comfort and progress of the ego vehicle but at the same time are not too aggressive towards other traffic participants. In order to maintain real time capability for courteous behavior and trajectory planning, an efficient formulation of the optimal control problem and corresponding solving algorithms are required. Consequently, a novel planning framework is presented which considers comfort and progress as well as the courtesy of actions in a graph-based behavior planning module. Utilizing the low level trajectory generation, the behavior result can be further optimized for driving comfort while satisfying constraints over the whole planning horizon. According experiments show the practicability and real time capability of the framework.", "reference": {"@cite_15": {"mid": "2963787234", "abstract": "Typically, autonomous cars optimize for a combination of safety, efficiency, and driving quality. But as we get better at this optimization, we start seeing behavior go from too conservative to too aggressive. The car's behavior exposes the incentives we provide in its cost function. In this work, we argue for cars that are not optimizing a purely selfish cost, but also try to be courteous to other interactive drivers. We formalize courtesy as a term in the objective that measures the increase in another driver's cost induced by the autonomous car's behavior. Such a courtesy term enables the robot car to be aware of possible irrationality of the human behavior, and plan accordingly. We analyze the effect of courtesy in a variety of scenarios. We find, for example, that courteous robot cars leave more space when merging in front of a human driver. Moreover, we find that such a courtesy term can help explain real human driver behavior on the NGSIM dataset.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["The most accurate affect recognition systems are based on electroencephalography @cite_27 .", "Although these systems achieve excellent results, they are limited by the use of dedicated sensors that require a controlled environment.", "The computer vision applications, on the other hand, are more suitable to work in real and uncontrolled scenarios, thanks to the use of more versatile sensors, such as RGB, RGB-D, or thermal cameras.", "Most of the vision based methods involve emotion recognition by the analysis of facial expressions @cite_18 @cite_15 @cite_14 .", "This is due to the presence of a great deal of labelled data in the state-of-the-art, organized in datasets, such as in @cite_4 .", "Although the face is one of the most discriminative ways to identify people's emotions, it is not always possible to capture facial expressions in large and crowded environments.", "This aspect motivated researchers to try other solutions, including poses and body movements."], "label": ["Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Other reference purpose", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Affective computing is a field of great interest in many computer vision applications, including video surveillance, behaviour analysis, and human-robot interaction. Most of the existing literature has addressed this field by analysing different sets of face features. However, in the last decade, several studies have shown how body movements can play a key role even in emotion recognition. The majority of these experiments on the body are performed by trained actors whose aim is to simulate emotional reactions. These unnatural expressions differ from the more challenging genuine emotions, thus invalidating the obtained results. In this paper, a solution for basic non-acted emotion recognition based on 3D skeleton and Deep Neural Networks (DNNs) is provided. The proposed work introduces three majors contributions. First, unlike the current state-of-the-art in non-acted body affect recognition, where only static or global body features are considered, in this work also temporal local movements performed by subjects in each frame are examined. Second, an original set of global and time-dependent features for body movement description is provided. Third, to the best of out knowledge, this is the first attempt to use deep learning methods for non-acted body affect recognition. Due to the novelty of the topic, only the UCLIC dataset is currently considered the benchmark for comparative tests. On the latter, the proposed method outperforms all the competitors.", "reference": {"@cite_18": {"mid": "2345305417", "abstract": "Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_14": {"mid": "2962711746", "abstract": "We have developed a convolutional neural network for the purpose of recognizing facial expressions in human beings. We have fine-tuned the existing convolutional neural network model trained on the visual recognition dataset used in the ILSVRC2012 to two widely used facial expression datasets - CFEE and RaFD, which when trained and tested independently yielded test accuracies of 74.79 and 95.71 , respectively. Generalization of results was evident by training on one dataset and testing on the other. Further, the image product of the cropped faces and their visual saliency maps were computed using Deep Multi-Layer Network for saliency prediction and were fed to the facial expression recognition CNN. In the most generalized experiment, we observed the top-1 accuracy in the test set to be 65.39 . General confusion trends between different facial expressions as exhibited by humans were also observed.", "ref_function": ["background", "method", "result", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2745497104", "abstract": "Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1,250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.", "ref_function": ["background", "background", "background", "objective", "method", "result", "result", "method", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "2151069331", "abstract": "During the last decades, information about the emotional state of users has become more and more important in human-computer interaction. Automatic emotion recognition enables the computer to recognize a user's emotional state and thus allows for appropriate reaction, which may pave the way for computers to act emotionally in the future. In the current study, we investigate different feature sets to build an emotion recognition system from electroencephalo-graphic signals. We used pictures from the International Affective Picture System to induce three emotional states: pleasant, neutral, and unpleasant. We designed a headband with four build-in electrodes at the forehead, which was used to record data from five subjects. Compared to standard EEG-caps, the headband is comfortable to wear and easy to attach, which makes it more suitable for everyday life conditions. To solve the recognition task we developed a system based on support vector machines. With this system we were able to achieve an average recognition rate up to 66.7 on subject dependent recognition, solely based on EEG signals.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2141990597", "abstract": "Conditional Random Fields (CRFs) can be used as a discriminative approach for simultaneous sequence segmentation and frame labeling. Latent-Dynamic Conditional Random Fields (LDCRFs) incorporates hidden state variables within CRFs which model sub-structure motion patterns and dynamics between labels. Motivated by the success of LDCRFs in gesture recognition, we propose a framework for automatic facial expression recognition from continuous video sequence by modeling temporal variations within shapes using LDCRFs. We show that the proposed approach outperforms CRFs for recognizing facial expressions. Using Principal Component Analysis (PCA) we study the separability of various expression classes in lower dimension projected spaces. By comparing the performance of CRFs and LDCRFs against that of Support Vector Machines (SVMs), we demonstrate that temporal variations within shapes are crucial in classifying expressions especially for those with a small range of facial motion like anger and sadness. We also show empirically that only using changes in facial appearance over time, without using shape variations, is not sufficient to obtain high performance for facial expression recognition.", "ref_function": ["background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Also closely related is the work of @cite_6 who apply the theory of Potential Games @cite_23 to the problem of miner hash rate allocation across multiple blockchains.", "They prove that, regardless of individual hash rate and coinbase rewards for each of the blockchains, hash rate allocation will converge to a pure equilibrium provided that miners follow .", "The model assumes minimal rationality on behalf of the players, i.e., that they follow an arbitrary better response step improving their individual payoffs.''", "@cite_6 do not identify a specific equilibrium point, nor do they specify what the should be.", "But their work anticipates some of the theoretical results we present in .", "Furthermore, they show that the equilibrium point can be changed by changing a blockchain's coinbase reward, a property that is emergent from the properties of the equilibrium and one that we exploit to increase security in .", "@cite_16 reached similar conclusions as @cite_6 using a slightly different game theoretical model of hash rate allocation across cryptocurrencies and mining pools."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the result relationship between own work and references", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method"], "target_paper": "All public blockchains are secured by a proof of opportunity cost among block producers. For example, the security offered by proof-of-work (PoW) systems, like Bitcoin, is due to spent computation; it is work precisely because it cannot be performed for free. In general, more resources provably lost in producing blocks yields more security for the blockchain. When two blockchains share the same mechanism for providing opportunity cost, as is the case when they share the same PoW algorithm, the two chains compete for resources from block producers. Indeed, if there exists a liquid market between resource types, then theoretically all blockchains will compete for resources. In this paper, we show that there exists a resource allocation equilibrium between any two blockchains, which is essentially driven by the fiat value of reward that each chain offers in return for providing security. We go on to prove that this equilibrium is singular and always achieved provided that block producers behave in a greedy, but cautious fashion. The opposite is true when they are overly greedy: resource allocation oscillates in extremes between the two chains. We show that these results hold both in practice and in a block generation simulation. Finally, we demonstrate several applications of this theory including a trustless price-ratio oracle, increased security for blockchains whose coins have lower fiat value, and a quantification of cost to allocating resources away from the equilibrium.", "reference": {"@cite_16": {"mid": "2911474632", "abstract": "We model the competition over several blockchains characterizing multiple cryptocurrencies as a non-cooperative game. Then, we specialize our results to two instances of the general game, showing properties of the Nash equilibrium. In particular, leveraging results about congestion games, we establish the existence of pure Nash equilibria and provide efficient algorithms for finding such equilibria.", "ref_function": ["background", "method", "result"], "cite_purpose": ["similarities"]}, "@cite_23": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2804529604", "abstract": "We formalize the current practice of strategic mining in multi-cryptocurrency markets as a game, and prove that any better-response learning in such games converges to equilibrium. We then offer a reward design scheme that moves the system configuration from any initial equilibrium to a desired one for any better-response learning of the miners. Our work introduces the first multi-coin strategic attack for adaptive and learning miners, as well as the study of reward design in a multi-agent system of learning agents.", "ref_function": ["background", "method", "objective"], "cite_purpose": ["background", "background", "similarities"]}}}
{"sentences": ["Several authors have sought to determine the optimal hash rate allocation between blockchains for miners or mining pools.", "@cite_7 argue that miners allocate their hash rate between multiple blockchains so as to minimize the risk associated with fluctuations in coin price.", "@cite_25 make a similar argument except that their measure of risk is volatility in the payout rate between mining pools.", "@cite_1 extend this model to mining across blockchains with different PoW algorithms.", "All of the above approaches are complimentary to the present work, which seeks only to explain miner behavior.", "In fact, miner-specific behavioral choices help to explain why the aggregate hash rate allocation does not fully allocate to one chain over another (see for details)."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references", "Other functional sentences"], "target_paper": "All public blockchains are secured by a proof of opportunity cost among block producers. For example, the security offered by proof-of-work (PoW) systems, like Bitcoin, is due to spent computation; it is work precisely because it cannot be performed for free. In general, more resources provably lost in producing blocks yields more security for the blockchain. When two blockchains share the same mechanism for providing opportunity cost, as is the case when they share the same PoW algorithm, the two chains compete for resources from block producers. Indeed, if there exists a liquid market between resource types, then theoretically all blockchains will compete for resources. In this paper, we show that there exists a resource allocation equilibrium between any two blockchains, which is essentially driven by the fiat value of reward that each chain offers in return for providing security. We go on to prove that this equilibrium is singular and always achieved provided that block producers behave in a greedy, but cautious fashion. The opposite is true when they are overly greedy: resource allocation oscillates in extremes between the two chains. We show that these results hold both in practice and in a block generation simulation. Finally, we demonstrate several applications of this theory including a trustless price-ratio oracle, increased security for blockchains whose coins have lower fiat value, and a quantification of cost to allocating resources away from the equilibrium.", "reference": {"@cite_1": {"mid": "2944490618", "abstract": "Mining is a central operation of all proof-of-work (PoW) based cryptocurrencies. The vast majority of miners today participate in \"mining pools\" instead of \"solo mining\" in order to lower risk and achieve a more steady income. However, this rise of participation in mining pools negatively affects the decentralization levels of most cryptocurrencies. In this work, we look into mining pools from the point of view of a miner: We present an analytical model and implement a computational tool that allows miners to optimally distribute their computational power over multiple pools and PoW cryptocurrencies (i.e. build a mining portfolio), taking into account their risk aversion levels. Our tool allows miners to maximize their risk-adjusted earnings by diversifying across multiple mining pools which enhances PoW decentralization. Finally, we run an experiment in Bitcoin historical data and demonstrate that a miner diversifying over multiple pools, as instructed by our model tool, receives a higher overall Sharpe ratio (i.e. average excess reward over its standard deviation volatility).", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2789931105", "abstract": "The rise of centralized mining pools for risk sharing does not necessarily undermine the decentralization required for public blockchains. However, mining pools as a financial innovation significantly escalates the arms race among competing miners and thus increases the energy consumption of proof-of-work-based blockchains. Each individual miner's cross-pool diversification and endogenous fees charged by pools generally sustain decentralization --- larger pools better internalize their externality on global hash rates, charge higher fees, attract disproportionately fewer miners, and thus grow slower. Empirical evidence from Bitcoin mining supports our model predictions, and the economic insights apply to many other blockchain protocols, as well as mainstream industries with similar characteristics.", "ref_function": ["background", "background", "background", "result"], "cite_purpose": ["differences"]}, "@cite_7": {"mid": "2809512071", "abstract": "Abrupt changes in the miner hash rate applied to a proof-of-work (PoW) blockchain can adversely affect user experience and security. Because different PoW blockchains often share hashing algorithms, miners face a complex choice in deciding how to allocate their hash power among chains. We present an economic model that leverages Modern Portfolio Theory to predict a miner\u2019s allocation over time using price data and inferred risk tolerance. The model matches actual allocations with mean absolute error within 20 for four out of the top five miners active on both Bitcoin (BTC) and Bitcoin Cash (BCH) blockchains. A model of aggregate allocation across those four miners shows excellent agreement in magnitude with the actual aggregate as well a correlation coefficient of 0.649. The accuracy of the aggregate allocation model is also sufficient to explain major historical changes in inter-block time (IBT) for BCH. Because estimates of miner risk are not time-dependent and our model is otherwise price-driven, we are able to use it to anticipate the effect of a major price shock on hash allocation and IBT in the BCH blockchain. Using a Monte Carlo simulation, we show that, despite mitigation by the new difficulty adjustment algorithm, a price drop of 50 could increase the IBT by 50 for at least a day, with a peak delay of 100 .", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["@cite_22 devised a Markov Decision Process (MDP) for discovering optimal selfish mining @cite_15 strategies.", "@cite_12 expanded the model to incorporate adjustable network parameters and include analysis of doublespend attacks.", "@cite_21 extend the MDP of @cite_12 to model mining difficulty adjustment.", "The biggest differences between these approaches and the present work is that the former analyze optimal behavior in single blockchains while the present work attempts to explain behavior across multiple blockchains."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the objective relationship between own work and references"], "target_paper": "All public blockchains are secured by a proof of opportunity cost among block producers. For example, the security offered by proof-of-work (PoW) systems, like Bitcoin, is due to spent computation; it is work precisely because it cannot be performed for free. In general, more resources provably lost in producing blocks yields more security for the blockchain. When two blockchains share the same mechanism for providing opportunity cost, as is the case when they share the same PoW algorithm, the two chains compete for resources from block producers. Indeed, if there exists a liquid market between resource types, then theoretically all blockchains will compete for resources. In this paper, we show that there exists a resource allocation equilibrium between any two blockchains, which is essentially driven by the fiat value of reward that each chain offers in return for providing security. We go on to prove that this equilibrium is singular and always achieved provided that block producers behave in a greedy, but cautious fashion. The opposite is true when they are overly greedy: resource allocation oscillates in extremes between the two chains. We show that these results hold both in practice and in a block generation simulation. Finally, we demonstrate several applications of this theory including a trustless price-ratio oracle, increased security for blockchains whose coins have lower fiat value, and a quantification of cost to allocating resources away from the equilibrium.", "reference": {"@cite_15": {"mid": "2886706106", "abstract": "The Bitcoin cryptocurrency records its transactions in a public log called the blockchain. Its security rests critically on the distributed protocol that maintains the blockchain, run by participants called miners. Conventional wisdom asserts that the mining protocol is incentive-compatible and secure against colluding minority groups, that is, it incentivizes miners to follow the protocol as prescribed.", "ref_function": ["background", "background", "background"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2952115223", "abstract": "Abstract Cryptographic assets such as Bitcoin and Ethereum provide distributed consensus with a Proof-of-Work protocol and incentive-based engineering. The consensus is inherently dependent on the value of the asset due to the incentives. The value of these assets frequently fluctuates, which in turn influences the incentive component of the consensus mechanism. For a proof-of-work consensus to be secure, the participation reward must have a perceived real-world value. The future of this perception is not at all clear. The recent 70 drop in the value of Bitcoin versus the US Dollar may be precipitating a circle of declining security of the platform which we explore in depth in this paper. In this paper, we analyze the impact of fluctuations on the security of Bitcoin now, and in the future. We introduce a novel method to examine the rationale of a miner based on the price fluctuations. We integrate our method with an existing security evaluation framework and simulator. Using our approach, we determine and report on the impact of the value of the cryptographic asset on the security of the blockchain given the miner\u2019s rationale. Our method allows us to evaluate the impact of different methods of incentive manipulation such as reduced block-reward and transaction fees, by simulation.", "ref_function": ["background", "background", "background", "background", "result", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "1523833175", "abstract": "The Bitcoin protocol requires nodes to quickly distribute newly created blocks. Strong nodes can, however, gain higher payoffs by withholding blocks they create and selectively postponing their publication. The existence of such selfish mining attacks was first reported by Eyal and Sirer, who have demonstrated a specific deviation from the standard protocol (a strategy that we name SM1).", "ref_function": ["background", "background", "background"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2536325433", "abstract": "Proof of Work (PoW) powered blockchains currently account for more than 90 of the total market capitalization of existing digital cryptocurrencies. Although the security provisions of Bitcoin have been thoroughly analysed, the security guarantees of variant (forked) PoW blockchains (which were instantiated with different parameters) have not received much attention in the literature. This opens the question whether existing security analysis of Bitcoin's PoW applies to other implementations which have been instantiated with different consensus and or network parameters. In this paper, we introduce a novel quantitative framework to analyse the security and performance implications of various consensus and network parameters of PoW blockchains. Based on our framework, we devise optimal adversarial strategies for double-spending and selfish mining while taking into account real world constraints such as network propagation, different block sizes, block generation intervals, information propagation mechanism, and the impact of eclipse attacks. Our framework therefore allows us to capture existing PoW-based deployments as well as PoW blockchain variants that are instantiated with different parameters, and to objectively compare the tradeoffs between their performance and security provisions.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["Closely related to our task of hallucinating the road layout is image inpainting.", "In recent years, deep convolutional neural networks (CNNs) enable the possibility of image inpainting with large missing areas, as CNNs can extract abstract semantic information from the observable context.", "The Context Encoder (CE) @cite_12 network is proposed to inpaint the image with large rectangle areas missing at the image center by applying reconstruction and adversarial loss @cite_13 in training.", "CE-like networks @cite_21 @cite_8 @cite_5 are proposed with additional discriminative networks applied on locally missing regions, or on the entire image in a patch-wise manner, which are able to perform inpainting with regions missing at arbitrary position.", "Given a trained generative network, Yeh al @cite_20 do inpainting by finding the embedding vector that minimizes the reconstruction loss by applying back-propagation to the input embedding vector.", "The main difference between inpainting and the tasks addressed by us and @cite_1 @cite_17 , is that the previously introduced methods of image inpainting are all in a setting in which the complete ground truth is available."], "label": ["General descriptions of the topic", "General descriptions of the topic", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references"], "target_paper": "We propose a novel single-step training strategy that allows convolutional encoder-decoder networks that use skip connections, to complete partially observed data by means of hallucination. This strategy is demonstrated for the task of completing 2-D road layouts as well as 3-D vehicle shapes. As input, it takes data from a partially observed domain, for which no ground truth is available, and data from an unpaired prior knowledge domain and trains the network in an end-to-end manner. Our single-step training strategy is compared against two state-of-the-art baselines, one using a two-step auto-encoder training strategy and one using an adversarial strategy. Our novel strategy achieves an improvement up to +12.2 F-measure on the Cityscapes dataset. The learned network intrinsically generalizes better than the baselines on unseen datasets, which is demonstrated by an improvement up to +23.8 F-measure on the unseen KITTI dataset. Moreover, our approach outperforms the baselines using the same backbone network on the 3-D shape completion benchmark by a margin of 0.006 Hamming distance.", "reference": {"@cite_8": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2791184993", "abstract": "Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2964294967", "abstract": "Given a single RGB image of a complex outdoor road scene in the perspective view, we address the novel problem of estimating an occlusion-reasoned semantic scene layout in the top-view. This challenging problem not only requires an accurate understanding of both the 3D geometry and the semantics of the visible scene, but also of occluded areas. We propose a convolutional neural network that learns to predict occluded portions of the scene layout by looking around foreground objects like cars or pedestrians. But instead of hallucinating RGB values, we show that directly predicting the semantics and depths in the occluded areas enables a better transformation into the top-view. We further show that this initial top-view representation can be significantly enhanced by learning priors and rules about typical road layouts from simulated or, if available, map data. Crucially, training our model does not require costly or subjective human annotations for occluded areas or the top-view, but rather uses readily available annotations for standard semantic segmentation in the perspective view. We extensively evaluate and analyze our approach on the KITTI and Cityscapes data sets.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["differences"]}, "@cite_5": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2963917315", "abstract": "Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "1710476689", "abstract": "For many AI projects, deep learning techniques are increasingly being used as the building blocks for innovative solutions ranging from image classification to object detection, image segmentation, image similarity, and text analytics (e.g., sentiment analysis, key phrase extraction). GANs, first introduced by (2014), are emerging as a powerful new approach toward teaching computers how to do complex tasks through a generative process. As noted by Yann LeCun (at http: bit.ly LeCunGANs ), GANs are truly the \u201ccoolest idea in machine learning in the last 20 years.\u201d", "ref_function": ["background", "background", "other"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2963420272", "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.", "ref_function": ["background", "background", "background", "method", "result", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["differences"]}}}
{"sentences": ["If the ground truth is not available, one has to hallucinate the region to be completed.", "Srikantha and Gall @cite_23 propose a system to hallucinate a depth map and a semantic map, given an RGB image and a noisy, incomplete depth map, which is able to remove the foreground objects.", "Schulter al @cite_1 proposes a CNN to conduct a similar task without depth, by intentionally adding random foreground masks during training.", "Recently, a VAE with two-step training @cite_4 is proposed for shape completion.", "In the first step, a canonical VAE is trained on a complete shape prior dataset which has no direct correspondence to the incomplete shape dataset.", "Then the amortized maximum likelihood (AML) is applied as supervision on the incomplete shape data with the decoder fixed in the second training step.", "This VAE approach is originally used to learn 3-D vehicle shape completion, but it can be generalized to similar tasks such as our 2-D road layout hallucinating.", "Therefore, we use this approach as a baseline, which is referred to as ."], "label": ["Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Describing used methods"], "target_paper": "We propose a novel single-step training strategy that allows convolutional encoder-decoder networks that use skip connections, to complete partially observed data by means of hallucination. This strategy is demonstrated for the task of completing 2-D road layouts as well as 3-D vehicle shapes. As input, it takes data from a partially observed domain, for which no ground truth is available, and data from an unpaired prior knowledge domain and trains the network in an end-to-end manner. Our single-step training strategy is compared against two state-of-the-art baselines, one using a two-step auto-encoder training strategy and one using an adversarial strategy. Our novel strategy achieves an improvement up to +12.2 F-measure on the Cityscapes dataset. The learned network intrinsically generalizes better than the baselines on unseen datasets, which is demonstrated by an improvement up to +23.8 F-measure on the unseen KITTI dataset. Moreover, our approach outperforms the baselines using the same backbone network on the 3-D shape completion benchmark by a margin of 0.006 Hamming distance.", "reference": {"@cite_4": {"mid": "2798314605", "abstract": "3D shape completion from partial point clouds is a fundamental problem in computer vision and computer graphics. Recent approaches can be characterized as either data-driven or learning-based. Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations. Learning-based approaches, in contrast, avoid the expensive optimization step and instead directly predict the complete shape from the incomplete observations using deep neural networks. However, full supervision is required which is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. Tackling 3D shape completion of cars on ShapeNet [5] and KITTI [18], we demonstrate that the proposed amortized maximum likelihood approach is able to compete with a fully supervised baseline and a state-of-the-art data-driven approach while being significantly faster. On ModelNet [49], we additionally show that the approach is able to generalize to other object categories as well.", "ref_function": ["background", "background", "background", "method", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2964294967", "abstract": "Given a single RGB image of a complex outdoor road scene in the perspective view, we address the novel problem of estimating an occlusion-reasoned semantic scene layout in the top-view. This challenging problem not only requires an accurate understanding of both the 3D geometry and the semantics of the visible scene, but also of occluded areas. We propose a convolutional neural network that learns to predict occluded portions of the scene layout by looking around foreground objects like cars or pedestrians. But instead of hallucinating RGB values, we show that directly predicting the semantics and depths in the occluded areas enables a better transformation into the top-view. We further show that this initial top-view representation can be significantly enhanced by learning priors and rules about typical road layouts from simulated or, if available, map data. Crucially, training our model does not require costly or subjective human annotations for occluded areas or the top-view, but rather uses readily available annotations for standard semantic segmentation in the perspective view. We extensively evaluate and analyze our approach on the KITTI and Cityscapes data sets.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2520145460", "abstract": "Building 3D scene models has been a longstanding goal of computer vision. The great progress in depth sensors brings us one step closer to achieving this in a single shot. However, depth sensors still produce imperfect measurements that are sparse and contain holes. While depth completion aims at tackling this issue, it ignores the fact that some regions of the scene are occluded by the foreground objects. Building a scene model would therefore require to hallucinate the depth behind these objects. In contrast with existing methods that either rely on manual input, or focus on the indoor scenario, we introduce a fully-automatic method to jointly complete and hallucinate depth and semantics in challenging outdoor scenes. To this end, we develop a two-layer model representing both the visible information and the hidden one. At the heart of our approach lies a formulation based on the Mumford-Shah functional, for which we derive an effective optimization strategy. Our experiments evidence that our approach can accurately fill the large holes in the input depth maps, segment the different kinds of objects in the scene, and hallucinate the depth and semantics behind the foreground objects.", "ref_function": ["background", "background", "background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Road layout hallucinating can be seen as a specific approach of road layout understanding, which is an important task for robot and intelligent vehicle navigation.", "One challenge is that the ego-centric sensory data usually contains occlusions of the foreground objects, which makes roads visually incomplete.", "Many works tackling occlusions are focusing on using front-view images, such as road boundary detection @cite_9 , and road segmentation @cite_22 .", "Less work has been carried out on the top-view ego-centric sensing.", "In @cite_1 , the proposed system can produce a top-view road layout, while the occlusion is still addressed on the front-view image by pre-processing.", "In the later top-view refinement, the GPS is heavily relied on for a paired reconstruction supervision.", "We use a variant of this method, which only uses unpaired prior knowledge and thus no GPS pairing, as the second baseline.", "It is referred to as ."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Describing used methods", "Not sure"], "target_paper": "We propose a novel single-step training strategy that allows convolutional encoder-decoder networks that use skip connections, to complete partially observed data by means of hallucination. This strategy is demonstrated for the task of completing 2-D road layouts as well as 3-D vehicle shapes. As input, it takes data from a partially observed domain, for which no ground truth is available, and data from an unpaired prior knowledge domain and trains the network in an end-to-end manner. Our single-step training strategy is compared against two state-of-the-art baselines, one using a two-step auto-encoder training strategy and one using an adversarial strategy. Our novel strategy achieves an improvement up to +12.2 F-measure on the Cityscapes dataset. The learned network intrinsically generalizes better than the baselines on unseen datasets, which is demonstrated by an improvement up to +23.8 F-measure on the unseen KITTI dataset. Moreover, our approach outperforms the baselines using the same backbone network on the 3-D shape completion benchmark by a margin of 0.006 Hamming distance.", "reference": {"@cite_9": {"mid": "2904840670", "abstract": "This paper is about the detection and inference of road boundaries from mono-images. Our goal is to trace out, in an image, the projection of road boundaries irrespective of whether or not the boundary is actually visible. Large scale occlusion by vehicles prohibits direct approaches - many scenes present 100 occlusion and so we must infer the boundary location using scene context. Such a problem is well suited to CNN derived approaches but the sinuous structure of a hidden narrow continuous curve running through the image presents challenges for conventional NN-architectures. We approach this as a coupled, two class detection problem-solving for occluded and non-occluded curve partitions with a continuity constraint. Our network output is in a hybrid discrete-continuous form which we interpret as measurements of segments of the true road boundary. These measurements are passed to a model selection stage which associates measurements to minimal number of a-priori unknown set of geometric primitives (cubic curves) representing road boundaries. We present a semi-supervised method which leverages a visual localisation to generate 25 thousand labelled images for training and testing - the results of which are presented in the conclusion of the paper.", "ref_function": ["background", "objective", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2964294967", "abstract": "Given a single RGB image of a complex outdoor road scene in the perspective view, we address the novel problem of estimating an occlusion-reasoned semantic scene layout in the top-view. This challenging problem not only requires an accurate understanding of both the 3D geometry and the semantics of the visible scene, but also of occluded areas. We propose a convolutional neural network that learns to predict occluded portions of the scene layout by looking around foreground objects like cars or pedestrians. But instead of hallucinating RGB values, we show that directly predicting the semantics and depths in the occluded areas enables a better transformation into the top-view. We further show that this initial top-view representation can be significantly enhanced by learning priors and rules about typical road layouts from simulated or, if available, map data. Crucially, training our model does not require costly or subjective human annotations for occluded areas or the top-view, but rather uses readily available annotations for standard semantic segmentation in the perspective view. We extensively evaluate and analyze our approach on the KITTI and Cityscapes data sets.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["differences"]}, "@cite_22": {"mid": "2807134386", "abstract": "Autonomous driving is becoming a reality, yet vehicles still need to rely on complex sensor fusion to understand the scene they act in. The ability to discern static environment and dynamic entities provides a comprehension of the road layout that poses constraints to the reasoning process about moving objects. We pursue this through a GAN-based semantic segmentation inpainting model to remove all dynamic objects from the scene and focus on understanding its static components such as streets, sidewalks and buildings. We evaluate this task on the Cityscapes dataset and on a novel synthetically generated dataset obtained with the CARLA simulator and specifically designed to quantitatively evaluate semantic segmentation inpaintings. We compare our methods with a variety of baselines working both in the RGB and segmentation domains.", "ref_function": ["background", "background", "objective", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": [": VQA is a task to answer the given question based on the input image.", "The question is usually embedded into a vector with LSTM @cite_6 , and the image is represented by the fixed-size grid features from ResNet @cite_4 .", "Recently, @cite_28 focuses on bottom-up attention of image features and proposes a set of salient image regions with natural expression and additional attributes detected by Faster-RCNN @cite_3 .", "Furthermore, its training set contains 1,600 object classes and 400 attribute classes, larger than the original 80 object classes."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "Reference to single investigations in the past: about method", "Other comments"], "target_paper": "The interaction between language and visual information has been emphasized in visual question answering (VQA) with the help of attention mechanism. However, the relationship between words in question has been underestimated, which makes it hard to answer questions that involve the relationship between multiple entities, such as comparison and counting. In this paper, we develop the graph reasoning networks to tackle this problem. Two kinds of graphs are investigated, namely inter-graph and intra-graph. The inter-graph transfers features of the detected objects to their related query words, enabling the output nodes to have both semantic and factual information. The intra-graph exchanges information between these output nodes from inter-graph to amplify implicit yet important relationship between objects. These two kinds of graphs cooperate with each other, and thus our resulting model can reason the relationship and dependence between objects, which leads to realization of multi-step reasoning. Experimental results on the GQA v1.1 dataset demonstrate the reasoning ability of our method to handle compositional questions about real-world images. We achieve state-of-the-art performance, boosting accuracy to 57.04 . On the VQA 2.0 dataset, we also receive a promising improvement on overall accuracy, especially on counting problem.", "reference": {"@cite_28": {"mid": "2745461083", "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr SPICE BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.", "ref_function": ["background", "objective", "objective", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2613718673", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2 mAP) and 2012 (70.4 mAP) using 300 proposals per image. Code is available at https: github.com ShaoqingRen faster_rcnn.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2194775991", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57 error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "ref_function": ["background", "background", "method", "result", "result", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["Based on the fusion methods of the two features, we can classify VQA models into two categories: early fusion models and later fusion models.", "Early fusion models try to fine-tune the image classification network with the intervention of the question, they insert the question embedding into the batch normalization layer @cite_23 @cite_31 to propose MODERN architecture.", "These models have less risk of over-fitting because of affecting less than 1 However, sometimes, the information given in the image is not enough to infer the right answer, common sense is required in the external knowledge-based models.", "@cite_9 builds the FVQA dataset based on DBpedia @cite_30 , ConceptNet @cite_14 and WebChild @cite_34 .", "@cite_1 queries the triplet (visual concept, relation, attribute) in this dataset to score the retrieved facts.", "And @cite_32 builds a relation graph based on the retrieved facts regarding the visual concept and attribute as nodes and relation as links to exchange information."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Other comments", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "The interaction between language and visual information has been emphasized in visual question answering (VQA) with the help of attention mechanism. However, the relationship between words in question has been underestimated, which makes it hard to answer questions that involve the relationship between multiple entities, such as comparison and counting. In this paper, we develop the graph reasoning networks to tackle this problem. Two kinds of graphs are investigated, namely inter-graph and intra-graph. The inter-graph transfers features of the detected objects to their related query words, enabling the output nodes to have both semantic and factual information. The intra-graph exchanges information between these output nodes from inter-graph to amplify implicit yet important relationship between objects. These two kinds of graphs cooperate with each other, and thus our resulting model can reason the relationship and dependence between objects, which leads to realization of multi-step reasoning. Experimental results on the GQA v1.1 dataset demonstrate the reasoning ability of our method to handle compositional questions about real-world images. We achieve state-of-the-art performance, boosting accuracy to 57.04 . On the VQA 2.0 dataset, we also receive a promising improvement on overall accuracy, especially on counting problem.", "reference": {"@cite_30": {"mid": "102708294", "abstract": "DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human-andmachine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_14": {"mid": "2016089260", "abstract": "ConceptNet is a freely available commonsense knowledge base and natural-language-processing tool-kit which supports many practical textual-reasoning tasks over real-world documents including topic-gisting, analogy-making, and other context oriented inferences. The knowledge base is a semantic network presently consisting of over 1.6 million assertions of commonsense knowledge encompassing the spatial, physical, social, temporal, and psychological aspects of everyday life. ConceptNet is generated automatically from the 700 000 sentences of the Open Mind Common Sense Project \u2014 a World Wide Web based collaboration with over 14 000 authors.", "ref_function": ["background", "background", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2964303913", "abstract": "Visual Question Answering (VQA) has attracted much attention in both computer vision and natural language processing communities, not least because it offers insight into the relationships between two important sources of information. Current datasets, and the models built upon them, have focused on questions which are answerable by direct analysis of the question and image alone. The set of such questions that require no external information to answer is interesting, but very limited. It excludes questions which require common sense, or basic factual knowledge to answer, for example. Here we introduce FVQA (Fact-based VQA), a VQA dataset which requires, and supports, much deeper reasoning. FVQA primarily contains questions that require external information to answer. We thus extend a conventional visual question answering dataset, which contains image-question-answer triplets, through additional image-question-answer-supporting fact tuples. Each supporting-fact is represented as a structural triplet, such as . We evaluate several baseline models on the FVQA dataset, and describe a novel model which is capable of reasoning about an image on the basis of supporting-facts.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2891394954", "abstract": "Question answering is an important task for autonomous agents and virtual assistants alike and was shown to support the disabled in efficiently navigating an overwhelming environment. Many existing methods focus on observation-based questions, ignoring our ability to seamlessly combine observed content with general knowledge. To understand interactions with a knowledge base, a dataset has been introduced recently and keyword matching techniques were shown to yield compelling results despite being vulnerable to misconceptions due to synonyms and homographs. To address this issue, we develop a learning-based approach which goes straight to the facts via a learned embedding space. We demonstrate state-of-the-art results on the challenging recently introduced fact-based visual question answering dataset, outperforming competing methods by more than (5 ).", "ref_function": ["background", "background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2962967746", "abstract": "Accurately answering a question about a given image requires combining observations with general knowledge. While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction, a novel fact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep net techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and a graph convolutional net method toreason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 10 compared to the state-of-the-art.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2963245493", "abstract": "It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic inputs are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the by a linguistic input. Specifically, we introduce Conditional Batch Normalization (CBN) as an efficient mechanism to modulate convolutional feature maps by a linguistic embedding. We apply CBN to a pre-trained Residual Network (ResNet), leading to the MODulatEd ResNet ( ) architecture, and show that this significantly improves strong baselines on two visual question answering tasks. Our ablation study confirms that modulating from the early stages of the visual processing is beneficial.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_34": {"mid": "1862719289", "abstract": "Applications are increasingly expected to make smart decisions based on what humans consider basic commonsense. An often overlooked but essential form of commonsense involves comparisons, e.g. the fact that bears are typically more dangerous than dogs, that tables are heavier than chairs, or that ice is colder than water. In this paper, we first rely on open information extraction methods to obtain large amounts of comparisons from the Web. We then develop a joint optimization model for cleaning and disambiguating this knowledge with respect to WordNet. This model relies on integer linear programming and semantic coherence scores. Experiments show that our model outperforms strong baselines and allows us to obtain a large knowledge base of disambiguated commonsense assertions.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Designing and proving mobile robot protocols is notoriously difficult.", "Formal methods encompass a long-lasting path of research that is meant to overcome errors of human origin.", "Unsurprisingly, this mechanized approach to protocol correctness was used in the context of mobile robots @cite_11 @cite_32 @cite_5 @cite_18 @cite_1 @cite_27 @cite_15 @cite_28 @cite_32 @cite_21 @cite_2 ."], "label": ["General descriptions of the topic", "General descriptions of the topic", "General reference to previous research or scholarship: research objective"], "target_paper": "The paper details the first successful attempt at using model-checking techniques to verify the correctness of distributed algorithms for robots evolving in a environment. The study focuses on the problem of rendezvous of two robots with lights. There exist many different rendezvous algorithms that aim at finding the minimal number of colors needed to solve rendezvous in various synchrony models (e.g., FSYNC, SSYNC, ASYNC). While these rendezvous algorithms are typically very simple, their analysis and proof of correctness tend to be extremely complex, tedious, and error-prone as impossibility results are based on subtle interactions between robots activation schedules. The paper presents a generic verification model written for the SPIN model-checker. In particular, we explain the subtle design decisions that allow to keep the search space finite and tractable, as well as prove several important theorems that support them. As a sanity check, we use the model to verify several known rendezvous algorithms in six different models of synchrony. In each case, we find that the results obtained from the model-checker are consistent with the results known in the literature. The model-checker outputs a counter-example execution in every case that is known to fail. In the course of developing and proving the validity of the model, we identified several fundamental theorems, including the ability for a well chosen algorithm and ASYNC scheduler to produce an emerging property of memory in a system of oblivious mobile robots, and why it is not a problem for luminous rendezvous algorithms.", "reference": {"@cite_18": {"mid": "1750856813", "abstract": "We propose a framework to build formal developments for robot networks using the Coq proof assistant, to state and prove formally various properties. We focus in this paper on impossibility proofs, as it is natural to take advantage of the Coq higher order calculus to reason about algorithms as abstract objects. We present in particular formal proofs of two impossibility results for convergence of oblivious mobile robots if respectively more than one half and more than one third of the robots exhibit Byzantine failures, starting from the original theorems by . Thanks to our formalisation, the corresponding Coq developments are quite compact. To our knowledge, these are the first certified (in the sense of formally proved) impossibility results for robot networks.", "ref_function": ["background", "background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_28": {"mid": "2264618208", "abstract": "This paper establishes a framework based on logic and automata theory in which to model and automatically verify that multiple mobile robots, with sensing abilities, moving asynchronously, correctly perform their tasks. The motivation is from practical scenarios in which the environment is not completely know to the robots, e.g., physical robots exploring a maze, or software agents exploring a hostile network. The framework shows how to express tasks in a logical language, and exhibits an algorithm solving the parameterised verification problem, where the graphs are treated as the parameter. The main assumption that yields decidability is that the robots take a bounded number of turns. We prove that dropping this assumption results in undecidability, even for robots with very limited (\u201clocal\u201d) sensing abilities.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2963688871", "abstract": "We study verification problems for autonomous swarms of mobile robots that self-organize and cooperate to solve global objectives. In particular, we focus in this paper on the model proposed by Suzuki and Yamashita of anonymous robots evolving in a discrete space with a finite number of locations (here, a ring). A large number of algorithms have been proposed working for rings whose size is not a priori fixed and can be hence considered as a parameter. Handmade correctness proofs of these algorithms have been shown to be error-prone, and recent attention had been given to the application of formal methods to automatically prove those. Our work is the first to study the verification problem of such algorithms in the parameterized case. We show that safety and reachability problems are undecidable for robots evolving asynchronously. On the positive side, we show that safety properties are decidable in the synchronous case, as well as in the asynchronous case for a particular class of algorithms. Several properties on the protocol can be decided as well. Decision procedures rely on an encoding in Presburger arithmetics formulae that can be verified by an SMT-solver. Feasibility of our approach is demonstrated by the encoding of several case studies.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "179805188", "abstract": "Recent advances in Distributed Computing highlight models and algorithms for autonomous swarms of mobile robots that self-organize and cooperate to solve global objectives. The overwhelming majority of works so far considers handmade algorithms and correctness proofs.", "ref_function": ["background", "background"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "1662376798", "abstract": "We consider deterministic terminating exploration of a grid by a team of asynchronous oblivious robots. We first consider the semi-synchronous atomic model ATOM. In this model, we exhibit the minimal number of robots to solve the problem w.r.t. the size of the grid. We then consider the asynchronous non-atomic model CORDA. ATOM being strictly stronger than CORDA, the previous bounds also hold in CORDA, and we propose deterministic algorithms in CORDA that matches these bounds. The above results show that except in two particular cases, 3 robots are necessary and sufficient to deterministically explore a grid of at least three nodes. The optimal number of robots for the two remaining cases is: 4 for the (2,2)-Grid and 5 for the (3,3)-Grid, respectively.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "2963504389", "abstract": "Recent advances in Distributed Computing highlight models and algorithms for autonomous swarms of mobile robots that self-organise and cooperate to solve global objectives. The overwhelming majority of works so far considers handmade algorithms and proofs of correctness. This paper builds upon a previously proposed formal framework to certify the correctness of impossibility results regarding distributed algorithms that are dedicated to autonomous mobile robots evolving in a continuous space. As a case study, we consider the problem of gathering all robots at a particular location, not known beforehand. A fundamental (but not yet formally certified) result, due to Suzuki and Yamashita, states that this simple task is impossible for two robots executing deterministic code and initially located at distinct positions. Not only do we obtain a certified proof of the original impossibility result, we also get the more general impossibility of gathering with an even number of robots, when any two robots are possibly initially at the same exact location.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2782960731", "abstract": "Swarms of mobile robots recently attracted the focus of the Distributed Computing community. One of the fundamental problems in this context is that of exploration: the robots must coordinate to visit all locations that are reachable from their initial positions. Despite its apparent simplicity, this problem proved quite hard to characterise fully, due to many model variants, leading to informal error-prone reasoning. Over the past few years, a significant effort permitted to set up a formal framework, relying on the Coq proof assistant, which was used to provide certified results when robots evolve in a continuous bi-dimensional Euclidean space. However, the most challenging issues with exploration arise in the discrete setting (a.k.a. graph), where locations are modeled as vertices and where edges between vertices denote the ability for a robot to move from one location to the next. We present a formal model to tackle problems and reason about robot algorithms arising in the discrete setting. Our approach extends and generalises previous research efforts focusing on the continuous model. As case studies, we consider fundamental impossibility results for exploration with stop in the discrete model. To our knowledge, those are the first certified results in this context. This framework paves the way for a general certification workflow dedicated to mobile robots on graphs.", "ref_function": ["background", "background", "background", "method", "other", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2338820189", "abstract": "Mobile robot networks emerged in the past few years as a promising distributed computing model. Existing work in the literature typically ensures the correctness of mobile robot protocols via ad hoc handwritten proofs, which, in the case of asynchronous execution models, are both cumbersome and error-prone. Our contribution is twofold. We first propose a formal model to describe mobile robot protocols operating in a discrete space i.e., with a finite set of possible robot positions, under synchrony and asynchrony assumptions. We translate this formal model into the DVE language, which is the input format of the model-checkers DiVinE and ITS tools, and formally prove the equivalence of the two models. We then verify several instances of two existing protocols for variants of the ring exploration in an asynchronous setting: exploration with stop and perpetual exclusive exploration. For the first protocol we refine the correctness bounds and for the second one, we exhibit a counter-example. This protocol is then modified and we establish the correctness of the new version with an inductive proof.", "ref_function": ["background", "background", "objective", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2538025500", "abstract": "The model of autonomous oblivious and anonymous mobile robots recently emerged as an attractive distributed computing abstraction that permits to assess the intrinsic difficulties of many fundamentals tasks, such as exploring or gathering in a discrete space. We present and implement a generic method for obtaining all possible protocols for a swarm of mobile robots operating in a particular discrete space. We use the exclusive perpetual exploration of anonymous rings as a case study. Our method permits to discover new protocols that solve the problem, and to assess specific optimization criteria (such as individual coverage, visits frequency, etc.) that are met by those protocols. To our knowledge, this is the first attempt to mechanize the discovery and fine-grained property testing of distributed mobile robot protocols.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Deep learning has significantly advanced state-of-the-art for hand pose estimation.", "The general trend has been the development of ever deeper and more sophisticated neural network architectures @cite_13 @cite_37 @cite_54 @cite_62 @cite_49 @cite_5 @cite_55 .", "However, such progress has also hinged on the availability of large amounts of annotated data @cite_51 @cite_57 @cite_50 .", "Obtaining accurate annotations, even for simple 3D joint coordinates, is extremely difficult and time consuming.", "Annotations generated by manually initializing trackers @cite_51 @cite_52 require carefully designed interfaces for 3D annotation on a 2D screen and there is often little consensus between human annotators @cite_58 .", "Motion-capture rigs @cite_50 and auxiliary sensors @cite_57 are fully automatic but are limited in the scenes in which they can be deployed.", "To mitigate the limitations of annotation, semi-supervised approaches @cite_7 @cite_56 @cite_10 and approaches coupling synthesized with real data @cite_0 @cite_59 @cite_31 have also been proposed."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken"], "target_paper": "We present a method for recovering the dense 3D surface of the hand by regressing the vertex coordinates of a mesh model from a single depth map. To this end, we use a two-stage 2D fully convolutional network architecture. In the first stage, the network estimates a dense correspondence field for every pixel on the depth map or image grid to the mesh grid. In the second stage, we design a differentiable operator to map features learned from the previous stage and regress a 3D coordinate map on the mesh grid. Finally, we sample from the mesh grid to recover the mesh vertices, and fit it an articulated template mesh in closed form. During inference, the network can predict all the mesh vertices, transformation matrices for every joint and the joint coordinates in a single forward pass. When given supervision on the sparse key-point coordinates, our method achieves state-of-the-art accuracy on NYU dataset for key point localization while recovering mesh vertices and a dense correspondence map. Our framework can also be learned through self-supervision by minimizing a set of data fitting and kinematic prior terms. With multi-camera rig during training to resolve self-occlusion, it can perform competitively with strongly supervised methods Without any human annotation.", "reference": {"@cite_37": {"mid": "2963119249", "abstract": "DeepPrior [18] is a simple approach based on Deep Learning that predicts the joint 3D locations of a hand given a depth map. Since its publication early 2015, it has been outperformed by several impressive works. Here we show that with simple improvements: adding ResNet layers, data augmentation, and better initial hand localization, we achieve better or similar performance than more sophisticated recent methods on the three main benchmarks (NYU, ICVL, MSRA) while keeping the simplicity of the original method. Our new implementation is available at https: github.com moberweger deep-prior-pp.", "ref_function": ["background", "background", "method", "other"], "cite_purpose": ["background"]}, "@cite_62": {"mid": "2896229066", "abstract": "Convolutional Neural Networks (CNNs)-based methods for 3D hand pose estimation with depth cameras usually take 2D depth images as input and directly regress holistic 3D hand pose. Different from these methods, our proposed Point-to-Point Regression PointNet directly takes the 3D point cloud as input and outputs point-wise estimations, i.e., heat-maps and unit vector fields on the point cloud, representing the closeness and direction from every point in the point cloud to the hand joint. The point-wise estimations are used to infer 3D joint locations with weighted fusion. To better capture 3D spatial information in the point cloud, we apply a stacked network architecture for PointNet with intermediate supervision, which is trained end-to-end. Experiments show that our method can achieve outstanding results when compared with state-of-the-art methods on three challenging hand pose datasets.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2962956488", "abstract": "We propose a simple and efficient method for exploiting synthetic images when training a Deep Network to predict a 3D pose from an image. The ability of using synthetic images for training a Deep Network is extremely valuable as it is easy to create a virtually infinite training set made of such images, while capturing and annotating real images can be very cumbersome. However, synthetic images do not resemble real images exactly, and using them for training can result in suboptimal performance. It was recently shown that for exemplar-based approaches, it is possible to learn a mapping from the exemplar representations of real images to the exemplar representations of synthetic images. In this paper, we show that this approach is more general, and that a network can also be applied after the mapping to infer a 3D pose: At run-time, given a real image of the target object, we first compute the features for the image, map them to the feature space of synthetic images, and finally use the resulting features as input to another network which predicts the 3D pose. Since this network can be trained very effectively by using synthetic images, it performs very well in practice, and inference is faster and more accurate than with an exemplar-based approach. We demonstrate our approach on the LINEMOD dataset for 3D object pose estimation from color images, and the NYU dataset for 3D hand pose estimation from depth maps. We show that it allows us to outperform the state-of-the-art on both datasets.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2963377353", "abstract": "State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose modelling the statistical relationship of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose or into a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To prevent over-fitting and to better exploit unlabeled depth maps, the generator and discriminator are trained jointly. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized samples and unlabeled depth maps. The proposed discriminator network architecture is highly efficient and runs at 90fps on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2964210253", "abstract": "The labeled data required to learn pose estimation for articulated objects is difficult to provide in the desired quantity, realism, density, and accuracy. To address this issue, we develop a method to learn representations, which are very specific for articulated poses, without the need for labeled training data. We exploit the observation that the object pose of a known object is predictive for the appearance in any known view. That is, given only the pose and shape parameters of a hand, the hand's appearance from any viewpoint can be approximated. To exploit this observation, we train a model that - given input from one view - estimates a latent representation, which is trained to be predictive for the appearance of the object when captured from another viewpoint. Thus, the only necessary supervision is the second view. The training process of this model reveals an implicit pose representation in the latent space. Importantly, at test time the pose representation can be inferred using only a single view. In qualitative and quantitative experiments we show that the learned representations capture detailed pose information. Moreover, when training the proposed method jointly with labeled and unlabeled data, it consistently surpasses the performance of its fully supervised counterpart, while reducing the amount of needed labeled samples by at least one order of magnitude.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_54": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_55": {"mid": "2963950354", "abstract": "We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-the-art methods based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D directional vector fields. The 2D 3D joint heat maps and 3D joint offsets are estimated via multitask network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes while enforcing consensus between the the estimated 3D pose and the pixel-wise 2D and 3D estimations by design. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-the-art approaches by a large margin. On the ICVL hand dataset, our method achieves similar accuracy compared to the nearly saturated result obtained by [5] and outperforms various other proposed methods. Code is available online1.", "ref_function": ["background", "method", "method", "method", "result", "background", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_52": {"mid": "2963577185", "abstract": "While many recent hand pose estimation methods critically rely on a training set of labelled frames, the creation of such a dataset is a challenging task that has been overlooked so far. As a result, existing datasets are limited to a few sequences and individuals, with limited accuracy, and this prevents these methods from delivering their full potential. We propose a semi-automated method for efficiently and accurately labeling each frame of a hand depth video with the corresponding 3D locations of the joints: The user is asked to provide only an estimate of the 2D reprojections of the visible joints in some reference frames, which are automatically selected to minimize the labeling work by efficiently optimizing a sub-modular loss function. We then exploit spatial, temporal, and appearance constraints to retrieve the full 3D poses of the hand over the complete sequence. We show that this data can be used to train a recent state-of-the-art hand pose estimation method, leading to increased accuracy.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_56": {"mid": "2892644985", "abstract": "Compared with depth-based 3D hand pose estimation, it is more challenging to infer 3D hand pose from monocular RGB images, due to substantial depth ambiguity and the difficulty of obtaining fully-annotated training data. Different from existing learning-based monocular RGB-input approaches that require accurate 3D annotations for training, we propose to leverage the depth images that can be easily obtained from commodity RGB-D cameras during training, while during testing we take only RGB inputs for 3D joint predictions. In this way, we alleviate the burden of the costly 3D annotations in real-world dataset. Particularly, we propose a weakly-supervised method, adaptating from fully-annotated synthetic dataset to weakly-labeled real-world dataset with the aid of a depth regularizer, which generates depth maps from predicted 3D pose and serves as weak supervision for 3D pose regression. Extensive experiments on benchmark datasets validate the effectiveness of the proposed depth regularizer in both weakly-supervised and fully-supervised settings.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_57": {"mid": "2606965392", "abstract": "In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.", "ref_function": ["background", "background", "method", "method", "result", "result", "result"], "cite_purpose": ["motivation", "background", "motivation"]}, "@cite_0": {"mid": "2963709863", "abstract": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.", "ref_function": ["background", "background", "objective", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_49": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_50": {"mid": "2963488642", "abstract": "We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["motivation", "background", "motivation"]}, "@cite_51": {"mid": "2075156252", "abstract": "We present a novel method for real-time continuous pose recovery of markerless complex articulable objects from a single depth image. Our method consists of the following stages: a randomized decision forest classifier for image segmentation, a robust method for labeled dataset generation, a convolutional network for dense feature extraction, and finally an inverse kinematics stage for stable real-time pose recovery. As one possible application of this pipeline, we show state-of-the-art results for real-time puppeteering of a skinned hand-model.", "ref_function": ["background", "method", "result"], "cite_purpose": ["motivation", "background", "background"]}, "@cite_5": {"mid": "2799191197", "abstract": "Convolutional Neural Network (CNN) has shown promising results for 3D hand pose estimation in depth images. Different from existing CNN-based hand pose estimation methods that take either 2D images or 3D volumes as the input, our proposed Hand PointNet directly processes the 3D point cloud that models the visible surface of the hand for pose regression. Taking the normalized point cloud as the input, our proposed hand pose regression network is able to capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose. In order to further improve the accuracy of fingertips, we design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location. Experiments on three challenging hand pose datasets show that our proposed method outperforms state-of-the-art methods.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_59": {"mid": "2901908207", "abstract": "Data labeling for learning 3D hand pose estimation models is a huge effort. Readily available, accurately labeled synthetic data has the potential to reduce the effort. However, to successfully exploit synthetic data, current state-of-the-art methods still require a large amount of labeled real data. In this work, we remove this requirement by learning to map from the features of real data to the features of synthetic data mainly using a large amount of synthetic and unlabeled real data. We exploit unlabeled data using two auxiliary objectives, which enforce that (i) the mapped representation is pose specific and (ii) at the same time, the distributions of real and synthetic data are aligned. While pose specifity is enforced by a self-supervisory signal requiring that the representation is predictive for the appearance from different views, distributions are aligned by an adversarial term. In this way, we can significantly improve the results of the baseline system, which does not use unlabeled data and outperform many recent approaches already with about 1 of the labeled real data. This presents a step towards faster deployment of learning based hand pose estimation, making it accessible for a larger range of applications.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_58": {"mid": "2214145768", "abstract": "Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.", "ref_function": ["background", "background", "method", "method", "result", "result", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2750326862", "abstract": "Abstract Hand pose estimation from single depth images is an essential topic in computer vision and human computer interaction. Despite recent advancements in this area promoted by convolutional neural networks, accurate hand pose estimation is still a challenging problem. In this paper we propose a novel approach named as pose guided structured region ensemble network (Pose-REN) to boost the performance of hand pose estimation. Under the guidance of an initially estimated pose, the proposed method extracts regions from the feature maps of convolutional neural network and generates more optimal and representative features for hand pose estimation. The extracted feature regions are then integrated hierarchically according to the topology of hand joints by tree-structured fully connections to regress the refined hand pose. The final hand pose is obtained by an iterative cascaded method. Comprehensive experiments on public hand pose datasets demonstrate that our proposed method outperforms state-of-the-art algorithms.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Person re-ID has been widely studied in the literature.", "Most of the existing methods @cite_17 @cite_14 @cite_21 @cite_10 @cite_12 @cite_4 @cite_16 @cite_15 @cite_8 @cite_9 @cite_5 focus on tackling the challenges of matching images with viewpoint and pose variations, or those with background clutter or occlusion presented.", "For example, Liu al @cite_16 develop a pose-transferable GAN-based @cite_13 framework to address image pose variations.", "Chen al @cite_9 integrate the conditional random field (CRF) with deep neural networks to learn more consistent multi-scale similarity metrics.", "The DaRe @cite_19 combines the feature embeddings extracted from different convolutional layers into a single embedding to train the model in a supervised fashion.", "Several attention-based methods @cite_10 @cite_4 @cite_8 are further proposed to focus on learning the discriminative parts to mitigate the effect of background clutter.", "While promising results have been presented, the above approaches typically assume that all images (both query and gallery) are of the same (or similar) resolution, which might not be practical in real-world re-ID applications.", "As a computer can only model a finite set of locations, a continuous 2D Euclidean space cannot be expressed in this model."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "Person re-identification (re-ID) solves the task of matching images across cameras and is among the research topics in vision community. Since query images in real-world scenarios might suffer from resolution loss, how to solve the resolution mismatch problem during person re-ID becomes a practical problem. Instead of applying separate image super-resolution models, we propose a novel network architecture of Resolution Adaptation and re-Identification Network (RAIN) to solve cross-resolution person re-ID. Advancing the strategy of adversarial learning, we aim at extracting resolution-invariant representations for re-ID, while the proposed model is learned in an end-to-end training fashion. Our experiments confirm that the use of our model can recognize low-resolution query images, even if the resolution is not seen during training. Moreover, the extension of our model for semi-supervised re-ID further confirms the scalability of our proposed method for real-world scenarios and applications.", "reference": {"@cite_13": {"mid": "2099471712", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_14": {"mid": "2604463754", "abstract": "Abstract Person re-identification (re-ID) and attribute recognition share a common target at learning pedestrian descriptions. Their difference consists in the granularity. Most existing re-ID methods only take identity labels of pedestrians into consideration. However, we find the attributes, containing detailed local descriptions, are beneficial in allowing the re-ID model to learn more discriminative feature representations. In this paper, based on the complementarity of attribute labels and ID labels, we propose an attribute-person recognition (APR) network, a multi-task network which learns a re-ID embedding and at the same time predicts pedestrian attributes. We manually annotate attribute labels for two large-scale re-ID datasets, and systematically investigate how person re-ID and attribute recognition benefit from each other. In addition, we re-weight the attribute predictions considering the dependencies and correlations among the attributes. The experimental results on two large-scale re-ID benchmarks demonstrate that by learning a more discriminative representation, APR achieves competitive re-ID performance compared with the state-of-the-art methods. We use APR to speed up the retrieval process by ten times with a minor accuracy drop of 2.92 on Market-1501. Besides, we also apply APR on the attribute recognition task and demonstrate improvement over the baselines.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}, "@cite_8": {"mid": "2798775284", "abstract": "Person Re-identification (ReID) is an important yet challenging task in computer vision. Due to the diverse background clutters, variations on viewpoints and body poses, it is far from solved. How to extract discriminative and robust features invariant to background clutters is the core problem. In this paper, we first introduce the binary segmentation masks to construct synthetic RGB-Mask pairs as inputs, then we design a mask-guided contrastive attention model (MGCAM) to learn features separately from the body and background regions. Moreover, we propose a novel region-level triplet loss to restrain the features learnt from different regions, i.e., pulling the features from the full image and body region close, whereas pushing the features from backgrounds away. We may be the first one to successfully introduce the binary mask into person ReID task and the first one to propose region-level contrastive learning. We evaluate the proposed method on three public datasets, including MARS, Market-1501 and CUHK03. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. Mask and code will be released upon request.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_9": {"mid": "2798874329", "abstract": "Person re-identification benefits greatly from deep neural networks (DNN) to learn accurate similarity metrics and robust feature embeddings. However, most of the current methods impose only local constraints for similarity learning. In this paper, we incorporate constraints on large image groups by combining the CRF with deep neural networks. The proposed method aims to learn the \"local similarity\" metrics for image pairs while taking into account the dependencies from all the images in a group, forming \"group similarities\". Our method involves multiple images to model the relationships among the local and global similarities in a unified CRF during training, while combines multi-scale local similarities as the predicted similarity in testing. We adopt an approximate inference scheme for estimating the group similarity, enabling end-to-end training. Extensive experiments demonstrate the effectiveness of our model that combines DNN and CRF for learning robust multi-scale local similarities. The overall results outperform those by state-of-the-arts with considerable margins on three widely-used benchmarks.", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_21": {"mid": "2796364723", "abstract": "Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually, local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that by employing a yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 by 17 in mAP and 6 in rank-1, CUHK03 by 4 in rank-1 and DukeMTMC-reID by 24 in mAP and 10 in rank-1.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2798458055", "abstract": "Person re-identification aims at finding a person of interest in an image gallery by comparing the probe image of this person with all the gallery images. It is generally treated as a retrieval problem, where the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. However, most existing methods only consider P2G affinities but ignore the affinities between all the gallery images (G2G affinity). Some frameworks incorporated G2G affinities into the testing process, which is not end-to-end trainable for deep neural networks. In this paper, we propose a novel group-shuffling random walk network for fully utilizing the affinity information between gallery images in both the training and testing processes. The proposed approach aims at end-to-end refining the P2G affinities based on G2G affinity information with a simple yet effective matrix operation, which can be integrated into deep neural networks. Feature grouping and group shuffle are also proposed to apply rich supervisions for learning better person features. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets by large margins, which demonstrate the effectiveness of our approach.", "ref_function": ["background", "background", "background", "method", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2798429327", "abstract": "Person re-identification (ReID) is an important task in the field of intelligent security. A key challenge is how to capture human pose variations, while existing benchmarks (i.e., Market1501, DukeMTMC-reID, CUHK03, etc.) do NOT provide sufficient pose coverage to train a robust ReID system. To address this issue, we propose a pose-transferrable person ReID framework which utilizes posetransferred sample augmentations (i.e., with ID supervision) to enhance ReID model training. On one hand, novel training samples with rich pose variations are generated via transferring pose instances from MARS dataset, and they are added into the target dataset to facilitate robust training. On the other hand, in addition to the conventional discriminator of GAN (i.e., to distinguish between REAL FAKE samples), we propose a novel guider sub-network which encourages the generated sample (i.e., with novel pose) towards better satisfying the ReID loss (i.e., cross-entropy ReID loss, triplet ReID loss). In the meantime, an alternative optimization procedure is proposed to train the proposed Generator-Guider-Discriminator network. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 show that our method achieves great performance improvement, and outperforms most state-of-the-art methods without elaborate designing the ReID model.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_10": {"mid": "2795013471", "abstract": "Typical person re-identification (ReID) methods usually describe each pedestrian with a single feature vector and match them in a task-specific metric space. However, the methods based on a single feature vector are not sufficient enough to overcome visual ambiguity, which frequently occurs in real scenario. In this paper, we propose a novel end-to-end trainable framework, called Dual ATtention Matching network (DuATM), to learn context-aware feature sequences and perform attentive sequence comparison simultaneously. The core component of our DuATM framework is a dual attention mechanism, in which both intra-sequence and inter-sequence attention strategies are used for feature refinement and feature-pair alignment, respectively. Thus, detailed visual cues contained in the intermediate feature sequences can be automatically exploited and properly compared. We train the proposed DuATM network as a siamese network via a triplet loss assisted with a de-correlation loss and a cross-entropy loss. We conduct extensive experiments on both image and video based ReID benchmark datasets. Experimental results demonstrate the significant advantages of our approach compared to the state-of-the-art methods.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_12": {"mid": "2962706983", "abstract": "Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["This paper contributes to the analysis of data that usually would be treated as a time series of spatially aggregated values.", "A good overview of visualization techniques for time-dependent data can be found in a book by @cite_57 .", "They focus primarily on time-dependent data in general, without specifically addressing spatial localization or scale-space approaches.", "@cite_28 proposed an analysis based on multivariate trend identification in time dependent data without spatial dependence.", "@cite_9 explore the use of parallel coordinates for the visualization of trajectories in multi-dimensional dynamical systems."], "label": ["Describing the objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.", "reference": {"@cite_57": {"mid": "1576209423", "abstract": "Time is an exceptional dimension that is common to many application domains such as medicine, engineering, business, or science. Due to the distinct characteristics of time, appropriate visual and analytical methods are required to explore and analyze them. This book starts with an introduction to visualization and historical examples of visual representations. At its core, the book presents and discusses a systematic view of the visualization of time-oriented data along three key questions: what is being visualized (data), why something is visualized (user tasks), and how it is presented (visual representation). To support visual exploration, interaction techniques and analytical methods are required that are discussed in separate chapters. A large part of this book is devoted to a structured survey of 101 different visualization techniques as a reference for scientists conducting related research as well as for practitioners seeking information on how their time-oriented data can best be visualized.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2042862761", "abstract": "In recent years scientific visualization has been driven by the need to visualize high-dimensional data sets within high-dimensional spaces. However most visualization methods are designed to show only some statistical features of the data set. The paper deals with the visualization of trajectories of high-dimensional dynamical systems which form a L sub n sup n data set of a smooth n-dimensional flow. Three methods that are based on the idea of parallel coordinates are presented and discussed. Visualizations done with these new methods are shown and an interactive visualization tool for the exploration of high-dimensional dynamical systems is proposed.", "ref_function": ["background", "background", "objective", "method", "method"], "cite_purpose": ["background"]}, "@cite_28": {"mid": "2105832013", "abstract": "We present a new algorithm to explore and visualize multivariate time-varying data sets. We identify important trend relationships among the variables based on how the values of the variables change over time and how those changes are related to each other in different spatial regions and time intervals. The trend relationships can be used to describe the correlation and causal effects among the different variables. To identify the temporal trends from a local region, we design a new algorithm called SUBDTW to estimate when a trend appears and vanishes in a given time series. Based on the beginning and ending times of the trends, their temporal relationships can be modeled as a state machine representing the trend sequence. Since a scientific data set usually contains millions of data points, we propose an algorithm to extract important trend relationships in linear time complexity. We design novel user interfaces to explore the trend relationships, to visualize their temporal characteristics, and to display their spatial distributions. We use several scientific data sets to test our algorithm and demonstrate its utilities.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Our approach is based on the concept of the space-time cube, coined by H \"a gerstrand @cite_16 in 1970.", "Since then it was repeatedly used, either explicitly or as an underlying concept.", "Recently, @cite_35 presented a useful overview of related techniques.", "They describe the theoretical concept of a generalized space-time cube together with a taxonomy of all elementary space-time operations and their combinations that can be performed on such a space-time cube.", "A more abstract approach to time-dependent volume data analysis has been proposed by @cite_41 , based on hyperplane slicing of a four-dimensional space-time hypercube.", "Other examples of slicing higher-dimensional data (not necessarily time dependent) include Sliceplorer @cite_0 , Hyperslice @cite_19 , Hypersliceplorer @cite_26 and HyperMoVal @cite_27 ."], "label": ["Describing used methods", "Reference to current state of knowledge", "Other reference purpose", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.", "reference": {"@cite_35": {"mid": "2393302563", "abstract": "We present the generalized space-time cube, a descriptive model for visualizations of temporal data. Visualizations are described as operations on the cube, which transform the cube's 3D shape into readable 2D visualizations. Operations include extracting subparts of the cube, flattening it across space or time or transforming the cubes geometry and content. We introduce a taxonomy of elementary space-time cube operations and explain how these operations can be combined and parameterized. The generalized space-time cube has two properties: 1 it is purely conceptual without the need to be implemented, and 2 it applies to all datasets that can be represented in two dimensions plus time e.g. geo-spatial, videos, networks, multivariate data. The proper choice of space-time cube operations depends on many factors, for example, density or sparsity of a cube. Hence, we propose a characterization of structures within space-time cubes, which allows us to discuss strengths and limitations of operations. We finally review interactive systems that support multiple operations, allowing a user to customize his view on the data. With this framework, we hope to facilitate the description, criticism and comparison of temporal data visualizations, as well as encourage the exploration of new techniques and systems. This paper is an extension of\uffbfBach et\uffbfal.'s 2014 work.", "ref_function": ["background", "background", "method", "method", "method", "result", "background", "objective", "method", "objective", "other", "other"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2815781823", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_41": {"mid": "2097621063", "abstract": "We present an alternative method for viewing time-varying volumetric data. We consider such data as a four-dimensional data field, rather than considering space and time as separate entities. If we treat the data in this manner, we can apply high dimensional slicing and projection techniques to generate an image hyperplane. The user is provided with an intuitive user interface to specify arbitrary hyperplanes in 4D, which can be displayed with standard volume rendering techniques. From the volume specification, we are able to extract arbitrary hyperslices, combine slices together into a hyperprojection volume, or apply a 4D raycasting method to generate the same results. In combination with appropriate integration operators and transfer functions, we are able to extract and present different space-time features to the user.", "ref_function": ["method", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2626618469", "abstract": "Multi-dimensional continuous functions are commonly visualized with 2D slices or topological views. Here, we explore 1D slices as an alternative approach to show such functions. Our goal with 1D slices is to combine the benefits of topological views, that is, screen space efficiency, with those of slices, that is a close resemblance of the underlying function. We compare 1D slices to 2D slices and topological views, first, by looking at their performance with respect to common function analysis tasks. We also demonstrate 3 usage scenarios: the 2D sinc function, neural network regression, and optimization traces. Based on this evaluation, we characterize the advantages and drawbacks of each of these approaches, and show how interaction can be used to overcome some of the shortcomings.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2103111128", "abstract": "HyperSlice is a new method for the visualization of scalar functions of many variables. With this method the multi-dimensional function is presented in a simple and easy to understand way in which all dimensions are treated identically. The central concept is the representation of a multi-dimensional function as a matrix of orthogonal two-dimensional slices. These two-dimensional slices lend themselves very well to interaction via direct manipulation, due to a one to one relation between screen space and variable space. Several interaction techniques, for navigation, the location of maxima, and the use of user-defined paths, are presented.", "ref_function": ["background", "background", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "1979686514", "abstract": "During the development of car engines, regression models that are based on machine learning techniques are increasingly important for tasks which require a prediction of results in real-time. While the validation of a model is a key part of its identification process, existing computation- or visualization-based techniques do not adequately support all aspects of model validation. The main contribution of this paper is an interactive approach called HyperMoVal that is designed to support multiple tasks related to model validation: 1) comparing known and predicted results, 2) analyzing regions with a bad fit, 3) assessing the physical plausibility of models also outside regions covered by validation data, and 4) comparing multiple models. The key idea is to visually relate one or more n-dimensional scalar functions to known validation data within a combined visualization. HyperMoVal lays out multiple 2D and 3D sub-projections of the n-dimensional function space around a focal point. We describe how linking HyperMoVal to other views further extends the possibilities for model validation. Based on this integration, we discuss steps towards supporting the entire workflow of identifying regression models. An evaluation illustrates a typical workflow in the application context of car-engine design and reports general feedback of domain experts and users of our approach. These results indicate that our approach significantly accelerates the identification of regression models and increases the confidence in the overall engineering process.", "ref_function": ["background", "background", "objective", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2133649345", "abstract": "The described locking mechanism permits two or more rods to be quickly fitted together and adjusted to a fixed, predetermined length. The locking mechanism is of a type such that the rotation of the rods by approximately 45 DEG -90 DEG automatically fixes the rods in place.", "ref_function": ["background", "background"], "cite_purpose": ["uses"]}}}
{"sentences": ["Space reformation techniques are valuable tools for gaining insight into the spatial structure of the data.", "Recent publications present methods for a decomposition using a space-filling curve in MotionRugs @cite_53 and Dynamic Volume Lines @cite_1 .", "These works transform continuous volume data into one-dimensional representations by following a space-filling curve through the volume.", "As this kind of space reformation suffers from delocalization , where points close to each other can end up far apart in the representation, @cite_30 tried to overcome this limitation by context-based space-filling curves.", "A space-filling curve is not applicable to our case as it does not provide the desired aggregation required for the statistical treatment of particle data.", "Several works address space reformation techniques for volume data, of which we highlight two: A general approach to volume transformation by @cite_6 , via the use of spatial transfer functions for 3D volume warping, and a curved planar reformation by @cite_17 , that was used in medical visualization.", "For more examples, @cite_38 provide a survey of flattening based techniques in medical visualization."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken", "Other reference purpose"], "target_paper": "Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.", "reference": {"@cite_30": {"mid": "2146432373", "abstract": "A context-based scanning technique for images is presented. An image is scanned along a context-based space filling curve that is computed so as to exploit inherent coherence in the image. The resulting one-dimensi onal representation of the image has improved autocorrelation compared with universal scans such as the PeanoHilbert space filling curve. An efficient algorithm for computing context-based space filling curves is presented. We also discuss the potential of improved autocorrelation of context-based space filling curves for image and video lossless compression.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_38": {"mid": "2860633713", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_53": {"mid": "2888207115", "abstract": "Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.", "ref_function": ["background", "background", "objective", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2888322672", "abstract": "The comparison of many members of an ensemble is difficult, tedious, and error-prone, which is aggravated by often just subtle differences. In this paper, we introduce Dynamic Volume Lines for the interactive visual analysis and comparison of sets of 3D volumes. Each volume is linearized along a Hilbert space-filling curve into a 1D Hilbert line plot, which depicts the intensities over the Hilbert indices. We present a nonlinear scaling of these 1D Hilbert line plots based on the intensity variations in the ensemble of 3D volumes, which enables a more effective use of the available screen space. The nonlinear scaling builds the basis for our interactive visualization techniques. An interactive histogram heatmap of the intensity frequencies serves as overview visualization. When zooming in, the frequencies are replaced by detailed 1D Hilbert line plots and optional functional boxplots. To focus on important regions of the volume ensemble, nonlinear scaling is incorporated into the plots. An interactive scaling widget depicts the local ensemble variations. Our brushing and linking interface reveals, for example, regions with a high ensemble variation by showing the affected voxels in a 3D spatial view. We show the applicability of our concepts using two case studies on ensembles of 3D volumes resulting from tomographic reconstruction. In the first case study, we evaluate an artificial specimen from simulated industrial 3D X-ray computed tomography (XCT). In the second case study, a real-world XCT foam specimen is investigated. Our results show that Dynamic Volume Lines can identify regions with high local intensity variations, allowing the user to draw conclusions, for example, about the choice of reconstruction parameters. Furthermore, it is possible to detect ring artifacts in reconstructions volumes.", "ref_function": ["background", "objective", "method", "method", "method", "method", "result", "background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2159162381", "abstract": "In this paper, we introduce the concept of spatial transfer functions as a unified approach to volume modeling and animation. A spatial transfer function is a function that defines the geometrical transformation of a scalar field in space, and is a generalization and abstraction of a variety of deformation methods. It facilitates a field based representation, and can thus be embedded into a volumetric scene graph under the algebraic framework of constructive volume geometry. We show that when spatial transfer functions are treated as spatial objects, constructive operations and conventional transfer functions can be applied to such spatial objects. We demonstrate spatial transfer functions in action with the aid of a collection of examples in volume visualization, sweeping, deformation and animation. In association with these examples, we describe methods for modeling and realizing spatial transfer functions, including simple procedural functions, operational decomposition of complex functions, large scale domain decomposition and temporal spatial transfer functions. We also discuss the implementation of spatial transfer functions in the vlib API and our efforts in deploying the technique in volume animation.", "ref_function": ["background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2119570434", "abstract": "Traditional volume visualization techniques may provide incomplete clinical information needed for applications in medical visualization. In the area of vascular visualization important features such as the lumen of a diseased vessel segment may not be visible. Curved planar reformation (CPR) has proven to be an acceptable practical solution. Existing CPR techniques, however, still have diagnostically relevant limitations. In this paper, we introduce two advances methods for efficient vessel visualization, based on the concept of CPR. Both methods benefit from relaxation of spatial coherence in favor of improved feature perception. We present a new technique to visualize the interior of a vessel in a single image. A vessel is resampled along a spiral around its central axis. The helical spiral depicts the vessel volume. Furthermore, a method to display an entire vascular tree without mutually occluding vessels is presented. Minimal rotations at the bifurcations avoid occlusions. For each viewing direction the entire vessel structure is visible.", "ref_function": ["background", "background", "background", "method", "method", "method", "background", "background", "method", "method", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Substantial work has been done on the visualization of trajectory-based simulation data, including projects such as OVITO @cite_34 , Trillion Particles @cite_23 , Multiscale HIV @cite_13 .", "These are primarily concerned with the sheer volume of the data and its direct visualization, rather than the computation of derived properties and their temporal analysis."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: research objective"], "target_paper": "Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.", "reference": {"@cite_34": {"mid": "2047968138", "abstract": "The Open Visualization Tool (OVITO) is a new 3D visualization software designed for post-processing atomistic data obtained from molecular dynamics or Monte Carlo simulations. Unique analysis, editing and animations functions are integrated into its easy-to-use graphical user interface. The software is written in object-oriented C++, controllable via Python scripts and easily extendable through a plug-in interface. It is distributed as open-source software and can be downloaded from the website http: ovito.sourceforge.net .", "ref_function": ["background", "background", "method", "other"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2890964348", "abstract": "Abstract We provide a high-level survey of multiscale molecular visualization techniques, with a focus on application-domain questions, challenges, and tasks. We provide a general introduction to molecular visualization basics and describe a number of domain-specific tasks that drive this work. These tasks, in turn, serve as the general structure of the following survey. First, we discuss methods that support the visual analysis of molecular dynamics simulations. We discuss, in particular, visual abstraction and temporal aggregation. In the second part, we survey multiscale approaches that support the design, analysis, and manipulation of DNA nanostructures and related concepts for abstraction, scale transition, scale-dependent modeling, and navigation of the resulting abstraction spaces. In the third part of the survey, we showcase approaches that support interactive exploration within large structural biology assemblies up to the size of bacterial cells. We describe fundamental rendering techniques as well as approaches for element instantiation, visibility management, visual guidance, camera control, and support of depth perception. We close the survey with a brief listing of important tools that implement many of the discussed approaches and a conclusion that provides some research challenges in the field.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2125184461", "abstract": "Petascale plasma physics simulations have recently entered the regime of simulating trillions of particles. These unprecedented simulations generate massive amounts of data, posing significant challenges in storage, analysis, and visualization. In this paper, we present parallel I O, analysis, and visualization results from a VPIC trillion particle simulation running on 120,000 cores, which produces 30TB of data for a single timestep. We demonstrate the successful application of H5Part, a particle data extension of parallel HDF5, for writing the dataset at a significant fraction of system peak I O rates. To enable efficient analysis, we develop hybrid parallel FastQuery to index and query data using multi-core CPUs on distributed memory hardware. We show good scalability results for the FastQuery implementation using up to 10,000 cores. Finally, we apply this indexing query-driven approach to facilitate the first-ever analysis and visualization of the trillion particle dataset.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Focusing on the trajectories, hierarchical particle grouping for large datasets has been done by @cite_33 .", "@cite_43 extract prominent trajectories from large particle data.", "present a specialized tool for exploring Monte Carlo simulations of photo-voltaic cells."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.", "reference": {"@cite_43": {"mid": "2086600232", "abstract": "An effective means for flow visualization is the depiction of particle trajectories. When rendering large amounts of these pathlines, standard visualization techniques suffer from several weaknesses, ranging from ambiguous depth perception to high geometrical complexity and decreased interactivity. This paper addresses these problems by choosing a novel approach to pathline visualization in 3D space, which we call Virtual Tubelets. It employs billboarding techniques in combination with suitable textures to create the illusion of three-dimensional tubes, which efficiently depict the particles' trajectories, while still maintaining interactive frame rates. Certain issues concerning virtual environments and immersive displays with multiple projection screens are resolved by choosing an appropriate orientation for the billboards. The use of modern, programmable graphics hardware allows for an additional speed-up of the rendering process and a further improvement of the image quality. This results in a nearly perfect illusion of tubular geometry, including plausible intersections and consistent illumination with the rest of the scene. To prove the efficiency of our approach, rendering speed and visual quality of Virtual Tubelets and conventional, polygonal tube renderings are compared.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "1967509282", "abstract": "Interactive visualization of large particle sets is required to analyze the complicated structures and formation processes in astrophysical particle simulations. While some research has been done on the development of visualization techniques for steady particle fields, only very few approaches have been proposed to interactively visualize large time-varying fields and their dynamics. Particle trajectories are known to visualize dynamic processes over time, but due to occlusion and visual cluttering such techniques have only been reported for very small particle sets so far. In this paper we present a novel technique to solve these problems, and we demonstrate the potential of our approach for the visual exploration of large astrophysical particle sequences. We present a new hierarchical space-time data structure for particle sets which allows for a scale-space analysis of trajectories in the simulated fields. In combination with visualization techniques that adapt to the respective scales, clusters of particles with homogeneous motion as well as separation and merging regions can be identified effectively. The additional use of mapping functions to modulate the color and size of trajectories allows emphasizing various particle properties like direction, speed, or particle-specific attributes like temperature. Furthermore, tracking of interactively selected particle subsets permits the user to focus on structures of interest.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
