{"sentences": ["Two other generalizations that can be considered are invariants of graphs in 3-manifolds, and invariants associated to other flat connections @cite_16 .", "We will analyze these in future work.", "Among other things, there should be a general relation between flat bundles and links in 3-manifolds on the one hand and finite covers and branched covers on the other hand @cite_26 ."], "label": ["Reference to single investigations in the past:  about objective", "Signalling Transition", "Reference to single investigations in the past: about result"], "target_paper": "Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.", "reference": {"@cite_16": {"mid": "1481005306", "abstract": "This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrod\u2013Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.", "ref_function": ["background", "objective"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "1641082372", "abstract": "Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.", "ref_function": ["background", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Model Based Dynamic Programming, hereafter referred to as MBDP-1 @cite_0 , is probably the most recent work that addresses the exact same issue as that considered in this paper.", "Both the approach presented in this paper and Brent's MBDP-1 are based on explicit probability models.", "Approaches not based on explicit probability models include those based on information theoretic criteria such as MDL , transitional probability or simple recurrent networks .", "The maximum likelihood approach due to Olivier:SGL68 is probabilistic in the sense that it is geared towards explicitly calculating the most probable segmentation of each block of input utterances.", "However, it is not based on a formal statistical model.", "To avoid needless repetition, we only describe Brent's MBDP-1 below and direct the interested reader at Brent:EPS99 which provides an excellent review of many of the algorithms mentioned above."], "label": ["Reference to single investigations in the past:  about objective", "Explaining the method relationship between own work and references", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Signalling Transition"], "target_paper": "A statistical model for segmentation and word discovery in child directed speech is presented. An incremental unsupervised learning algorithm to infer word boundaries based on this model is described and results of empirical tests showing that the algorithm is competitive with other models that have been used for similar tasks are also presented.", "reference": {"@cite_0": {"mid": "2074546930", "abstract": "This paper presents a model-based, unsupervised algorithm for recovering word boundaries in a natural-language text from which they have been deleted. The algorithm is derived from a probability model of the source that generated the text. The fundamental structure of the model is specified abstractly so that the detailed component models of phonology, word-order, and word frequency can be replaced in a modular fashion. The model yields a language-independent, prior probability distribution on all possible sequences of all possible words over a given alphabet, based on the assumption that the input was generated by concatenating words from a fixed but unknown lexicon. The model is unusual in that it treats the generation of a complete corpus, regardless of length, as a single event in the probability space. Accordingly, the algorithm does not estimate a probability distribution on wordss instead, it attempts to calculate the prior probabilities of various word sequences that could underlie the observed text. Experiments on phonemic transcripts of spontaneous speech by parents to young children suggest that our algorithm is more effective than other proposed algorithms, at least when utterance boundaries are given and the text includes a substantial number of short utterances.", "ref_function": ["background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Recently we were able to characterize the graphs that can occur at most @math times as a subgraph isomorph in an @math -vertex planar graph: they are exactly the 3-connected planar graphs @cite_41 .", "However our proof does not lead to an efficient algorithm for 3-connected planar subgraph isomorphism.", "In this paper we use different techniques which do not depend on high-order connectivity."], "label": ["Describing used methods", "Explaining the inadequacies of previous studies", "Describing used methods"], "target_paper": "We solve the subgraph isomorphism problem in planar graphs in linear time, for any pattern of constant size. Our results are based on a technique of partitioning the planar graph into pieces of small tree-width, and applying dynamic programming within each piece. The same methods can be used to solve other planar graph problems including connectivity, diameter, girth, induced subgraph isomorphism, and shortest paths.", "reference": {"@cite_41": {"mid": "2074992286", "abstract": "It is well known that any planar graph contains at most O(n) complete subgraphs. We extend this to an exact characterization: G occurs O(n) times as a subgraph of any planar graph, if and only if G is three-connected. We generalize these results to similarly characterize certain other minor-closed families of graphs; in particular, G occurs O(n) times as a subgraph of the Kb,c-free graphs, b \u2265 c and c \u2264 4, iff G is c-connected. Our results use a simple Ramsey-theoretic lemma that may be of independent interest. \u00a9 1993 John Wiley & Sons, Inc.", "ref_function": ["background", "background", "method", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["A further genuine and important approach to the spinor-tensor transition was developed starting probably with Crawford by P. Lounesto, @cite_6 and references there.", "He investigated the question, how a spinor field can be reconstructed from known tensor densities.", "The major characterization is derived, using Fierz-Kofink identities, from elements called Boomerangs --because they are able to come back to the spinorial picture.", "Lounesto's result is a characterization of spinors based on multi-vector relations which unveils a new unknown type of spinor."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Daviau showed the equivalence of matrix Dirac theory, formulated within a spinor bundle (S_x C _x^4 ), to a Clifford algebraic formulation within space Clifford algebra (C ( R ^3 , ) M _ 2 ( C ) P ) Pauli algebra (matrices) \u2243 \u210d \u2a01 \u210d \u2243 biquaternions. We will show, that Daviau's map \u03b8: ( : C ^4 M _ 2 ( C ) ) is an isomorphism. It is shown that Hestenes' and Parra's formulations are equivalent to Daviau's Clifford algebra formulation, which uses outer automorphisms. The connection between different formulations is quite remarkable, since it connects the left and right action on the Pauli algebra itself viewed as a bi-module with the left (resp. right) action of the enveloping algebra (P^ P P^T on P ). The isomorphism established in this article and given by Daviau's map does clearly show that right and left actions are of similar type. This should be compared with attempts of Hestenes, Daviau, and others to interprete the right action as the iso-spin freedom.", "reference": {"@cite_6": {"mid": "2082565556", "abstract": "A historical review of spinors is given together with a construction of spinor spaces as minimal left ideals of Clifford algebras. Spinor spaces of euclidean spaces over reals have a natural linear structure over reals, complex numbers or quaternions. Clifford algebras have involutions which induce bilinear forms or scalar products on spinor spaces. The automorphism groups of these scalar products of spinors are determined and also classified.", "ref_function": ["background", "background", "background", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Pioneering research in dynamic runtime optimization was done by Hansen @cite_8 who first described a fully automated system for runtime code optimization.", "His system was similar in structure to our system---it was composed of a loader, a profiler, and an optimizer---but used profiling data only to decide when to optimize and what to optimize, not how to optimize.", "Also, his system interpreted code prior to optimization, since load time code generation was too memory and time consuming at the time."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "We present an open architecture for just-in-time code generation and dynamic code optimization that is flexible, customizable, and extensible. While previous research has primarily investigated functional aspects of such a system, architectural aspects have so far remained unexplored. In this paper, we argue that these properties are important to generate optimal code for a variety of hardware architectures and different processor generations within processor families. These properties are also important to make system-level code generation useful in practice.", "reference": {"@cite_8": {"mid": "2101776604", "abstract": "Abstract : This thesis investigates adaptive compiler systems that perform, during program execution, code optimizations based on the dynamic behavior of the program as opposed to current approaches that employ a fixed code generation strategy, i.e., one in which a predetermined set of code optimizations are applied at compile-time to an entire program. The main problems associated with such adaptive systems are studied in general: which optimizations to apply to what parts of the program and when. Two different optimization strategies result: an ideal scheme which is not practical to implement, and a more basic scheme that is. The design of a practical system is discussed for the FORTRAN IV language. The system was implemented and tested with programs having different behavioral characteristics.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Hansen's work was followed by several other projects that have investigated the benefits of runtime optimization: the Smalltalk @cite_33 and SELF @cite_0 systems that focused on the benefits of dynamic optimization in an object-oriented environment; Morph'', a project developed at Harvard University @cite_16 ; and the system described by the authors of this paper @cite_4 @cite_30 .", "Other projects have experimented with optimization at link time rather than at runtime @cite_18 .", "At link time, many of the problems described in this paper are non-existent.", "Among them the decision when to optimize, what to optimize, and how to replace code.", "However, there is also a price to pay, namely that it cannot be performed in the presence of dynamic loading."], "label": ["General reference to previous research or scholarship: research objective", "Reference to single investigations in the past:  about objective", "Explaining the inadequacies of previous studies", "Not sure", "Explaining the inadequacies of previous studies"], "target_paper": "We present an open architecture for just-in-time code generation and dynamic code optimization that is flexible, customizable, and extensible. While previous research has primarily investigated functional aspects of such a system, architectural aspects have so far remained unexplored. In this paper, we argue that these properties are important to generate optimal code for a variety of hardware architectures and different processor generations within processor families. These properties are also important to make system-level code generation useful in practice.", "reference": {"@cite_30": {"mid": "1542872339", "abstract": "Despite the apparent success of the Java Virtual Machine, its lackluster performance makes it ill-suited for many speed-critical applications. Although the latest just-in-time compilers and dedicated Java processors try to remedy this situation, optimized code compiled directly from a C program source is still considerably faster than software transported via Java byte-codes. This is true even if the Java byte-codes are subsequently further translated into native code. In this paper, we claim that these performance penalties are not a necessary consequence of machine-independence, but related to Java's particular intermediate representation and runtime architecture. We have constructed a prototype and are further developing a software transportability scheme founded on a tree-based alternative to Java byte-codes. This tree-based intermediate representation is not only twice as compact as Java byte-codes, but also contains more high-level information, some of which is critical for advanced code optimizations. Our architecture not only provides on-the-fly code generation from this intermediate representation, but also continuous re-optimization of the existing code-base by a low-priority background process. The re-optimization process is guided by up-to-the-minute profiling data, leading to superior runtime performance.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2112378054", "abstract": "Modifying code after the compiler has generated it can be useful for both optimization and instrumentation. Several years ago we designed the Mahler system, which uses link-time code modification for a variety of tools on our experimental Titan workstations. Killian\u2019s Pixie tool works even later, translating a fully-linked MIPS executable file into a new version with instrumentation added. Recently we wanted to develop a hybrid of the two, that would let us experiment with both optimization and instrumentation on a standard workstation, preferably without requiring us to modify the normal compilers and linker. This paper describes prototypes of two hybrid systems, closely related to Mahler and Pixie. We implemented basic-block counting in both, and compare the resulting time and space expansion to those of Mahler and Pixie.", "ref_function": ["background", "background", "background", "objective", "objective", "method"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "1541680547", "abstract": "In the past few years, code optimization has become a major field of research. Many efforts have been undertaken to find new sophisticated algorithms that fully exploit the computing power of today's advanced microprocessors. Most of these algorithms do very well in statically linked, monolithic software systems, but perform perceptibly worse in extensible systems. The modular structure of these systems imposes a natural barrier for intermodular compile-time optimizations. In this paper we discuss a different approach in which optimization is no longer performed at compile-time, but is delayed until runtime. Reoptimized module versions are generated on-the-fly while the system is running, replacing earlier less optimized versions.", "ref_function": ["background", "background", "background", "background", "method", "method"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "1993318777", "abstract": "The Smalltalk-80* programming language includes dynamic storage allocation, full upward funargs, and universally polymorphic procedures; the Smalltalk-80 programming system features interactive execution with incremental compilation, and implementation portability. These features of modern programming systems are among the most difficult to implement efficiently, even individually. A new implementation of the Smalltalk-80 system, hosted on a small microprocessor-based computer, achieves high performance while retaining complete (object code) compatibility with existing implementations. This paper discusses the most significant optimization techniques developed over the course of the project, many of which are applicable to other languages. The key idea is to represent certain runtime state (both code and data) in more than one form, and to convert between forms when needed.", "ref_function": ["background", "background", "background", "method", "objective"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2167997514", "abstract": "Crossing abstraction boundaries often incurs a substantial run-time overhead in the form of frequent procedure calls. Thus, pervasive use of abstraction, while desirable from a design standpoint, may lead to very inefficient programs. Aggressively optimizing compilers can reduce this overhead but conflict with interactive programming environments because they introduce long compilation pauses and often preclude source-level debugging. Thus, programmers are caught on the horns of two dilemmas: they have to choose between abstraction and efficiency, and between responsive programming environments and efficiency. This dissertation shows how to reconcile these seemingly contradictory goals. Four new techniques work together to achieve this: - Type feedback achieves high performance by allowing the compiler to inline message sends based on information extracted from the runtime system. - Adaptive optimization achieves high responsiveness without sacrificing performance by using a fast compiler to generate initial code while automatically recompiling heavily used program parts with an optimizing compiler. - Dynamic deoptimization allows source-level debugging of optimized code by transparently recreating non-optimized code as needed. - Polymorphic inline caching speeds up message dispatch and, more significantly, collects concrete type information for the compiler. With better performance yet good interactive behavior, these techniques reconcile exploratory programming, ubiquitous abstraction, and high performance.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2153228154", "abstract": "The Morph system provides a framework for automatic collection and management of profile information and application of profile-driven optimizations. In this paper, we focus on the operating system support that is required to collect and manage profile information on an end-user's workstation in an automatic, continuous, and transparent manner. Our implementation for a Digital Alpha machine running Digital UNIX 4.0 achieves run-time overheads of less than 0.3 during profile collection. Through the application of three code layout optimizations, we further show that Morph can use statistical profiles to improve application performance. With appropriate system support, automatic profiling and optimization is both possible and effective.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Common to the above-mentioned work is that the main focus has always been on functional aspects, that is how to profile and which optimizations to perform.", "Related to this is research on how to boost application performance by combining profiling data and code optimizations at compile time (not at runtime), including work on method dispatch optimizations for object-oriented programming languages @cite_22 @cite_35 , profile-guided intermodular optimizations @cite_3 @cite_26 , code positioning techniques @cite_13 @cite_25 , and profile-guided data cache locality optimizations @cite_29 @cite_10 @cite_12 ."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: research objective"], "target_paper": "We present an open architecture for just-in-time code generation and dynamic code optimization that is flexible, customizable, and extensible. While previous research has primarily investigated functional aspects of such a system, architectural aspects have so far remained unexplored. In this paper, we argue that these properties are important to generate optimal code for a variety of hardware architectures and different processor generations within processor families. These properties are also important to make system-level code generation useful in practice.", "reference": {"@cite_35": {"mid": "2141293928", "abstract": "Polymorphic inline caches (PICs) provide a new way to reduce the overhead of polymorphic message sends by extending inline caches to include more than one cached lookup result per call site. For a set of typical object-oriented SELF programs, PICs achieve a median speedup of 11 .", "ref_function": ["background", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2030400507", "abstract": "SUMMARY This paper describes critical implementation issues that must be addressed to develop a fully automatic inliner. These issues are: integration into a compiler, program representation, hazard prevention, expansion sequence control, and program modification. An automatic inter-file inliner that uses profile information has been implemented and integrated into an optimizing C compiler. The experimental results show that this inliner achieves significant speedups for production C programs.", "ref_function": ["background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2164470257", "abstract": "The Smalltalk-80 system makes it possible to write programs quickly by providing object-oriented programming, incremental compilation, run-time type checking, use-extensible data types and control structures, and an interactive graphical interface. However, the potential savings in programming effort have been curtailed by poor performance in widely available computers or high processor cost. Smalltalk-80 systems pose tough challenges for implementors: dynamic data typing, a high-level instruction set, frequent and expensive procedure calls, and object-oriented storage management. The dissertation documents two results that run counter to conventional wisdom: that a reduced instruction set computer can offer excellent performance for a system with dynamic data typing such as Smalltalk-80, and that automatic storage reclamation need not be time-consuming. This project was sponsored by Defense Advance Research Projects Agency (DoD) ARPA Order No. 3803, monitored by Naval Electronic System Command under Contractor No. N00034-R-0251. It was also sponsored by Defense Advance Research Projects Agency (DoD) ARPA Order No. 4871, monitored by Naval Electronic Systems Command under Contract No. N00039-84-C-0089.", "ref_function": ["background", "background", "background", "result", "other", "other", "other", "other", "other", "other"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2117703621", "abstract": "The cost of accessing main memory is increasing. Machine designers have tried to mitigate the consequences of the processor and memory technology trends underlying this increasing gap with a variety of techniques to reduce or tolerate memory latency. These techniques, unfortunately, are only occasionally successful for pointer-manipulating programs. Recent research has demonstrated the value of a complementary approach, in which pointer-based data structures are reorganized to improve cache locality.This paper studies a technique for using a generational garbage collector to reorganize data structures to produce a cache-conscious data layout, in which objects with high temporal affinity are placed next to each other, so that they are likely to reside in the same cache block. The paper explains how to collect, with low overhead, real-time profiling information about data access patterns in object-oriented languages, and describes a new copying algorithm that utilizes this information to produce a cache-conscious object layout.Preliminary results show that this technique reduces cache miss rates by 21--42 , and improves program performance by 14--37 over Cheney's algorithm. We also compare our layouts against those produced by the Wilson-Lam-Moher algorithm, which attempts to improve program locality at the page level. Our cache-conscious object layouts reduces cache miss rates by 20--41 and improves program performance by 18--31 over their algorithm, indicating that improving locality at the page level is not necessarily beneficial at the cache level.", "ref_function": ["background", "background", "background", "method", "result", "method", "result"], "cite_purpose": ["background"]}, "@cite_29": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_3": {"mid": "1524758670", "abstract": "We have developed a system called OM to explore the problem of code optimization at link-time. OM takes a collection of object modules constituting the entire program, and converts the object code into a symbolic Register Transfer Language (RTL) form that can be easily manipulated. This RTL is then transformed by intermodule optimization and finally converted back into object form. Although much high-level information about the program is gone at link-time, this approach enables us to perform optimizations that a compiler looking at a single module cannot see. Since object modules are more or less independent of the particular source language or compiler, this also gives us the chance to improve the code in ways that some compilers might simply have missed. To test the concept, we have used OM to build an optimizer that does interprocedural code motion. It moves simple loop-invariant code out of loops, even when the loop body extends across many procedures and the loop control is in a different procedure from the invariant code. Our technique also easily handles \u2018\u2018loops\u2019\u2019 induced by recursion rather than iteration. Our code motion technique makes use of an interprocedural liveness analysis to discover dead registers that it can use to hold loop-invariant results. This liveness analysis also lets us perform interprocedural dead code elimination. We applied our code motion and dead code removal to SPEC benchmarks compiled with optimization using the standard compilers for the DECstation 5000. Our system improved the performance by 5 on average and by more than 14 in one case. More improvement should be possible soon; at present we move only simple load and load-address operations out of loops, and we scavenge registers to hold these values, rather than completely reallocating them. This paper will appear in the March issue of Journal of Programming Languages. It replaces Technical Note TN-31, an earlier version of the same material.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2116672403", "abstract": "This paper presents the results of our investigation of code positioning techniques using execution profile data as input into the compilation process. The primary objective of the positioning is to reduce the overhead of the instruction memory hierarchy. After initial investigation in the literature, we decided to implement two prototypes for the Hewlett-Packard Precision Architecture (PA-RISC). The first, built on top of the linker, positions code based on whole procedures. This prototype has the ability to move procedures into an order that is determined by a \u201cclosest is best\u201d strategy. The second prototype, built on top of an existing optimizer package, positions code based on basic blocks within procedures. Groups of basic blocks that would be better as straight-line sequences are identified as chains . These chains are then ordered according to branch heuristics. Code that is never executed during the data collection runs can be physically separated from the primary code of a procedure by a technique we devised called procedure splitting . The algorithms we implemented are described through examples in this paper. The performance improvements from our work are also summarized in various tables and charts.", "ref_function": ["background", "objective", "method", "method", "method", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2162612712", "abstract": "A dynamic instruction trace often contains many unnecessary instructions that are required only by the unexecuted portion of the program. Hot-cold optimization (HCO) is a technique that realizes this performance opportunity. HCO uses profile information to partition each routine into frequently executed (hot) and infrequently executed (cold) parts. Unnecessary operations in the hot portion are removed, and compensation code is added on transitions from hot to cold as needed. We evaluate HCO on a collection of large Windows NT applications. HCO is most effective on the programs that are call intensive and have flat profiles, providing a 3-8 reduction in path length beyond conventional optimization.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "177739376", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["For Tcl @cite_13 two integration solutions exist: the TclBlend binding @cite_11 and the Jacl implementation @cite_14 .", "TclBlend is a binding between Java and Tcl, which, as LuaJava, allows Java objects to be manipulated by scripts.", "Some operations, such as access to fields and static method invocations, require specific functions.", "Calls to instance methods are handled naturally by Tcl commands."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Scripting languages are becoming more and more important as a tool for software development, as they provide great flexibility for rapid prototyping and for configuring componentware applications. In this paper we present LuaJava, a scripting tool for Java. LuaJava adopts Lua, a dynamically typed interpreted language, as its script language. Great emphasis is given to the transparency of the integration between the two languages, so that objects from one language can be used inside the other like native objects. The final result of this integration is a tool that allows the construction of configurable Java applications, using off-the-shelf components, in a high abstraction level.", "reference": {"@cite_14": {"mid": "2162914120", "abstract": "This paper describes the motivations and strategies behind our group\u2019s efforts to integrate the Tcl and Java programming languages. From the Java perspective, we wish to create a powerful scripting solution for Java applications and operating environments. From the Tcl perspective, we want to allow for cross-platform Tcl extensions and leverage the useful features and user community Java has to offer. We are specifically focusing on Java tasks like Java Bean manipulation, where a scripting solution is preferable to using straight Java code. Our goal is to create a synergy between Tcl and Java, similar to that of Visual Basic and Visual C++ on the Microsoft desktop, which makes both languages more powerful together than they are individually.", "ref_function": ["background", "objective", "method", "method", "objective"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2789138443", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "196441419", "abstract": "A mechanical brake actuator includes a manual lever which is self-locking in the active braking position. In such position, the lever and associated cable means applies tension to a spring whose force is applied to the plunger of a hydraulic master cylinder included in the conventional turntable hydraulic brake system. In the event of minor leakage and or thermal changes in the hydraulic braking system, the spring force exerted by the mechanical actuator maintains safe braking pressure when the crane is parked. When the mechanical actuator is in a release mode, the turntable hydraulic brake is foot pedal operated from the crane operator's cab without interference from the mechanical actuator.", "ref_function": ["background", "background", "background", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The main objective of this chapter was to study the basic @math -branes that one encounters in M-theory, and to treat them in a unified way.", "The need to unify the treatment is inspired by U-duality @cite_22 @cite_86 @cite_144 , which states that from the effective lower dimensional space-time point of view, all the charges carried by the different branes are on the same footing.", "While string theory breaks' this U-duality symmetry, choosing the NSNS string to be the fundamental object of the perturbative theory, the supergravity low-energy effective theories realize the U-duality at the classical level."], "label": ["Describing the objective", "Explaining the method relationship between own work and references", "Explain the significance of references"], "target_paper": "The thesis begins with an introduction to M-theory (at a graduate student's level), starting from perturbative string theory and proceeding to dualities, D-branes and finally Matrix theory. The following chapter treats, in a self-contained way, of general classical p-brane solutions. Black and extremal branes are reviewed, along with their semi-classical thermodynamics. We then focus on intersecting extremal branes, the intersection rules being derived both with and without the explicit use of supersymmetry. The last three chapters comprise more advanced aspects of brane physics, such as the dynamics of open branes, the little theories on the world-volume of branes and how the four dimensional Schwarzschild black hole can be mapped to an extremal configuration of branes, thus allowing for a statistical interpretation of its entropy. The original results were already reported in hep-th 9701042, hep-th 9704190, hep-th 9710027 and hep-th 9801053.", "reference": {"@cite_86": {"mid": "1987603965", "abstract": "Abstract The strong coupling dynamics of string theories in dimension d \u2a7e 4 are studied. It is argued, among other things, that eleven-dimensional supergravity arises as a low energy limit of the ten-dimensional Type IIA superstring, and that a recently conjectured duality between the heterotic string and Type IIA superstrings controls the strong coupling dynamics of the heterotic string in five, six, and seven dimensions and implies S -duality for both heterotic and Type II strings.", "ref_function": ["background", "background"], "cite_purpose": [""]}, "@cite_22": {"mid": "2141847212", "abstract": "Abstract The effective action for type II string theory compactified on a six-torus is N = 8 supergravity, which is known to have an E7 duality symmetry. We show that this is broken by quantum effects to a discrete subgroup, E 7 ( Z ) , which contains both the T-duality group O(6, 6; Z ) and the S-duality group SL(2; Z ). We present evidence for the conjecture that E 7 ( Z ) is an exact \u2018U-duality\u2019 symmetry of type II string theory. This conjecture requires certain extreme black hole states to be identified with massive modes of the fundamental string. The gauge bosons from the Ramond-Ramond sector couple not to string excitations but to solitons. We discuss similar issues in the context of toroidal string compactifications to other dimensions, compactifications of the type II string on K3 \u00d7 T2 and compactifications of 11-dimensional supermembrane theory.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": [""]}, "@cite_144": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": [""]}}}
{"sentences": ["The inherent depth ambiguity in 3d pose estimation from monocular images limits the estimation accuracy.", "Extensive research has been done to exploit extra information contained in temporal sequences.", "Zhou al @cite_35 formulate an optimization problem to search for the 3d configuration with the highest probability given 2d confidence maps and solve the problem using Expectation-Maximization.", "Tekin al @cite_5 use a CNN to align bounding box of consecutive frames and then generate a spatial-temporal volume based on which they extract 3d HOG features and regress the 3d pose for the central frame.", "Mehta al @cite_4 propose a real-time system for 3d pose estimation and apply temporal filtering to yield temporally consistent 3d poses."], "label": ["Reference to current state of knowledge", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Existing deep learning approaches on 3d human pose estimation for videos are either based on Recurrent or Convolutional Neural Networks (RNNs or CNNs). However, RNN-based frameworks can only tackle sequences with limited frames because sequential models are sensitive to bad frames and tend to drift over long sequences. Although existing CNN-based temporal frameworks attempt to address the sensitivity and drift problems by concurrently processing all input frames in the sequence, the existing state-of-the-art CNN-based framework is limited to 3d pose estimation of a single frame from a sequential input. In this paper, we propose a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation. Our approach processes all input frames concurrently to avoid the sensitivity and drift problems, and yet outputs the 3d pose estimates for every frame in the input sequence. More specifically, the 3d poses in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix. The trajectory bases matrix is precomputed from matrix factorization approaches such as Singular Value Decomposition (SVD) or Discrete Cosine Transform (DCT), and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. We demonstrate the effectiveness of our framework on long sequences by achieving state-of-the-art performances on multiple benchmark datasets. Our source code is available at: this https URL.", "reference": {"@cite_35": {"mid": "2963688992", "abstract": "This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables to take into account considerable uncertainties in 2D joint locations. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset.", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2963592930", "abstract": "We propose an efficient approach to exploiting motion information from consecutive frames of a video sequence to recover the 3D pose of people. Previous approaches typically compute candidate poses in individual frames and then link them in a post-processing step to resolve ambiguities. By contrast, we directly regress from a spatio-temporal volume of bounding boxes to a 3D pose in the central frame. We further show that, for this approach to achieve its full potential, it is essential to compensate for the motion in consecutive frames so that the subject remains centered. This then allows us to effectively overcome ambiguities and improve upon the state-of-the-art by a large margin on the Human3.6m, HumanEva, and KTH Multiview Football 3D human pose estimation benchmarks.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2611932403", "abstract": "We present the first real-time method to capture the full global 3D skelet al pose of a human in a stable, temporally consistent manner using a single RGB camera. Our method combines a new convolutional neural network (CNN) based pose regressor with kinematic skeleton fitting. Our novel fully-convolutional pose formulation regresses 2D and 3D joint positions jointly in real time and does not require tightly cropped input frames. A real-time kinematic skeleton fitting method uses the CNN output to yield temporally stable 3D global pose reconstructions on the basis of a coherent kinematic skeleton. This makes our approach the first monocular RGB method usable in real-time applications such as 3D character control---thus far, the only monocular methods for such applications employed specialized RGB-D cameras. Our method's accuracy is quantitatively on par with the best offline 3D monocular RGB pose estimation methods. Our results are qualitatively comparable to, and sometimes better than, results from monocular RGB-D approaches, such as the Kinect. However, we show that our approach is more broadly applicable than RGB-D solutions, i.e., it works for outdoor scenes, community videos, and low quality commodity RGB cameras.", "ref_function": ["method", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Recently, RNN-based frameworks are used to deal with sequential input data.", "Lin al @cite_1 use a multi-stage framework based on Long Short-term Memory (LSTM) units to estimate the 3d pose from the extracted 2d features and estimated 3d pose in the previous stage.", "Coskun al @cite_32 propose to learn a human motion model using Kalman Filter and implement it with LSTMs.", "Hossain al @cite_36 design a sequence-to-sequence network with LSTM units to first encode a sequence of motions in the form of 2d joint locations and then decode the 3d poses of the sequence.", "However, RNNs are sensitive to erroneous inputs and tend to drift over long sequences.", "To overcome the shortcomings of RNNs, a CNN-based framework is proposed by Pavllo al @cite_26 to aggregate temporal information using dilated convolutions.", "Despite being successful at regressing a single frame from a sequence of input, it cannot concurrently output the 3d pose estimations for all frames in the sequence."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Existing deep learning approaches on 3d human pose estimation for videos are either based on Recurrent or Convolutional Neural Networks (RNNs or CNNs). However, RNN-based frameworks can only tackle sequences with limited frames because sequential models are sensitive to bad frames and tend to drift over long sequences. Although existing CNN-based temporal frameworks attempt to address the sensitivity and drift problems by concurrently processing all input frames in the sequence, the existing state-of-the-art CNN-based framework is limited to 3d pose estimation of a single frame from a sequential input. In this paper, we propose a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation. Our approach processes all input frames concurrently to avoid the sensitivity and drift problems, and yet outputs the 3d pose estimates for every frame in the input sequence. More specifically, the 3d poses in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix. The trajectory bases matrix is precomputed from matrix factorization approaches such as Singular Value Decomposition (SVD) or Discrete Cosine Transform (DCT), and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. We demonstrate the effectiveness of our framework on long sequences by achieving state-of-the-art performances on multiple benchmark datasets. Our source code is available at: this https URL.", "reference": {"@cite_36": {"mid": "2769237672", "abstract": "In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately (12.2 ) and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.", "ref_function": ["background", "method", "method", "method", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2903549000", "abstract": "In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11 , and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at this https URL", "ref_function": ["background", "method", "method", "result", "result", "other"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2963383668", "abstract": "3D Human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2963203809", "abstract": "One-shot pose estimation for tasks such as body joint localization, camera pose estimation, and object tracking are generally noisy, and temporal filters have been extensively used for regularization. One of the most widely-used methods is the Kalman filter, which is both extremely simple and general. However, Kalman filters require a motion model and measurement model to be specified a priori, which burdens the modeler and simultaneously demands that we use explicit models that are often only crude approximations of reality. For example, in the pose-estimation tasks mentioned above, it is common to use motion models that assume constant velocity or constant acceleration, and we believe that these simplified representations are severely inhibitive. In this work, we propose to instead learn rich, dynamic representations of the motion and noise models. In particular, we propose learning these models from data using long shortterm memory, which allows representations that depend on all previous observations and all previous states. We evaluate our method using three of the most popular pose estimation tasks in computer vision, and in all cases we obtain state-of-the-art performance.", "ref_function": ["background", "background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Inspired by matrix factorization methods commonly used in Structure-from-Motion (SfM) @cite_23 and non-rigid SfM @cite_33 , several works @cite_3 @cite_45 @cite_21 on 3d human pose estimation factorize the sequence of 3d human poses into a linear combination of shape bases.", "Akhter al @cite_42 suggest a duality of the factorization in the trajectory space.", "We extend the idea of matrix factorization to learning a deep network that estimates the coefficients of the trajectory bases from a sequence of 2d poses as inputs.", "The 3d poses of all frames are recovered concurrently as the linear combinations of the trajectory bases with the estimated coefficients."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Describing used methods", "Describing the results"], "target_paper": "Existing deep learning approaches on 3d human pose estimation for videos are either based on Recurrent or Convolutional Neural Networks (RNNs or CNNs). However, RNN-based frameworks can only tackle sequences with limited frames because sequential models are sensitive to bad frames and tend to drift over long sequences. Although existing CNN-based temporal frameworks attempt to address the sensitivity and drift problems by concurrently processing all input frames in the sequence, the existing state-of-the-art CNN-based framework is limited to 3d pose estimation of a single frame from a sequential input. In this paper, we propose a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation. Our approach processes all input frames concurrently to avoid the sensitivity and drift problems, and yet outputs the 3d pose estimates for every frame in the input sequence. More specifically, the 3d poses in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix. The trajectory bases matrix is precomputed from matrix factorization approaches such as Singular Value Decomposition (SVD) or Discrete Cosine Transform (DCT), and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. We demonstrate the effectiveness of our framework on long sequences by achieving state-of-the-art performances on multiple benchmark datasets. Our source code is available at: this https URL.", "reference": {"@cite_33": {"mid": "2124600577", "abstract": "The paper addresses the problem of recovering 3D non-rigid shape models from image sequences. For example, given a video recording of a talking person, we would like to estimate a 3D model of the lips and the full face and its internal modes of variation. Many solutions that recover 3D shape from 2D image sequences have been proposed; these so-called structure-from-motion techniques usually assume that the 3D object is rigid. For example, C. Tomasi and T. Kanades' (1992) factorization technique is based on a rigid shape matrix, which produces a tracking matrix of rank 3 under orthographic projection. We propose a novel technique based on a non-rigid model, where the 3D shape in each frame is a linear combination of a set of basis shapes. Under this model, the tracking matrix is of higher rank, and can be factored in a three-step process to yield pose, configuration and shape. To the best of our knowledge, this is the first model free approach that can recover from single-view video sequences nonrigid shape models. We demonstrate this new algorithm on several video sequences. We were able to recover 3D non-rigid human face and animal models with high accuracy.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_42": {"mid": "2138816964", "abstract": "Existing approaches to nonrigid structure from motion assume that the instantaneous 3D shape of a deforming object is a linear combination of basis shapes, which have to be estimated anew for each video sequence. In contrast, we propose that the evolving 3D structure be described by a linear combination of basis trajectories. The principal advantage of this approach is that we do not need to estimate any basis vectors during computation. We show that generic bases over trajectories, such as the Discrete Cosine Transform (DCT) basis, can be used to compactly describe most real motions. This results in a significant reduction in unknowns, and corresponding stability in estimation. We report empirical performance, quantitatively using motion capture data, and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing).", "ref_function": ["background", "background", "background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2155196764", "abstract": "Reconstructing an arbitrary configuration of 3D points from their projection in an image is an ill-posed problem. When the points hold semantic meaning, such as anatomical landmarks on a body, human observers can often infer a plausible 3D configuration, drawing on extensive visual memory. We present an activity-independent method to recover the 3D configuration of a human figure from 2D locations of anatomical landmarks in a single image, leveraging a large motion capture corpus as a proxy for visual memory. Our method solves for anthropometrically regular body pose and explicitly estimates the camera via a matching pursuit algorithm operating on the image projections. Anthropometric regularity (i.e., that limbs obey known proportions) is a highly informative prior, but directly applying such constraints is intractable. Instead, we enforce a necessary condition on the sum of squared limb-lengths that can be solved for in closed form to discourage implausible configurations in 3D. We evaluate performance on a wide variety of human poses captured from different viewpoints and show generalization to novel 3D configurations and robustness to missing data.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_45": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2138835141", "abstract": "Inferring scene geometry and camera motion from a stream of images is possible in principle, but is an ill-conditioned problem when the objects are distant with respect to their size. We have developed a factorization method that can overcome this difficulty by recovering shape and motion under orthography without computing depth as an intermediate step. An image stream can be represented by the 2FxP measurement matrix of the image coordinates of P points tracked through F frames. We show that under orthographic projection this matrix is of rank 3. Based on this observation, the factorization method uses the singular-value decomposition technique to factor the measurement matrix into two matrices which represent object shape and camera rotation respectively. Two of the three translation components are computed in a preprocessing stage. The method can also handle and obtain a full solution from a partially filled-in measurement matrix that may result from occlusions or tracking failures. The method gives accurate results, and does not introduce smoothing in either shape or motion. We demonstrate this with a series of experiments on laboratory and outdoor image streams, with and without occlusions.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": [": Deep learning models have achieved great success in recent years, whose representative examples include convolutional neural network(CNN) @cite_12 @cite_1 .", "CNN has been mainly used to deal with image data and shows outstanding performance on various computer vision tasks.", "Besides CNN, there also exist many other types of deep learning models, e.g., recurrent neural net @cite_10 @cite_9 , deep autoencoder @cite_21 , deep boltzmann machine @cite_13 , and GAN @cite_5 , etc."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Among various optimization algorithms, ADAM can achieve outstanding performance and has been widely used in model learning. ADAM has the advantages of fast convergence with both momentum and adaptive learning rate. For deep neural network learning problems, since their objective functions are nonconvex, ADAM can also get stuck in local optima easily. To resolve such a problem, the genetic evolutionary ADAM (GADAM) algorithm, which combines the ADAM and genetic algorithm, was introduced in recent years. To further maximize the advantages of the GADAM model, we propose to implement the boosting strategy for unit model training in GADAM. In this paper, we introduce a novel optimization algorithm, namely Boosting based GADAM (BGADAM). We will show that after adding the boosting strategy to the GADAM model, it can help unit models jump out the local optima and converge to better solutions.", "reference": {"@cite_13": {"mid": "189596042", "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2091432990", "abstract": "In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled \u201cNew Types of Deep Neural Network Learning for Speech Recognition and Related Applications,\u201d as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.", "ref_function": ["background", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2025768430", "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2099471712", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "179875071", "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50 reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18 reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5 on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition", "ref_function": ["background", "background", "result", "result", "other"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2163605009", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5 and 17.0 which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3 , compared to 26.2 achieved by the second-best entry.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Multi-task learning with deep neural networks has gained increasing attention within NLP community over the past decades.", "@cite_3 and @cite_21 described most of the existing techniques for multi-task learning in deep neural networks.", "Generally, existing MTL methods can be categorised as @cite_29 @cite_10 and @cite_20 @cite_32 ."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Multi-Task Learning (MTL) aims at boosting the overall performance of each individual task by leveraging useful information contained in multiple related tasks. It has shown great success in natural language processing (NLP). Currently, a number of MLT architectures and learning mechanisms have been proposed for various NLP tasks. However, there is no systematic exploration and comparison of different MLT architectures and learning mechanisms for their strong performance in-depth. In this paper, we conduct a thorough examination of typical MTL methods on a broad range of representative NLP tasks. Our primary goal is to understand the merits and demerits of existing MTL methods in NLP tasks, thus devising new hybrid architectures intended to combine their strengths.", "reference": {"@cite_29": {"mid": "2963877604", "abstract": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2742079690", "abstract": "Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.", "ref_function": ["objective", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2914526845", "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (, 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7 (2.2 absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at this https URL.", "ref_function": ["background", "background", "method", "method", "result", "other"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2624871570", "abstract": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.", "ref_function": ["background", "objective", "method", "objective"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2308486447", "abstract": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.", "ref_function": ["background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In @cite_106 (inspired by @cite_54 ) a solution is presented which corresponds to a M5 @math M5=1 configuration, which follows the harmonic superposition rule, provided however that the harmonic functions depend on the respective relative transverse space (i.e.", "they are functions of two different spaces).", "The problem now is that the harmonic functions do not depend on the overall transverse space (which is 1-dimensional in the case above), the configuration thus not being localized there.", "A method actually inspired by the one presented here to derive the intersecting brane solutions, has been applied in @cite_89 to the intersections of this second kind.", "Imposing that the functions depend on the relative transverse space(s) (with factorized dependence) and not on the overall one, the authors of @cite_89 arrive at a formula for the intersections very similar to intersectionrules , with @math on the l.h.s.", "This rule correctly reproduces the M5 @math M5=1 configuration, and moreover also all the configurations of two D-branes with 8 Neumann-Dirichlet directions, which preserve @math supersymmetries but were excluded from the intersecting solutions derived in this chapter (only the configurations with 4 ND directions were found as solutions).", "One such configuration is e.g.", "D0 @math D8."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Not sure", "Not sure"], "target_paper": "The thesis begins with an introduction to M-theory (at a graduate student's level), starting from perturbative string theory and proceeding to dualities, D-branes and finally Matrix theory. The following chapter treats, in a self-contained way, of general classical p-brane solutions. Black and extremal branes are reviewed, along with their semi-classical thermodynamics. We then focus on intersecting extremal branes, the intersection rules being derived both with and without the explicit use of supersymmetry. The last three chapters comprise more advanced aspects of brane physics, such as the dynamics of open branes, the little theories on the world-volume of branes and how the four dimensional Schwarzschild black hole can be mapped to an extremal configuration of branes, thus allowing for a statistical interpretation of its entropy. The original results were already reported in hep-th 9701042, hep-th 9704190, hep-th 9710027 and hep-th 9801053.", "reference": {"@cite_54": {"mid": "2065552713", "abstract": "We derive an exact stringlike soliton solution of [ital D]=10 heterotic string theory. The solution possesses SU(2)[times]SU(2) instanton structure in the eight-dimensional space transverse to the world sheet of the soliton.", "ref_function": ["background", "background"], "cite_purpose": ["background"]}, "@cite_106": {"mid": "1992572456", "abstract": "We construct new supersymmetric solutions of D = 11 supergravity describing n orthogonally \u201coverlapping\u201d membranes and fivebranes for n = 2,\u2026,8. Overlapping branes arise after separating intersecting branes in a direction transverse to all of the branes. The solutions, which generalize known intersecting brane solutions, preserve at least 2\u2212n of the supersymmetry. Each pairwise overlap involves a membrane overlapping a membrane in a 0-brane, a fivebrane overlapping a fivebrane in a 3-brane or a membrane overlapping a fivebrane in a string. After reducing n overlapping membranes to obtain n overlapping D-2-branes in D = 10, T-duality generates new overlapping D-brane solutions in type IIA and type IIB string theory. Uplifting certain type IIA solutions leads to the D = 11 solutions. Some of the new solutions reduce to dilaton black holes in D = 4. Additionally, we present a D = 10 solution that describes two D-5-branes overlapping in a string. T-duality then generates further D = 10 solutions and uplifting one of the type IIA solutions gives a new D = 11 solution describing two fivebranes overlapping in a string.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_89": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["similarities", "background"]}}}
{"sentences": ["Nowadays, telecom companies use widely big data in order to mine the behaviour of their customers, improve the quality of service that they provide and reduce the customers' churn.", "Towards this direction, demographic statistics, network deployments and call detail records (CDRs) are key factors that need to be carefully integrated in order to make accurate predictions.", "Though there are various open source data for the first two factors, researchers rarely have access to traffic demand data, since it is a sensitive information for the operators.", "Therefore, researchers need to rely on synthetic models, which do not always capture accurately large-scale mobile networks @cite_5 ."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "Reference to current state of knowledge", "Reference to current state of knowledge"], "target_paper": "In this work, we are interested in the applications of big data in the telecommunication domain, analysing two weeks of datasets provided by Telecom Italia for Milan and Trento. Our objective is to identify hotspots which are places with very high communication traffic relative to others and measure the interaction between them. We model the hotspots as nodes in a graph and then apply node centrality metrics that quantify the importance of each node. We review five node centrality metrics and show that they can be divided into two families: the first family is composed of closeness and betweenness centrality whereas the second family consists of degree, PageRank and eigenvector centrality. We then proceed with a statistical analysis in order to evaluate the consistency of the results over the two weeks. We find out that the ranking of the hotspots under the various centrality metrics remains practically the same with the time for both Milan and Trento. We further identify that the relative difference of the values of the metrics is smaller for PageRank centrality than for closeness centrality and this holds for both Milan and Trento. Finally, our analysis reveals that the variance of the results is significantly smaller for Trento than for Milan.", "reference": {"@cite_5": {"mid": "2741581007", "abstract": "In a world of open data and large-scale measurements, it is often feasible to obtain a real-world trace to fit to one's research problem. Feasible, however, does not imply simple. Taking next-generation cellular network planning as a case study, in this paper we describe a large-scale dataset, combining topology, traffic demand from call detail records, and demographic information throughout a whole country. We investigate how these aspects interact, revealing effects that are normally not captured by smaller-scale or synthetic datasets. In addition to making the resulting dataset available for download, we discuss how our experience can be generalized to other scenarios and case studies, i.e., how everyone can construct a similar dataset from publicly available information.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["motivation"]}}}
{"sentences": ["For example, the authors in @cite_4 analyse an heterogeneous cellular network which consists of different types of nodes, such as macrocells and microcells.", "Nowadays a popular model is the one from Wyner @cite_0 , but it fails to fully capture a real heterogeneous cellular network because it is simplistic.", "Another approach is to use the spatial Poisson point process model (SPPP) @cite_9 , which can be derived from the premise that all base stations are uniformly distributed.", "However, a city can be classified in different areas, which have different population densities.", "These different areas can be characterised as dense urban, urban and suburban.", "To be able to classify the heterogeneous networks into these areas, the authors introduce SPPP for homogeneous and inhomogeneous sets.", "They show that the SPPP-model captures accurately both urban and suburban areas, whereas this is not the case for dense urban areas, because of a considerable population concentrated in small areas."], "label": ["Reference to single investigations in the past:  about objective", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "In this work, we are interested in the applications of big data in the telecommunication domain, analysing two weeks of datasets provided by Telecom Italia for Milan and Trento. Our objective is to identify hotspots which are places with very high communication traffic relative to others and measure the interaction between them. We model the hotspots as nodes in a graph and then apply node centrality metrics that quantify the importance of each node. We review five node centrality metrics and show that they can be divided into two families: the first family is composed of closeness and betweenness centrality whereas the second family consists of degree, PageRank and eigenvector centrality. We then proceed with a statistical analysis in order to evaluate the consistency of the results over the two weeks. We find out that the ranking of the hotspots under the various centrality metrics remains practically the same with the time for both Milan and Trento. We further identify that the relative difference of the values of the metrics is smaller for PageRank centrality than for closeness centrality and this holds for both Milan and Trento. Finally, our analysis reveals that the variance of the results is significantly smaller for Trento than for Milan.", "reference": {"@cite_0": {"mid": "2131070905", "abstract": "The Wyner model has been widely used to model and analyze cellular networks due to its simplicity and analytical tractability. Its key aspects include fixed user locations and the deterministic and homogeneous interference intensity. While clearly a significant simplification of a real cellular system, which has random user locations and interference levels that vary by several orders of magnitude over a cell, a common presumption by theorists is that the Wyner model nevertheless captures the essential aspects of cellular interactions. But is this true? To answer this question, we compare the Wyner model to a model that includes random user locations and fading. We consider both uplink and downlink transmissions and both outage-based and average-based metrics. For the uplink, for both metrics, we conclude that the Wyner model is in fact quite accurate for systems with a sufficient number of simultaneous users, e.g., a CDMA system. Conversely, it is broadly inaccurate otherwise. Turning to the downlink, the Wyner model becomes inaccurate even for systems with a large number of simultaneous users. In addition, we derive an approximation for the main parameter in the Wyner model - the interference intensity term, which depends on the path loss exponent.", "ref_function": ["background", "background", "background", "background", "method", "method", "result", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["uses"]}, "@cite_4": {"mid": "2005411736", "abstract": "In heterogeneous cellular networks spatial characteristics of base stations (BSs) influence the system performance intensively. Existing models like two-dimensional hexagonal grid model or homogeneous spatial poisson point process (SPPP) are based on the assumption that BSs are ideal or uniformly distributed, but the aggregation behavior of users in hot spots has an important effect on the location of low power nodes (LPNs), so these models fail to characterize the distribution of BSs in the current mobile cellular networks. In this paper, firstly existing spatial models are analyzed. Then, based on real data from a mobile operator in one large city of China, a set of spatial models is proposed in three typical regions: dense urban, urban and suburban. For dense urban area, \u201cTwo Tiers Poisson Cluster Superimposed Process\u201d is proposed to model the spatial characteristics of real-world BSs. Specifically, for urban and suburban area, conventional SPPP model still can be used. Finally, the fundamental relationship between user behavior and BS distribution is illustrated and summarized. Numerous results show that SPPP is only appropriate in the urban and suburban regions where users are not gathered together obviously. Principal parameters of these models are provided as reference for the theoretical analysis and computer simulation, which describe the complex spatial configuration more reasonably and reflect the current mobile cellular network performance more precisely.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In the field of marine robotics, @cite_10 used locally weighted projection regression to compensate the mismatch between the physics based model and the sensors reading of the AUV Nessie.", "Auto-regressive networks augmented with a genetic algorithm as a gating network were used to identify the model of a simulated AUV with variable mass.", "In a previous work @cite_16 , an on-line adaptation method was proposed to model the change in the damping forces resulting from a structural change of an AUVs mechanical structure.", "The algorithm showed good adaptation capability but was only limited to modelling the damping effect of an AUV model.", "In this work we build upon our the results of @cite_14 @cite_11 to provide a general framework for on-line learning of AUV fully coupled nonlinear dynamics, and validating the proposed approach on simulated data as well as real robot data."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Describing used methods"], "target_paper": "Learning the dynamics of robots from data can help achieve more accurate tracking controllers, or aid their navigation algorithms. However, when the actual dynamics of the robots change due to external conditions, on-line adaptation of their models is required to maintain high fidelity performance. In this work, a framework for on-line learning of robot dynamics is developed to adapt to such changes. The proposed framework employs an incremental support vector regression method to learn the model sequentially from data streams. In combination with the incremental learning, strategies for including and forgetting data are developed to obtain better generalization over the whole state space. The framework is tested in simulation and real experimental scenarios demonstrating its adaptation capabilities to changes in the robot\u2019s dynamics.", "reference": {"@cite_14": {"mid": "2766583623", "abstract": "This work addresses a data driven approach which employs a machine learning technique known as Support Vector Regression (SVR), to identify the coupled dynamical model of an autonomous underwater vehicle. To train the regressor, we use a dataset collected from the robot's on-board navigation sensors and actuators. To achieve a better fit to the experimental data, a variant of a radial-basis-function kernel is used in combination with the SVR which accounts for the different complexities of each of the contributing input features of the model. We compare our method to other explicit hydrodynamic damping models that were identified using the total least squares method and with less complex SVR methods. To analyze the transferability, we clearly separate training and testing data obtained in real-world experiments. Our presented method shows much better results especially compared to classical approaches.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["extends"]}, "@cite_16": {"mid": "2774694743", "abstract": "This paper presents an online technique which employs incremental support vector regression to learn the damping term of an underwater vehicle motion model, subject to dynamical changes in the vehicle's body. To learn the damping term, we use data collected from the robot's on-board navigation sensors and actuator encoders. We introduce a new sample-efficient methodology which accounts for adding new training samples, removing old samples, and outlier rejection. The proposed method is tested in a real-world experimental scenario to account for the model's dynamical changes due to a change in the vehicle's geometrical shape.", "ref_function": ["background", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2022798939", "abstract": "Navigation is instrumental in the successful deployment of Autonomous Underwater Vehicles (AUVs). Sensor hardware is installed on AUVs to support navigational accuracy. Sensors, however, may fail during deployment, thereby jeopardizing the mission. This work proposes a solution, based on an adaptive dynamic model, to accurately predict the navigation of the AUV. A hydrodynamic model, derived from simple laws of physics, is integrated with a powerful non-parametric regression method. The incremental regression method, namely the Locally Weighted Projection Regression (LWPR), is used to compensate for un-modeled dynamics, as well as for possible changes in the operating conditions of the vehicle. The augmented hydrodynamic model is used within an Extended Kalman Filter, to provide optimal estimations of the AUV\u2019s position and orientation. Experimental results demonstrate an overall improvement in the prediction of the vehicle\u2019s acceleration and velocity.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["extends"]}}}
{"sentences": ["In recent years, research regarding image matching has been influenced by the developments in other areas of computer vision.", "Deep learning architectures have been developed both for image matching @cite_10 @cite_1 @cite_17 and geopositioning @cite_13 @cite_5 @cite_18 with attractive results."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken"], "target_paper": "In this work, we present a camera geopositioning system based on matching a query image against a database with panoramic images. For matching, our system uses memory vectors aggregated from global image descriptors based on convolutional features to facilitate fast searching in the database. To speed up searching, a clustering algorithm is used to balance geographical positioning and computation time. We refine the obtained position from the query image using a new outlier removal algorithm. The matching of the query image is obtained with a recall@5 larger than 90 for panorama-to-panorama matching. We cluster available panoramas from geographically adjacent locations into a single compact representation and observe computational gains of approximately 50 at the cost of only a small (approximately 3 ) recall loss. Finally, we present a coordinate estimation algorithm that reduces the median geopositioning error by up to 20 .", "reference": {"@cite_13": {"mid": "1946093182", "abstract": "The recent availability of geo-tagged images and rich geospatial data has inspired a number of algorithms for image based geolocalization. Most approaches predict the location of a query image by matching to ground-level images with known locations (e.g., street-view data). However, most of the Earth does not have ground-level reference photos available. Fortunately, more complete coverage is provided by oblique aerial or \u201cbird's eye\u201d imagery. In this work, we localize a ground-level query image by matching it to a reference database of aerial imagery. We use publicly available data to build a dataset of 78K aligned crossview image pairs. The primary challenge for this task is that traditional computer vision approaches cannot handle the wide baseline and appearance variation of these cross-view pairs. We use our dataset to learn a feature representation in which matching views are near one another and mismatched views are far apart. Our proposed approach, Where-CNN, is inspired by deep learning success in face verification and achieves significant improvements over traditional hand-crafted features and existing deep features learned from other large-scale databases. We show the effectiveness of Where-CNN in finding matches between street view and aerial view imagery and demonstrate the ability of our learned features to generalize to novel locations.", "ref_function": ["background", "background", "background", "background", "method", "method", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2199890863", "abstract": "We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales. To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States. Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2609950940", "abstract": "Location recognition is commonly treated as visual instance retrieval on \"street view\" imagery. The dataset items and queries are panoramic views, i.e. groups of images taken at a single location. This work introduces a novel panorama-to-panorama matching process, either by aggregating features of individual images in a group or by explicitly constructing a larger panorama. In either case, multiple views are used as queries. We reach near perfect location recognition on a standard benchmark with only four query views.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "1969891195", "abstract": "We address the problem of geo-registering ground-based multi-view stereo models by ground-to-aerial image matching. The main contribution is a fully automated geo-registration pipeline with a novel viewpoint-dependent matching method that handles ground to aerial viewpoint variation. We conduct large-scale experiments which consist of many popular outdoor landmarks in Rome. The proposed approach demonstrates a high success rate for the task, and dramatically outperforms state-of-the-art techniques, yielding geo-registration at pixel-level accuracy.", "ref_function": ["background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2204975001", "abstract": "Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It also has been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregating methods developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptor. In this paper we investigate possible ways to aggregate local deep features to produce compact descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. In addition, we suggest a simple yet efficient query expansion scheme suitable for the proposed aggregation method. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.", "ref_function": ["background", "background", "method", "method", "objective", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2614218061", "abstract": "We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELF (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which shares most network layers with the descriptor. This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives---in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELF outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins. Code and dataset can be found at the project webpage: this https URL .", "ref_function": ["background", "background", "method", "method", "result", "method", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Convolutional features extracted from the deep layers of CNNs have shown great utility when addressing image matching and retrieval problems.", "Babenko @cite_10 employ pre-trained networks to generate descriptors based on high-level convolutional features used for retrieving images of various landmarks.", "Sunderhauf @cite_2 solve the problem of urban scene recognition, employing salient regions and convolutional features of local objects.", "This method is extended in @cite_8 , where additional spatial information is used to increase the algorithm performance."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "In this work, we present a camera geopositioning system based on matching a query image against a database with panoramic images. For matching, our system uses memory vectors aggregated from global image descriptors based on convolutional features to facilitate fast searching in the database. To speed up searching, a clustering algorithm is used to balance geographical positioning and computation time. We refine the obtained position from the query image using a new outlier removal algorithm. The matching of the query image is obtained with a recall@5 larger than 90 for panorama-to-panorama matching. We cluster available panoramas from geographically adjacent locations into a single compact representation and observe computational gains of approximately 50 at the cost of only a small (approximately 3 ) recall loss. Finally, we present a coordinate estimation algorithm that reduces the median geopositioning error by up to 20 .", "reference": {"@cite_8": {"mid": "2518534307", "abstract": "Recent work by [1] demonstrated improved visual place recognition using proposal regions coupled with features from convolutional neural networks (CNN) to match landmarks between views. In this work we extend the approach by introducing descriptors built from landmark features which also encode the spatial distribution of the landmarks within a view. Matching descriptors then enforces consistency of the relative positions of landmarks between views. This has a significant impact on performance. For example, in experiments on 10 image-pair datasets, each consisting of 200 urban locations with significant differences in viewing positions and conditions, we recorded average precision of around 70 (at 100 recall), compared with 58 obtained using whole image CNN features and 50 for the method in [1].", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2204975001", "abstract": "Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It also has been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregating methods developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptor. In this paper we investigate possible ways to aggregate local deep features to produce compact descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. In addition, we suggest a simple yet efficient query expansion scheme suitable for the proposed aggregation method. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.", "ref_function": ["background", "background", "method", "method", "objective", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "1162411702", "abstract": "Place recognition has long been an incompletely solved problem in that all approaches involve significant compromises. Current methods address many but never all of the critical challenges of place recognition \u2013 viewpoint-invariance, condition-invariance and minimizing training requirements. Here we present an approach that adapts state-of-the-art object proposal techniques to identify potential landmarks within an image for place recognition. We use the astonishing power of convolutional neural network features to identify matching landmark proposals between images to perform place recognition over extreme appearance and viewpoint variations. Our system does not require any form of training, all components are generic enough to be used off-the-shelf. We present a range of challenging experiments in varied viewpoint and environmental conditions. We demonstrate superior performance to current state-of-the- art techniques. Furthermore, by building on existing and widely used recognition frameworks, this approach provides a highly compatible place recognition system with the potential for easy integration of other techniques such as object detection and semantic scene interpretation.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The problem of geopositioning can be seen as a dedicated branch of image retrieval.", "In this case, the objective is to compute extrinsic parameters (or coordinates) of a camera capturing the query image, based on the matched georeferenced images from a database.", "There exist many different algorithms and neural network architectures that attempt to identify the geographical location of a street-level query image.", "Lin @cite_13 learn deep representations for matching aerial and ground images.", "Workman @cite_18 use spatial features at multiple scales which are fused with street-level features, to solve the problem of geolocalization.", "@cite_5 , a fully automated processing pipeline matches multi-view stereo (MVS) models to aerial images.", "This matching algorithm handles the viewpoint variance across aerial and street-level images."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "General reference to previous research or scholarship: research objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "In this work, we present a camera geopositioning system based on matching a query image against a database with panoramic images. For matching, our system uses memory vectors aggregated from global image descriptors based on convolutional features to facilitate fast searching in the database. To speed up searching, a clustering algorithm is used to balance geographical positioning and computation time. We refine the obtained position from the query image using a new outlier removal algorithm. The matching of the query image is obtained with a recall@5 larger than 90 for panorama-to-panorama matching. We cluster available panoramas from geographically adjacent locations into a single compact representation and observe computational gains of approximately 50 at the cost of only a small (approximately 3 ) recall loss. Finally, we present a coordinate estimation algorithm that reduces the median geopositioning error by up to 20 .", "reference": {"@cite_5": {"mid": "1969891195", "abstract": "We address the problem of geo-registering ground-based multi-view stereo models by ground-to-aerial image matching. The main contribution is a fully automated geo-registration pipeline with a novel viewpoint-dependent matching method that handles ground to aerial viewpoint variation. We conduct large-scale experiments which consist of many popular outdoor landmarks in Rome. The proposed approach demonstrates a high success rate for the task, and dramatically outperforms state-of-the-art techniques, yielding geo-registration at pixel-level accuracy.", "ref_function": ["background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2199890863", "abstract": "We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales. To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States. Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "1946093182", "abstract": "The recent availability of geo-tagged images and rich geospatial data has inspired a number of algorithms for image based geolocalization. Most approaches predict the location of a query image by matching to ground-level images with known locations (e.g., street-view data). However, most of the Earth does not have ground-level reference photos available. Fortunately, more complete coverage is provided by oblique aerial or \u201cbird's eye\u201d imagery. In this work, we localize a ground-level query image by matching it to a reference database of aerial imagery. We use publicly available data to build a dataset of 78K aligned crossview image pairs. The primary challenge for this task is that traditional computer vision approaches cannot handle the wide baseline and appearance variation of these cross-view pairs. We use our dataset to learn a feature representation in which matching views are near one another and mismatched views are far apart. Our proposed approach, Where-CNN, is inspired by deep learning success in face verification and achieves significant improvements over traditional hand-crafted features and existing deep features learned from other large-scale databases. We show the effectiveness of Where-CNN in finding matches between street view and aerial view imagery and demonstrate the ability of our learned features to generalize to novel locations.", "ref_function": ["background", "background", "background", "background", "method", "method", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["A common factor of the above work is that it either requires the combination of aerial and street-level images for geopositioning, or extensive training on specific datasets.", "Both cases and their solutions cannot be easily generalized.", "In our approach, we utilize georeferenced, street-level panoramic images only and a pre-trained CNN combined with image matching techniques for coordinate estimation.", "This avoids lengthy training and labeling procedures and assumes street-level data to be available without requiring aerial images.", "Furthermore, and unlike @cite_1 , we do not assume that our query and database images originate from the same imaging devices."], "label": ["Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Describing used methods", "Describing the results", "Describing used methods"], "target_paper": "In this work, we present a camera geopositioning system based on matching a query image against a database with panoramic images. For matching, our system uses memory vectors aggregated from global image descriptors based on convolutional features to facilitate fast searching in the database. To speed up searching, a clustering algorithm is used to balance geographical positioning and computation time. We refine the obtained position from the query image using a new outlier removal algorithm. The matching of the query image is obtained with a recall@5 larger than 90 for panorama-to-panorama matching. We cluster available panoramas from geographically adjacent locations into a single compact representation and observe computational gains of approximately 50 at the cost of only a small (approximately 3 ) recall loss. Finally, we present a coordinate estimation algorithm that reduces the median geopositioning error by up to 20 .", "reference": {"@cite_1": {"mid": "2609950940", "abstract": "Location recognition is commonly treated as visual instance retrieval on \"street view\" imagery. The dataset items and queries are panoramic views, i.e. groups of images taken at a single location. This work introduces a novel panorama-to-panorama matching process, either by aggregating features of individual images in a group or by explicitly constructing a larger panorama. In either case, multiple views are used as queries. We reach near perfect location recognition on a standard benchmark with only four query views.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["differences"]}}}
{"sentences": ["Kirchhoff index or equivalently effective graph resistance based measures have been instrumental in quantifying the effect of noise on the expected steady state dispersion in linear dynamical networks, particularly in the ones with the consensus dynamics, for instance see @cite_23 @cite_8 @cite_22 .", "Furthermore, limits on robustness measures that quantify expected steady-state dispersion due to external stochastic disturbances in linear dynamical networks are also studied in @cite_9 @cite_10 .", "To maximize robustness in networks by minimizing their Kirchhoff indices, various optimization approaches (e.g., @cite_26 @cite_1 ) including graph-theoretic ones @cite_0 have been proposed.", "The main objective there is to determine crucial edges that need to be added or maintained to maximize robustness under given constraints @cite_11 ."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: research objective"], "target_paper": "In this paper, we study the relationship between two crucial properties in linear dynamical networks of diffusively coupled agents, that is controllability and robustness to noise and structural changes in the network. In particular, for any given network size and diameter, we identify networks that are maximally robust and then analyze their strong structural controllability. We do so by determining the minimum number of leaders to make such networks completely controllable with arbitrary coupling weights between agents. Similarly, we design networks with the same given parameters that are completely controllable independent of coupling weights through a minimum number of leaders, and then also analyze their robustness. We utilize the notion of Kirchhoff index to measure network robustness to noise and structural changes. Our controllability analysis is based on novel graph-theoretic methods that offer insights on the important connection between network robustness and strong structural controllability in such networks.", "reference": {"@cite_26": {"mid": "1987717935", "abstract": "The effective resistance between two nodes of a weighted graph is the electrical resistance seen between the nodes of a resistor network with branch conductances given by the edge weights. The effective resistance comes up in many applications and fields in addition to electrical network analysis, including, for example, Markov chains and continuous-time averaging networks. In this paper we study the problem of allocating edge weights on a given graph in order to minimize the total effective resistance, i.e., the sum of the resistances between all pairs of nodes. We show that this is a convex optimization problem and can be solved efficiently either numerically or, in some cases, analytically. We show that optimal allocation of the edge weights can reduce the total effective resistance of the graph (compared to uniform weights) by a factor that grows unboundedly with the size of the graph. We show that among all graphs with @math nodes, the path has the largest value of optimal total effective resistance and the complete graph has the least.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2287065438", "abstract": "This work considers the robustness of uncertain consensus networks. The stability properties of consensus networks with negative edge weights are also examined. We show that the network is unstable if either the negative weight edges form a cut in the graph or any single negative edge weight has a magnitude less than the inverse of the effective resistance between the two incident nodes. These results are then used to analyze the robustness of the consensus network with additive but bounded perturbations of the edge weights. It is shown that the small-gain condition is related again to cuts in the graph and effective resistance. For the single edge case, the small-gain condition is also shown to be exact. The results are then extended to consensus networks with nonlinear couplings.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "1818569528", "abstract": "The graphical notion of effective resistance has found wide-ranging applications in many areas of pure mathematics, applied mathematics and control theory. By the nature of its construction, effective resistance can only be computed in undirected graphs and yet in several areas of its application, directed graphs arise as naturally (or more naturally) than undirected ones. In Part I of this work, we propose a generalization of effective resistance to directed graphs that preserves its control-theoretic properties in relation to consensus-type dynamics. We proceed to analyze the dependence of our algebraic definition on the structural properties of the graph and the relationship between our construction and a graphical distance. The results make possible the calculation of effective resistance between any two nodes in any directed graph and provide a solid foundation for the application of effective resistance to problems involving directed graphs.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_1": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2156286761", "abstract": "This paper studies an interesting graph measure that we call the effective graph resistance. The notion of effective graph resistance is derived from the field of electric circuit analysis where it is defined as the accumulated effective resistance between all pairs of vertices. The objective of the paper is twofold. First, we survey known formulae of the effective graph resistance and derive other representations as well. The derivation of new expressions is based on the analysis of the associated random walk on the graph and applies tools from Markov chain theory. This approach results in a new method to approximate the effective graph resistance. A second objective of this paper concerns the optimisation of the effective graph resistance for graphs with given number of vertices and diameter, and for optimal edge addition. A set of analytical results is described, as well as results obtained by exhaustive search. One of the foremost applications of the effective graph resistance we have in mind, is the analysis of robustness-related problems. However, with our discussion of this informative graph measure we hope to open up a wealth of possibilities of applying the effective graph resistance to all kinds of networks problems. \u00a9 2011 Elsevier Inc. All rights reserved.", "ref_function": ["background", "background", "objective", "method", "method", "background", "objective", "method", "method", "result", "other"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2099689250", "abstract": "In this paper we study robustness of consensus in networks of coupled single integrators driven by white noise. Robustness is quantified as the H 2 norm of the closed-loop system. In particular we investigate how robustness depends on the properties of the underlying (directed) communication graph. To this end several classes of directed and undirected communication topologies are analyzed and compared. The trade-off between speed of convergence and robustness to noise is also investigated.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2736121037", "abstract": "This paper investigates the robustness of strong structural controllability for linear time-invariant directed networked systems with respect to structural perturbations, including edge additions and deletions. In this regard, an algorithm is presented that is initiated by endowing each node of a network with a successive set of integers. Using this algorithm, a new notion of perfect graphs associated with a network is introduced, and tight upper bounds on the number of edges that can be added to, or removed from a network, while ensuring strong structural controllability, are derived. Moreover, we obtain a characterization of critical edges with respect to edge additions and deletions; these sets are the maximal sets of edges whose any subset can be respectively added to, or removed from a network, while preserving strong structural controllability.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["To quantify controllability, several approaches have been adapted, including determining the minimum number of inputs (leader nodes) needed to (structurally or strong structurally) control a network, determining the worst-case control energy, metrics based on controllability Gramians, and so on (e.g., see @cite_7 @cite_5 ).", "Strong structural controllability, due to its independence on coupling weights between nodes, is a generalized notion of controllability with practical implications.", "There have been recent studies providing graph-theoretic characterizations of this concept @cite_20 @cite_13 @cite_17 .", "There are numerous other studies regarding leader selection to optimize network performance measures under various constraints, such as to minimize the deviation from consensus in a noisy environment @cite_4 @cite_2 , and to maximize various controllability measures, for instance @cite_15 @cite_18 @cite_25 @cite_14 .", "Recently, optimization methods are also presented to select leader nodes that exploit submodularity properties of performance measures for network robustness and structural controllability @cite_5 @cite_3 ."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken"], "target_paper": "In this paper, we study the relationship between two crucial properties in linear dynamical networks of diffusively coupled agents, that is controllability and robustness to noise and structural changes in the network. In particular, for any given network size and diameter, we identify networks that are maximally robust and then analyze their strong structural controllability. We do so by determining the minimum number of leaders to make such networks completely controllable with arbitrary coupling weights between agents. Similarly, we design networks with the same given parameters that are completely controllable independent of coupling weights through a minimum number of leaders, and then also analyze their robustness. We utilize the notion of Kirchhoff index to measure network robustness to noise and structural changes. Our controllability analysis is based on novel graph-theoretic methods that offer insights on the important connection between network robustness and strong structural controllability in such networks.", "reference": {"@cite_18": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_14": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_4": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2111725629", "abstract": "This paper studies the problem of controlling complex networks, i.e., the joint problem of selecting a set of control nodes and of designing a control input to steer a network to a target state. For this problem, 1) we propose a metric to quantify the difficulty of the control problem as a function of the required control energy, 2) we derive bounds based on the system dynamics (network topology and weights) to characterize the tradeoff between the control energy and the number of control nodes, and 3) we propose an open-loop control strategy with performance guarantees. In our strategy, we select control nodes by relying on network partitioning, and we design the control input by leveraging optimal and distributed control techniques. Our findings show several control limitations and properties. For instance, for Schur stable and symmetric networks: 1) if the number of control nodes is constant, then the control energy increases exponentially with the number of network nodes; 2) if the number of control nodes is a fixed fraction of the network nodes, then certain networks can be controlled with constant energy independently of the network dimension; and 3) clustered networks may be easier to control because, for sufficiently many control nodes, the control energy depends only on the controllability properties of the clusters and on their coupling strength. We validate our results with examples from power networks, social networks and epidemics spreading.", "ref_function": ["background", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_2": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_5": {"mid": "1938602245", "abstract": "Controllability and observability have long been recognized as fundamental structural properties of dynamical systems, but have recently seen renewed interest in the context of large, complex networks of dynamical systems. A basic problem is sensor and actuator placement: choose a subset from a finite set of possible placements to optimize some real-valued controllability and observability metrics of the network. Surprisingly little is known about the structure of such combinatorial optimization problems. In this paper, we show that several important classes of metrics based on the controllability and observability Gramians have a strong structural property that allows for either efficient global optimization or an approximation guarantee by using a simple greedy heuristic for their maximization. In particular, the mapping from possible placements to several scalar functions of the associated Gramian is either a modular or submodular set function. The results are illustrated on randomly generated systems and on a problem of power-electronic actuator placement in a model of the European power grid.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_15": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2315383458", "abstract": "In this technical note, we study the controllability of diffusively coupled networks from a graph theoretic perspective. We consider leader-follower networks, where the external control inputs are injected to only some of the agents, namely the leaders. Our main result relates the controllability of such systems to the graph distances between the agents. More specifically, we present a graph topological lower bound on the rank of the controllability matrix. This lower bound is tight, and it is applicable to systems with arbitrary network topologies, coupling weights, and number of leaders. An algorithm for computing the lower bound is also provided. Furthermore, as a prominent application, we present how the proposed bound can be utilized to select a minimal set of leaders for achieving controllability, even when the coupling weights are unknown.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2049708951", "abstract": "This paper examines strong structural controllability of linear-time-invariant networked systems. We provide necessary and sufficient conditions for strong structural controllability involving constrained matchings over the bipartite graph representation of the network. An O(n2) algorithm to validate if a set of inputs leads to a strongly structurally controllable network and to find such an input set is proposed. The problem of finding such a set with minimal cardinality is shown to be NP-complete. Minimal cardinality results for strong and weak structural controllability are compared.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2763583074", "abstract": "Characterization of network controllability through its topology has recently gained a lot of attention in the systems and control community. Using the notion of balancing sets, in this note, such a network-centric approach for the controllability of certain families of undirected networks is investigated. Moreover, by introducing the notion of a generalized zero forcing set, the structural controllability of undirected networks is discussed; in this direction, lower bounds on the dimension of the controllable subspace are derived. In addition, a method is proposed that facilitates synthesis of structural and strong structural controllable networks as well as examining preservation of network controllability under structural perturbations.", "ref_function": ["background", "objective", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Very recently in @cite_21 , trade-off between controllability and fragility in complex networks is investigated.", "Fragility measures the smallest perturbation in edge weights to make the network unstable.", "Authors in @cite_21 show that networks that require small control energy, as measured by the eigen values of the controllability Gramian, to drive from one state to another are more fragile and vice versa.", "In our work, for control performance, we consider minimum leaders for strong structural controllability, which is independent of coupling weights; and for robustness, we utilize the Kirchhoff index which measures robustness to noise as well as to structural changes in the underlying network graph.", "Moreover, in this work we focus on designing and comparing extremal networks for these properties.", "The rest of the paper is organized as follows: Section describes preliminaries and network dynamics.", "Section explains the measures for robustness and controllability, and also outlines the main problems.", "Section presents maximally robust networks for a given @math and @math , and also analyzes their controllability.", "Section provides a design of maximally controllable networks and also evaluates their robustness.", "Finally, Section concludes the paper."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about result", "Describing used methods", "Describing used methods", "Signalling Transition", "Signalling Transition", "Signalling Transition", "Signalling Transition", "Signalling Transition"], "target_paper": "In this paper, we study the relationship between two crucial properties in linear dynamical networks of diffusively coupled agents, that is controllability and robustness to noise and structural changes in the network. In particular, for any given network size and diameter, we identify networks that are maximally robust and then analyze their strong structural controllability. We do so by determining the minimum number of leaders to make such networks completely controllable with arbitrary coupling weights between agents. Similarly, we design networks with the same given parameters that are completely controllable independent of coupling weights through a minimum number of leaders, and then also analyze their robustness. We utilize the notion of Kirchhoff index to measure network robustness to noise and structural changes. Our controllability analysis is based on novel graph-theoretic methods that offer insights on the important connection between network robustness and strong structural controllability in such networks.", "reference": {"@cite_21": {"mid": "2887109490", "abstract": "Mathematical theories and empirical evidence suggest that several complex natural and man-made systems are fragile: as their size increases, arbitrarily small and localized alterations of the system parameters may trigger system-wide failures. Examples are abundant, from perturbation of the population densities leading to extinction of species in ecological networks [1], to structural changes in metabolic networks preventing reactions [2], cascading failures in power networks [3], and the onset of epileptic seizures following alterations of structural connectivity among populations of neurons [4]. While fragility of these systems has long been recognized [5], convincing theories of why natural evolution or technological advance has failed, or avoided, to enhance robustness in complex systems are still lacking. In this paper we propose a mechanistic explanation of this phenomenon. We show that a fundamental tradeoff exists between fragility of a complex network and its controllability degree, that is, the control energy needed to drive the network state to a desirable state. We provide analytical and numerical evidence that easily controllable networks are fragile, suggesting that natural and man-made systems can either be resilient to parameters perturbation or efficient to adapt their state in response to external excitations and controls.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["Design-World is also based on the method used in Carletta's JAM simulation for the Edinburgh Map-Task @cite_10 .", "JAM is based on the Map-Task Dialogue corpus, where the goal of the task is for the planning agent, the instructor, to instruct the reactive agent, the instructee, how to get from one place to another on the map.", "JAM focuses on efficient strategies for recovery from error and parametrizes agents according to their communicative and error recovery strategies.", "Given good error recovery strategies, Carletta argues that high risk' strategies are more efficient, where efficiency is a measure of the number of utterances in the dialogue.", "While the focus here is different, we have shown that that the number of utterances is just one parameter for evaluating performance, and that the task definition determines when strategies are effective."], "label": ["Describing used methods", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about result", "Describing the results"], "target_paper": "Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents' resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents' resources and communicative strategies.", "reference": {"@cite_10": {"mid": "2159574206", "abstract": "The Principle of Parsimony states that people usually try to complete tasks with the least effort that will produce a satisfactory solution. In task-oriented dialogue, this produces a tension between conveying information carefully to the partner and leaving it to be inferred, risking a misunderstanding and the need for recovery. Using natural dialogue examples, primarily from the HCRC Map Task, we apply the Principle of Parsimony to a range of information types and identify a set of applicable recovery strategies. We argue that risk-taking and recovery are crucial for efficient dialogue because they pinpoint which information must be transferred and allow control of the interaction to switch to the participant who can best guide the course of the dialogue.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["uses"]}}}
{"sentences": ["We concentrate here on the related work involving addition of constraints to imperative languages.", "For an overview of related work pertaining to the language we refer the reader to @cite_0 ."], "label": ["General descriptions of the topic", "Signalling Transition"], "target_paper": "The aim of the Alma project is the design of a strongly typed constraint programming language that combines the advantages of logic and imperative programming. The first stage of the project was the design and implementation of Alma-0, a small programming language that provides a support for declarative programming within the imperative programming framework. It is obtained by extending a subset of Modula-2 by a small number of features inspired by the logic programming paradigm. In this paper we discuss the rationale for the design of Alma-0, the benefits of the resulting hybrid programming framework, and the current work on adding constraint processing capabilities to the language. In particular, we discuss the role of the logical and customary variables, the interaction between the constraint store and the program, and the need for lists.", "reference": {"@cite_0": {"mid": "1968265180", "abstract": "We describe here an implemented small programming language, called Alma-O, that augments the expressive power of imperative programming by a limited number of features inspired by the logic programming paradigm. These additions encourage declarative programming and make it a more attractive vehicle for problems that involve search. We illustrate the use of Alma-O by presenting solutions to a number of classical problems, including \u03b1-\u03b2 search, STRIPS planning, knapsack, and Eight Queens. These solutions are substantially simpler than their counterparts written in the imperative or in the logic programming style and can be used for different purposes without any modification. We also discuss here the implementation of Alma-O and an operational, executable, semantics of a large subset of the language.", "ref_function": ["background", "background", "method", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["To our knowledge, lexical databases have been used only once before in TC, apart from our previous work.", "Hearst @cite_10 adapted a disambiguation algorithm by Yarowsky using WordNet to recognize category occurrences.", "Categories are made of WordNet terms, which is not the general case of standard or user-defined categories.", "It is a hard task to adapt WordNet subsets to pre-existing categories, especially when they are domain dependent.", "Hearst's approach has shown promising results confirmed by our previous work @cite_23 and present results."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to current state of knowledge", "Reference to single investigations in the past: about result"], "target_paper": "Automatic Text Categorization (TC) is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection. We suggest the utilization of additional resources like lexical databases to increase the amount of information that TC systems make use of, and thus, to improve their performance. Our approach integrates WordNet information with two training approaches through the Vector Space Model. The training approaches we test are the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning) algorithms. Results obtained from evaluation show that the integration of WordNet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories.", "reference": {"@cite_10": {"mid": "1493108551", "abstract": "This dissertation investigates the role of contextual information in the automated retrieval and display of full-text documents, using robust natural language processing algorithms to automatically detect structure in and assign topic labels to texts. Many long texts are comprised of complex topic and subtopic structure, a fact ignored by existing information access methods. I present two algorithms which detect such structure, and two visual display paradigms which use the results of these algorithms to show the interactions of multiple main topics, multiple subtopics, and the relations between main topics and subtopics. The first algorithm, called TextTiling , recognizes the subtopic structure of texts as dictated by their content. It uses domain-independent lexical frequency and distribution information to partition texts into multi-paragraph passages. The results are found to correspond well to reader judgments of major subtopic boundaries. The second algorithm assigns multiple main topic labels to each text, where the labels are chosen from pre-defined, intuitive category sets; the algorithm is trained on unlabeled text. A new iconic representation, called TileBars uses TextTiles to simultaneously and compactly display query term frequency, query term distribution and relative document length. This representation provides an informative alternative to ranking long texts according to their overall similarity to a query. For example, a user can choose to view those documents that have an extended discussion of one set of terms and a brief but overlapping discussion of a second set of terms. This representation also allows for relevance feedback on patterns of term distribution. TileBars display documents only in terms of words supplied in the user query. For a given retrieved text, if the query words do not correspond to its main topics, the user cannot discern in what context the query terms were used. For example, a query on contaminants may retrieve documents whose main topics relate to nuclear power, food, or oil spills. To address this issue, I describe a graphical interface, called Cougar , that displays retrieved documents in terms of interactions among their automatically-assigned main topics, thus allowing users to familiarize themselves with the topics and terminology of a text collection.", "ref_function": ["background", "background", "method", "method", "method", "result", "method", "background", "background", "background", "method", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "1575569168", "abstract": "Automatic text categorization is a complex and useful task for many natural language processing applications. Recent approaches to text categorization focus more on algorithms than on resources involved in this operation. In contrast to this trend, we present an approach based on the integration of widely available resources as lexical databases and training collections to overcome current limitations of the task. Our approach makes use of WordNet synonymy information to increase evidence for bad trained categories. When testing a direct categorization, a WordNet based one, a training algorithm, and our integrated approach, the latter exhibits a better perfomance than any of the others. Incidentally, WordNet based approach perfomance is comparable with the training approach one.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Lexical databases have been employed recently in word sense disambiguation.", "For example, Agirre and Rigau @cite_4 make use of a semantic distance that takes into account structural factors in WordNet for achieving good results for this task.", "Additionally, Resnik @cite_3 combines the use of WordNet and a text collection for a definition of a distance for disambiguating noun groupings.", "Although the text collection is not a training collection (in the sense of a collection of manually labeled texts for a pre-defined text processing task), his approach can be regarded as the most similar to ours in the disambiguation setting.", "Finally, Ng and Lee @cite_11 make use of several sources of information inside a training collection (neighborhood, part of speech, morphological form, etc.)", "to get good results in disambiguating unrestricted text."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Automatic Text Categorization (TC) is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection. We suggest the utilization of additional resources like lexical databases to increase the amount of information that TC systems make use of, and thus, to improve their performance. Our approach integrates WordNet information with two training approaches through the Vector Space Model. The training approaches we test are the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning) algorithms. Results obtained from evaluation show that the integration of WordNet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories.", "reference": {"@cite_4": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_3": {"mid": "1608874027", "abstract": "Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve. However, for many tasks, one is interested in relationships among word senses, not words. This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns \u2014 the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2157025692", "abstract": "In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WORDNET.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The methods that are most similar to our techniques are the on-line algorithms used in @cite_16 and @cite_10 .", "In the first, two algorithms, a multiplicative update and additive update algorithms suggested in @cite_5 are evaluated in the domain, and are shown to perform somewhat better than Rocchio's algorithm.", "While both these works make use of multiplicative update algorithms, as we do, there are two major differences between those studies and the current one.", "First, there are some important technical differences between the algorithms used.", "Second, the algorithms we study here are mistake-driven; they update the weight vector only when a mistake is made, and not after every example seen.", "The Experts algorithm studied in @cite_10 is very similar to a basic version of the algorithm which we study here.", "The way we treat the negative weights is different, though, and significantly more efficient, especially in sparse domains (see ).", "Cohen and Singer experiment also, using the same algorithm, with more complex features (sparse n-grams) and show that, as expected, it yields better results."], "label": ["Explaining the method relationship between own work and references", "Reference to single investigations in the past: about result", "Explaining the method relationship between own work and references", "Explaining the method relationship between own work and references", "Explaining the method relationship between own work and references", "Explaining the method relationship between own work and references", "Describing used methods", "Reference to single investigations in the past: about method"], "target_paper": "Learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words. Three characteristic properties of this domain are (a) very high dimensionality, (b) both the learned concepts and the instances reside very sparsely in the feature space, and (c) a high variation in the number of active features in an instance. In this work we study three mistake-driven learning algorithms for a typical task of this nature -- text categorization. We argue that these algorithms -- which categorize documents by learning a linear separator in the feature space -- have a few properties that make them ideal for this domain. We then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain. In particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training. Overall, we present an algorithm, a variation of Littlestone's Winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set.", "reference": {"@cite_5": {"mid": "2069317438", "abstract": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+ -). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+ -) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+ -) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+ -) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data.", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["uses"]}, "@cite_16": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["similarities"]}, "@cite_10": {"mid": "2440833291", "abstract": "Two recently implemented machine-learning algorithms, RIPPER and sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the \u201ccontext\u201d of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences, both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information.", "ref_function": ["background", "background", "method", "result", "result"], "cite_purpose": ["similarities", "similarities"]}}}
{"sentences": ["Grosz, Sidner and Lochbaum @cite_7 @cite_1 developed a SharedPlan approach to modelling collaborative discourse, and Sidner formulated an artificial language for modeling such discourse.", "Sidner viewed a collaborative planning process as proposal acceptance and proposal rejection sequences.", "Her artificial language treats an utterance such as Why do X?", "as a proposal for the hearer to provide support for his proposal to do X.", "However, Sidner's work is descriptive and does not provide a mechanism for determining when and how such a proposal should be made nor how responses should be formulated in information-sharing subdialogues."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "In expert-consultation dialogues, it is inevitable that an agent will at times have insufficient information to determine whether to accept or reject a proposal by the other agent. This results in the need for the agent to initiate an information-sharing subdialogue to form a set of shared beliefs within which the agents can effectively re-evaluate the proposal. This paper presents a computational strategy for initiating such information-sharing subdialogues to resolve the system's uncertainty regarding the acceptance of a user proposal. Our model determines when information-sharing should be pursued, selects a focus of information-sharing among multiple uncertain beliefs, chooses the most effective information-sharing strategy, and utilizes the newly obtained information to re-evaluate the user proposal. Furthermore, our model is capable of handling embedded information-sharing subdialogues.", "reference": {"@cite_1": {"mid": "2148389694", "abstract": "A model of plan recognition in discourse must be based on intended recognition, distinguish each agent's beliefs and intentions from the other's, and avoid assumptions about the correctness or completeness of the agents' beliefs. In this paper, we present an algorithm for plan recognition that is based on the Shared-Plan model of collaboration (Grosz and Sidner, 1990; , 1990) and that satisfies these constraints.", "ref_function": ["background", "method"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "332028463", "abstract": "Abstract : Discourses are fundamentally instances of collaboration behavior. We propose a model of the collaborative plans of agents achieving joint goals and illustrate the role of these plans in discourses. Three types of collaborative plans, called Shared Plans, are formulated for joint goals requiring simultaneous, conjoined or sequential actions on the part of the agents who participate in the plans and the discourse; a fourth type of Shared Plan is presented for the circumstance where two agents communicate, but only one acts.", "ref_function": ["background", "objective", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Several researchers have studied the role of clarification dialogues in disambiguating user plans @cite_3 @cite_5 and in understanding referring expressions @cite_8 .", "developed an automated librarian that could revise its beliefs and intentions and could generate responses as an attempt to revise the user's beliefs and intentions.", "Although their system had rules for asking the user whether he holds a particular belief and for telling the system's attitude toward a belief, the emphasis of their work was on conflict resolution and plan disambiguation.", "Thus they did not investigate a comprehensive strategy for information-sharing during proposal evaluation.", "For example, they did not identify situations in which information-sharing is necessary, did not address how to select a focus of information-sharing when there are multiple uncertain beliefs, did not consider requesting the user's justifications for a belief, etc.", "In addition, they do not provide an overall dialogue planner that takes into account discourse structure and appropriately captures embedded subdialogues."], "label": ["General reference to previous research or scholarship: research objective", "Reference to single investigations in the past: about result", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "In expert-consultation dialogues, it is inevitable that an agent will at times have insufficient information to determine whether to accept or reject a proposal by the other agent. This results in the need for the agent to initiate an information-sharing subdialogue to form a set of shared beliefs within which the agents can effectively re-evaluate the proposal. This paper presents a computational strategy for initiating such information-sharing subdialogues to resolve the system's uncertainty regarding the acceptance of a user proposal. Our model determines when information-sharing should be pursued, selects a focus of information-sharing among multiple uncertain beliefs, chooses the most effective information-sharing strategy, and utilizes the newly obtained information to re-evaluate the user proposal. Furthermore, our model is capable of handling embedded information-sharing subdialogues.", "reference": {"@cite_5": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2037768686", "abstract": "Recognizing the plan underlying a query aids in the generation of an appropriate response. In this paper, we address the problem of how to generate cooperative responses when the user's plan is ambiguous. We show that it is not always necessary to resolve the ambiguity, and provide a procedure that estimates whether the ambiguity matters to the task of formulating a response. The procedure makes use of the critiquing of possible plans and identifies plans with the same fault. We illustrate the process of critiquing with examples. If the ambiguity does matter, we propose to resolve the ambiguity by entering into a clarification dialogue with the user and provide a procedure that performs this task. Together, these procedures allow a question-answering system to take advantage of the interactive and collaborative nature of dialogue in order to recognize plans and resolve ambiguity. This work therefore presents a view of generation in advice-giving contexts which is different from the straightforward model of a passive selection of responses to questions asked by users. We also report on a trial implementation in a course-advising domain, which provides insights on the practicality of the procedures and directions for future research.", "ref_function": ["background", "objective", "method", "method", "method", "method", "method", "objective", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "1541473376", "abstract": "This paper presents a computational model of how conversational participants collaborate in order to make a referring action successful. The model is based on the view of language as goal-directed behavior. We propose that the content of a referring expression can be accounted for by the planning paradigm. Not only does this approach allow the processes of building referring expressions and identifying their referents to be captured by plan construction and plan inference, it also allows us to account for how participants clarify a referring expression by using meta-actions that reason about and manipulate the plan derivation that corresponds to the referring expression. To account for how clarification goals arise and how inferred clarification plans affect the agent, we propose that the agents are in a certain state of mind, and that this state includes an intention to achieve the goal of referring and a plan that the agents are currently considering. It is this mental state that sanctions the adoption of goals and the acceptance of inferred plans, and so acts as a link between understanding and generation.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The results from our linguistic analysis are consistent with other research on sublanguages in the instructions domain, in both French and English, e.g., @cite_5 @cite_4 .", "Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator."], "label": ["Explaining the result relationship between own work and references", "Describing used methods"], "target_paper": "This paper presents an analysis conducted on a corpus of software instructions in French in order to establish whether task structure elements (the procedural representation of the users' tasks) are alone sufficient to control the grammatical resources of a text generator. We show that the construct of genre provides a useful additional source of control enabling us to resolve undetermined cases.", "reference": {"@cite_5": {"mid": "2047374384", "abstract": "This paper discusses an approach to planning the content of instructional texts. The research is based on a corpus study of 15 French procedural texts ranging from step-by-step device manuals to general artistic procedures. The approach taken starts from an AI task planner building a task representation, from which semantic carriers are selected. The most appropriate RST relations to communicate these carriers are then chosen according to heuristics developed during the corpus analysis.", "ref_function": ["objective", "background", "method", "method"], "cite_purpose": ["similarities"]}, "@cite_4": {"mid": "2131196772", "abstract": "Instructional texts have been the object of many studies recently, motivated by the increased need to produce manuals (especially multilingual manuals) coupled with the cost of translators and technical writers. Because these studies concentrate on aspects other than the linguistic realisation of instructions -- for example, the integration of text and graphics - they all generate a sequence of steps required to achieve a task, using imperatives. Our research so far shows, however, that manuals can in fact have different styles, i. e., not all instructions are stated using a sequence of imperatives, and that, furthermore, different parts of manuals often use different styles. In this paper, we present our preliminary results from an analysis of over 30 user guides manuals for consumer appliances and discuss some of the implications.", "ref_function": ["background", "background", "background", "result"], "cite_purpose": ["similarities"]}}}
{"sentences": ["A notion that is central to recent work on ellipsis, and which has been present in embryonic form, as we have seen, even in the early work on coordination, is that of parallelism as a key element in the determination of implicit meanings.", "Asher @cite_3 defines parallelism as"], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method"], "target_paper": "In previous work we studied a new type of DCGs, Datalog grammars, which are inspired on database theory. Their efficiency was shown to be better than that of their DCG counterparts under (terminating) OLDT-resolution. In this article we motivate a variant of Datalog grammars which allows us a meta-grammatical treatment of coordination. This treatment improves in some respects over previous work on coordination in logic grammars, although more research is needed for testing it in other respects.", "reference": {"@cite_3": {"mid": "1495022714", "abstract": "Preface. Introduction. 1. From Events to Propositions: a Tour of Abstract Entities, Eventualities and the Nominals that Denote them. 2. A Crash Course in DRT. 3. Attitudes and Attitude Descriptions. 4. The Semantic Representation for Sentential Nominals. 5. Problems for the Semantics of Nominals. 6. Anaphora and Abstract Entities. 7. A Theory of Discourse Structure for an Analysis of Abstract Entity Anaphora. 8. Applying the Theory of Discourse Structure to the Anaphoric Phenomena. 9. Applications of the Theory of Discourse Structure to Concept Anaphora and VP Ellipsis. 10. Model Theory for Abstract Entities and its Philosophical Implications. Conclusion. Bibliography. Index.", "ref_function": ["other", "other", "other", "background", "background", "other", "background", "background", "background", "background", "background", "result", "background", "background", "background", "background", "background", "method", "background", "background", "background", "background", "other", "other", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["a) neither method formulates exactly how parallelism is to be determined- it is just postulated as a prerequisite to the resolution of ellipsis (although @cite_6 speculates on possible ways of formulating this, leaving it for future work)"], "label": ["Reference to single investigations in the past: about method"], "target_paper": "In previous work we studied a new type of DCGs, Datalog grammars, which are inspired on database theory. Their efficiency was shown to be better than that of their DCG counterparts under (terminating) OLDT-resolution. In this article we motivate a variant of Datalog grammars which allows us a meta-grammatical treatment of coordination. This treatment improves in some respects over previous work on coordination in logic grammars, although more research is needed for testing it in other respects.", "reference": {"@cite_6": {"mid": "2119997945", "abstract": "We describe an implementation in Carpenter's typed feature formalism, ALE, of a discourse grammar of the kind proposed by Scha, Polanyi, et al We examine their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization. We describe an augmentation of the ALE system to encompass these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of VP-ellipsis which exhibit a strict sloppy ambiguity.", "ref_function": ["background", "method"], "cite_purpose": ["motivation", "background"]}}}
{"sentences": ["By examining ellipsis in the context of coordinated structures, which are parallel by definition, and by using extended DLGs, we provide a method in which parallel structures are detected and resolved through syntactic and semantic criteria, and which can be applied to either grammars using different semantic representations- feature structure, @math -calculus, or other.", "We exemplify using a logic based semantics along the lines of @cite_8 ."], "label": ["Describing used methods", "Describing used methods"], "target_paper": "In previous work we studied a new type of DCGs, Datalog grammars, which are inspired on database theory. Their efficiency was shown to be better than that of their DCG counterparts under (terminating) OLDT-resolution. In this article we motivate a variant of Datalog grammars which allows us a meta-grammatical treatment of coordination. This treatment improves in some respects over previous work on coordination in logic grammars, although more research is needed for testing it in other respects.", "reference": {"@cite_8": {"mid": "2006589508", "abstract": "Logic grammars are grammars expressible in predicate logic. Implemented in the programming language Prolog, logic grammar systems have proved to be a good basis for natural language processing. One of the most difficult constructions for natural language grammars to treat is coordination (construction with conjunctions like 'and'). This paper describes a logic grammar formalism, modifier structure grammars (MSGs), together with an interpreter written in Prolog, which can handle coordination (and other natural language constructions) in a reasonable and general way. The system produces both syntactic analyses and logical forms, and problems of scoping for coordination and quantifiers are dealt with. The MSG formalism seems of interest in its own right (perhaps even outside natural language processing) because the notions of syntactic structure and semantic interpretation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["similarities"]}}}
{"sentences": ["Ubiquitous computing @cite_4 proposes that very small computational devices (i.e., ubiquitous computers) be embedded and integrated into physical environments in such a way that they operate seamlessly and almost transparently.", "These devices are aware of their physical surroundings.", "In contrast to ubiquitous computers, our barcode (color-code) system is a low cost and reliable solution to making everything a computer.", "Suppose that every page in a book has a unique barcode.", "When the user opens a page, its page ID is detected by the system, so it can supply specific information regarding the page.", "When the user adds some information to the page, the system stores it with the page ID tagged for later retrieval.", "This is almost the same as having a computer in every page of the book without the cost.", "Our ID-aware system is better than ubiquitous computers from the viewpoint of reliability and cost-performance, since it does not require batteries and never breaks down."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Describing the results", "Describing the motivation", "Describing the motivation", "Describing the motivation", "Describing the motivation", "Describing the results"], "target_paper": "Augmented reality is a research area that tries to embody an electronic information space within the real world, through computational devices. A crucial issue within this area, is the recognition of real world objects or situations. In natural language processing, it is much easier to determine interpretations of utterances, even if they are ill-formed, when the context or situation is fixed. We therefore introduce robust, natural language processing into a system of augmented reality with situation awareness. Based on this idea, we have developed a portable system, called the Ubiquitous Talker. This consists of an LCD display that reflects the scene at which a user is looking as if it is a transparent glass, a CCD camera for recognizing real world objects with color-bar ID codes, a microphone for recognizing a human voice and a speaker which outputs a synthesized voice. The Ubiquitous Talker provides its user with some information related to a recognized object, by using the display and voice. It also accepts requests or questions as voice inputs. The user feels as if he she is talking with the object itself through the system.", "reference": {"@cite_4": {"mid": "2084069552", "abstract": "Ubiquitous computing is the method of enhancing computer use by making many computers available throughout the physical environment, but making them effectively invisible to the user. Since we started this work at Xerox PARC in 1988, a number of researchers around the world have begun to work in the ubiquitous computing framework. This paper explains what is new and different about the computer science in ubiquitous computing. It starts with a brief overview of ubiquitous computing, and then elaborates through a series of examples drawn from various subdisciplines of computer science: hardware components (e.g. chips), network protocols, interaction substrates (e.g. software for screens and pens), applications, privacy, and computational methods. Ubiquitous computing offers a framework for new and exciting research across the spectrum of computer science.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["From an applied angle, our observations on degeneracies and handling polynomial systems with infinitely many roots nicely complement the work of Emiris and Canny @cite_43 .", "In particular, their sparse resultant based algorithms for polynomial system solving can now be made to work even when problem B occurs.", "Also, an added benefit of working torically (as opposed to the classical approach of working in projective space) is the increased efficiency of the sparse resultant: the resulting matrix calculations (for polynomial system solving) are much smaller and faster.", "In particular, whereas it was remarked in @cite_41 that Gr \"obner basis methods are likely to be faster than the GCP for sparse polynomial systems, the toric GCP appears to be far more competitive in such a comparison."], "label": ["Explaining the result relationship between own work and references", "Explaining the method relationship between own work and references", "Describing the results", "Explaining the result relationship between own work and references"], "target_paper": "This paper reexamines univariate reduction from a toric geometric point of view. We begin by constructing a binomial variant of the @math -resultant and then retailor the generalized characteristic polynomial to fully exploit sparsity in the monomial structure of any given polynomial system. We thus obtain a fast new algorithm for univariate reduction and a better understanding of the underlying projections. As a corollary, we show that a refinement of Hilbert's Tenth Problem is decidable within single-exponential time. We also show how certain multisymmetric functions of the roots of polynomial systems can be calculated with sparse resultants.", "reference": {"@cite_41": {"mid": "1976392590", "abstract": "Multipolynomial resultants provide the most efficient methods known (in terms as asymptoticcomplexity) for solving certain systems of polynomial equations or eliminating variables (, 1988). The resultant of f\"1, ..., f\"n in K[x\"1,...,x\"m] will be a polynomial in m-n+1 variables which is zero when the system f\"1=0 has a solution in ^m ( the algebraic closure of K). Thus the resultant defines a projection operator from ^m to ^(^m^-^n^+^1^). However, resultants are only exact conditions for homogeneous systems, and in the affine case just mentioned, the resultant may be zero even if the system has no affine solution. This is most serious when the solution set of the system of polynomials has ''excess components'' (components of dimension >m-n), which may not even be affine, since these cause the resultant to vanish identically. In this paper we describe a projection operator which is not identically zero, but which is guaranteed to vanish on all the proper (dimension=m-n) components of the system f\"i=0. Thus it fills the role of a general affine projection operator or variable elimination ''black box'' which can be used for arbitrary polynomial systems. The construction is based on a generalisation of the characteristic polynomial of a linear system to polynomial systems. As a corollary, we give a single-exponential time method for finding all the isolated solution points of a system of polynomials, even in the presence of infinitely many solutions, at infinity or elsewhere.", "ref_function": ["background", "background", "background", "background", "background", "method", "method", "method", "method"], "cite_purpose": ["differences"]}, "@cite_43": {"mid": "2066130115", "abstract": "Abstract We propose a new and efficient algorithm for computing the sparse resultant of a system of n + 1 polynomial equations in n unknowns. This algorithm produces a matrix whose entries are coefficients of the given polynomials and is typically smaller than the matrices obtained by previous approaches. The matrix determinant is a non-trivial multiple of the sparse resultant from which the sparse resultant itself can be recovered. The algorithm is incremental in the sense that successively larger matrices are constructed until one is found with the above properties. For multigraded systems, the new algorithm produces optimal matrices, i.e. expresses the sparse resultant as a single determinant. An implementation of the algorithm is described and experimental results are presented. In addition, we propose an efficient algorithm for computing the mixed volume of n polynomials in n variables. This computation provides an upper bound on the number of common isolated roots. A publicly available implementation of the algorithm is presented and empirical results are reported which suggest that it is the fastest mixed volume code to date.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["similarities"]}}}
{"sentences": ["The simplest part-of-speech taggers are bigram or trigram models @cite_12 @cite_1 .", "They require a relatively large tagged training text.", "Transformation-based tagging as introduced by also requires a hand-tagged text for training.", "No pretagged text is necessary for Hidden Markov Models @cite_7 @cite_13 @cite_2 .", "Still, a lexicon is needed that specifies the possible parts of speech for every word.", "have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge", "Reference to current state of knowledge", "General reference to previous research or scholarship: research objective", "Reference to current state of knowledge", "Not sure"], "target_paper": "This paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types. The algorithm is evaluated on the Brown Corpus.", "reference": {"@cite_7": {"mid": "2100796029", "abstract": "Abstract A system for part-of-speech tagging is described. It is based on a hidden Markov model which can be trained using a corpus of untagged text. Several techniques are introduced to achieve robustness while maintaining high performance. Word equivalence classes are used to reduce the overall number of parameters in the model, alleviating the problem of obtaining reliable estimates for individual words. The context for category prediction is extended selectively via predefined networks, rather than using a uniformly higher-order conditioning which requires exponentially more parameters with increasing context. The networks are embedded in a first-order model and network structure is developed by analysis of erros, and also via linguistic considerations. To compensate for incomplete dictionary coverage, the categories of unknown words are predicted using both local context and suffix information to aid in disambiguation. An evaluation was performed using the Brown corpus and different dictionary arrangements were investigated. The techniques result in a model that correctly tags approximately 96 of the text. The flexibility of the methods is illustrated by their use in a tagging program for French.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2166394306", "abstract": "We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45 on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.", "ref_function": ["background", "method", "result", "result", "objective"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2046224275", "abstract": "We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96 . We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.", "ref_function": ["background", "method", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "1509596266", "abstract": "A consideration of problems engendered by the use of concordances to study additional word senses. The use of factor analysis as a research tool in lexicography is discussed. It is shown that this method provides information not obtainable through other approaches. This includes provision of several major senses for each word, an indication of the relationship between collocational patterns, & a more detailed analysis of the senses themselves. Sample factor analyses for the collocates of certain & right are presented & discussed. 3 Tables, 11 References. B. Annesser Murray", "ref_function": ["background", "background", "method", "method", "method", "other", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Spatial modeling of signal distributions using contour lines has been addressed in @cite_13 @cite_31 @cite_21 .", "Modeling the spatial distribution with uniformly spaced contour levels and tracking their variation using time-series analysis in sensors was studied in @cite_21 .", "Using non-uniformly spaced contour lines was reported first in @cite_31 .", "They assumed the probability density function () of the signal strength and used method to calculate the optimal sub-optimal contour levels.", "An iterative algorithm was proposed in @cite_13 to extract the of the signal strength with low cost in spatial monitoring of the signal distribution."], "label": ["General reference to previous research or scholarship: research objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "A significantly low cost and tractable progressive learning approach is proposed and discussed for efficient spatiotemporal monitoring of a completely unknown, two dimensional correlated signal distribution in localized wireless sensor field. The spatial distribution is compressed into a number of its contour lines and only those sensors that their sensor observations are in a @math margin of the contour levels are reporting to the information fusion center (IFC). The proposed algorithm progressively finds the model parameters in iterations, by using extrapolation in curve fitting, and stochastic gradient method for spatial monitoring. The IFC tracks the signal variations using these parameters, over time. The monitoring performance and the cost of the proposed algorithm are discussed, in this letter.", "reference": {"@cite_31": {"mid": "2085176360", "abstract": "This paper presents algorithms for efficiently detecting the variation of a distributed signal over space and time using large scale wireless sensor networks. The proposed algorithms use contours for estimating the spatial distribution of a signal. A contour tracking algorithm is proposed to efficiently monitor the variations of the contours with time. Use of contours reduces the communication cost by reducing the participation of sensor nodes for the monitoring tasks. The proposed schemes use multi-sensor collaboration techniques and non-uniform contour levels to reduce the error in reconstructing the signal distribution. Results from computer simulations are presented to demonstrate the performance of the proposed schemes.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_21": {"mid": "2147259327", "abstract": "In many applications of sensor networks, the sink needs to keep track of the history of sensed data of a monitored region for scientific analysis or supporting historical queries. We call these historical data a time series of value distributions or snapshots. Obviously, to build the time series snapshots by requiring all of the sensors to transmit their data to the sink periodically is not energy efficient. In this paper, we introduce the idea of gradient boundary and propose the gradient boundary detection (GBD) algorithm to construct these time series snapshots of a monitored region. In GBD, a monitored region is partitioned into a set of subregions and all sensed data in one subregion are within a predefined value range, namely, the gradient interval. Sensors located on the boundaries of the subregions are required to transmit the data to the sink and, then, the sink recovers all subregions to construct snapshots of the monitored area. In this process, only the boundary sensors transmit their data and, therefore, energy consumption is greatly reduced. The simulation results show that GBD is able to build snapshots with a comparable accuracy and has up to 40 percent energy savings compared with the existing approaches for large gradient intervals.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_13": {"mid": "2774751177", "abstract": "A novel on-demand, adaptive compressed sensing approach is proposed and discussed for spatial monitoring of correlated massive data in dense wireless sensor field. The spatial monitoring is based on collecting the sensor observations of a subset of sensors that their reading is in \u0394 margin of a set of contour levels. The contour level set is adaptively updated in order to reduce the mean square error of reconstruction of the spatial distribution. The reconstruction error and the number of involved sensing nodes as cost are used to compare the proposed approach with spatial monitoring using equally spaced contour levels.", "ref_function": ["background", "method", "method", "method"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["In a previous paper @cite_15 , we trained a neural network to disambiguate part-of-speech using context; however, no information about the word that is to be categorized was used.", "This scheme fails for cases like The soldiers rarely come home.''", "vs.", "The soldiers will come home.''", "where the context is identical and information about the lexical item in question ( rarely'' vs. will'') is needed in combination with context for correct classification.", "In this paper, we will compare two tagging algorithms, one based on classifying word types, and one based on classifying words-plus-context."], "label": ["Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Describing the objective"], "target_paper": "This paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types. The algorithm is evaluated on the Brown Corpus.", "reference": {"@cite_15": {"mid": "2163514362", "abstract": "This paper presents a method for inducing the parts of speech of a language and part-of-speech labels for individual words from a large text corpus. Vector representations for the part-of-speech of a word are formed from entries of its near lexical neighbors. A dimensionality reduction creates a space representing the syntactic categories of unambiguous words. A neural net trained on these spatial representations classifies individual contexts of occurrence of ambiguous words. The method classifies both ambiguous and unambiguous words correctly with high accuracy.", "ref_function": ["background", "method", "method", "method", "method"], "cite_purpose": ["motivation", "differences"]}}}
{"sentences": ["Magerman discussed the poor performance of his parser SPATTER on sentences with conjunctions @cite_9 .", "As a result, he augmented SPATTER's probabilistic model with an additional conjunction feature.", "However, he reported that though SPATTER's performance on conjoined sentences improves with the conjunction feature, a significant percentage is still misanalyzed, as the simple conjunction feature model finds it difficult to capture long distance dependencies."], "label": ["Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "In this paper, we propose a novel strategy which is designed to enhance the accuracy of the parser by simplifying complex sentences before parsing. This approach involves the separate parsing of the constituent sub-sentences within a complex sentence. To achieve that, the divide-and-conquer strategy first disambiguates the roles of the link words in the sentence and segments the sentence based on these roles. The separate parse trees of the segmented sub-sentences and the noun phrases within them are then synthesized to form the final parse. To evaluate the effects of this strategy on parsing, we compare the original performance of a dependency parser with the performance when it is enhanced with the divide-and-conquer strategy. When tested on 600 sentences of the IPSM'95 data sets, the enhanced parser saw a considerable error reduction of 21.2 in its accuracy.", "reference": {"@cite_9": {"mid": "1924403233", "abstract": "Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules. In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78 compared to the grammar-based parser's 69 .", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Jones explored another type of link words, the punctuations @cite_7 .", "He showed successfully that for longer sentences, a grammar which makes use of punctuation massively outperforms one which does not.", "Besides improving parsing accuracy, the use of punctuations also significantly reduces the number of possible parses generated.", "However, as theoretical forays into the syntactic roles of punctuation are limited, the grammar he designed can only cover a subset of all punctuation phenomena.", "Unexpected constructs thus cause the grammar to fail completely."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "In this paper, we propose a novel strategy which is designed to enhance the accuracy of the parser by simplifying complex sentences before parsing. This approach involves the separate parsing of the constituent sub-sentences within a complex sentence. To achieve that, the divide-and-conquer strategy first disambiguates the roles of the link words in the sentence and segments the sentence based on these roles. The separate parse trees of the segmented sub-sentences and the noun phrases within them are then synthesized to form the final parse. To evaluate the effects of this strategy on parsing, we compare the original performance of a dependency parser with the performance when it is enhanced with the divide-and-conquer strategy. When tested on 600 sentences of the IPSM'95 data sets, the enhanced parser saw a considerable error reduction of 21.2 in its accuracy.", "reference": {"@cite_7": {"mid": "2039117335", "abstract": "Few, if any, current NLP systems make any significant use of punctuation. Intuitively, a treatment of punctuation seems necessary to the analysis and production of text. Whilst this has been suggested in the fields of discourse structure, it is still unclear whether punctuation can help in the syntactic field. This investigation attempts to answer this question by parsing some corpus-based material with two similar grammars --- one including rules for punctuation, the other ignoring it. The punctuated grammar significantly out-performs the unpunctuated one, and so the conclusion is that punctuation can play a useful role in syntactic processing.", "ref_function": ["background", "background", "background", "objective", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["perform an analysis similar to ours as they also investigate convergence properties of a class probability estimator, their start and end point are very different though.", "While we start with theory from proper scoring rules, their paper directly starts with the class probability estimator as found in @cite_5 .", "The problem is that the estimator in @cite_5 only appears as a side remark, and it is unclear to which extent this is the best, only or even the correct choice.", "This paper contributes to close this gap and answers those questions.", "They show that the estimator converges to a unique class probability model.", "In relation to this one can view this paper as an investigation of this unique class probability model and we give necessary and sufficient conditions that lead to convergence to the true class probabilities.", "Note also that their paper uses convex methods, while our work in comparison draws from the theory of proper scoring rules."], "label": ["Explaining the objective relationship between own work and references", "Explaining the method relationship between own work and references", "Explaining the inadequacies of previous studies", "Describing the objective", "Reference to single investigations in the past: about result", "Describing used methods", "Explaining the method relationship between own work and references"], "target_paper": "In this work we investigate to which extent one can recover class probabilities within the empirical risk minimization (ERM) paradigm. The main aim of our paper is to extend existing results and emphasize the tight relations between empirical risk minimization and class probability estimation. Based on existing literature on excess risk bounds and proper scoring rules, we derive a class probability estimator based on empirical risk minimization. We then derive fairly general conditions under which this estimator will converge, in the L1-norm and in probability, to the true class probabilities. Our main contribution is to present a way to derive finite sample L1-convergence rates of this estimator for different surrogate loss functions. We also study in detail which commonly used loss functions are suitable for this estimation problem and finally discuss the setting of model-misspecification as well as a possible extension to asymmetric loss functions.", "reference": {"@cite_5": {"mid": "2023163512", "abstract": "We study how closely the optimal Bayes error rate can be approximately reached using a classification algorithm that computes a classifier by minimizing a convex upper bound of the classification error function. The measurement of closeness is characterized by the loss function used in the estimation. We show that such a classification scheme can be generally regarded as a (nonmaximum-likelihood) conditional in-class probability estimate, and we use this analysis to compare various convex loss functions that have appeared in the literature. Furthermore, the theoretical insight allows us to design good loss functions with desirable properties. Another aspect of our analysis is to demonstrate the consistency of certain classification methods using convex risk minimization. This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines. It also shows their limitations and suggests possible improvements.", "ref_function": ["background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["differences", "motivation"]}}}
{"sentences": ["The probability estimator we use also appears in @cite_10 where it is used to derive excess risk bounds, referred to as surrogate risk bounds, for bipartite ranking.", "The methods used are very similar in the sense that these are also based on proper scoring rules.", "The difference is again the focus, and even more so the conditions used.", "They introduce the notion of strongly proper scoring rules which directly allows one to bound the @math -norm, and thus the @math -norm, of the estimator in terms of the excess risk.", "We show that convergence can be achieved already under milder conditions.", "We then use the concept of modulus of continuity, of which strongly proper scoring rules are a particular case, to analyze the rate of convergence."], "label": ["Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references", "Explaining the method relationship between own work and references", "Reference to single investigations in the past: about method", "Describing the results", "Describing used methods"], "target_paper": "In this work we investigate to which extent one can recover class probabilities within the empirical risk minimization (ERM) paradigm. The main aim of our paper is to extend existing results and emphasize the tight relations between empirical risk minimization and class probability estimation. Based on existing literature on excess risk bounds and proper scoring rules, we derive a class probability estimator based on empirical risk minimization. We then derive fairly general conditions under which this estimator will converge, in the L1-norm and in probability, to the true class probabilities. Our main contribution is to present a way to derive finite sample L1-convergence rates of this estimator for different surrogate loss functions. We also study in detail which commonly used loss functions are suitable for this estimation problem and finally discuss the setting of model-misspecification as well as a possible extension to asymmetric loss functions.", "reference": {"@cite_10": {"mid": "2141789531", "abstract": "The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we show that such (nonpairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as strongly proper. Our proof technique is much simpler than that of (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact consistent for bipartite ranking; moreover, our results allow us to quantify the bipartite ranking regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clemencon and Robbiano (2011).", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["similarities"]}}}
{"sentences": ["On an unweighted graph, Gabow @cite_13 showed how to compute the minimum cut in @math time, where @math is the capacity of the minimum cut.", "Karger @cite_29 improved Gabow's algorithm by applying random sampling, achieving runtime @math The @math notation hides @math factors.", "Las Vegas.", "The sampling technique developed by Karger @cite_29 , combined with the tree-packing technique devised by Gabow @cite_13 , form the basis of Karger's near-linear time minimum cut algorithm @cite_16 .", "As previously mentioned, this technique finds the minimum cut in an undirected, weighted graph in @math time with high probability."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Not sure", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "We consider the minimum cut problem in undirected, weighted graphs. We give a simple algorithm to find a minimum cut that @math -respects (cuts two edges of) a spanning tree @math of a graph @math . This procedure can be used in place of the complicated subroutine given in Karger's near-linear time minimum cut algorithm (J. ACM, 2000). We give a self-contained version of Karger's algorithm with the new procedure, which is easy to state and relatively simple to implement. It produces a minimum cut on an @math -edge, @math -vertex graph in @math time with high probability. This performance matches that achieved by Karger, thereby matching the current state of the art.", "reference": {"@cite_29": {"mid": "2150516767", "abstract": "We use random sampling as a tool for solving undirected graph problems. We show that the sparse graph, or skeleton, that arises when we randomly sample a graph's edges will accurately approximate the value of all cuts in the original graph with high probability. This makes sampling effective for problems involving cuts in graphs. We present fast randomized (Monte Carlo and Las Vegas) algorithms for approximating and exactly finding minimum cuts and maximum flows in unweighted, undirected graphs. Our cut-approximation algorithms extend unchanged to weighted graphs while our weighted-graph flow algorithms are somewhat slower. Our approach gives a general paradigm with potential applications to any packing problem. It has since been used in a near-linear time algorithm for finding minimum cuts, as well as faster cut and flow algorithms. Our sampling theorems also yield faster algorithms for several other cut-based problems, including approximating the best balanced cut of a graph, finding a k-connected orientation of a 2k-connected graph, and finding integral multicommodity flows in graphs with a great deal of excess capacity. Our methods also improve the efficiency of some parallel cut and flow algorithms. Our methods also apply to the network design problem, where we wish to build a network satisfying certain connectivity requirements between vertices. We can purchase edges of various costs and wish to satisfy the requirements at minimum total cost. Since our sampling theorems apply even when the sampling probabilities are different for different edges, we can apply randomized rounding to solve network design problems. This gives approximation algorithms that guarantee much better approximations than previous algorithms whenever the minimum connectivity requirement is large. As a particular example, we improve the best approximation bound for the minimum k-connected subgraph problem from 1.85 to [math not displayed].", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_16": {"mid": "1964510837", "abstract": "We significantly improve known time bounds for solving the minimum cut problem on undirected graphs. We use a \"semiduality\" between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques. We give a randomized (Monte Carlo) algorithm that finds a minimum cut in an m -edge, n -vertex graph with high probability in O (m log 3 n ) time. We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O( m log 3 n ) time. This variant has an optimal RNC parallelization. Both variants improve on the previous best time bound of O ( n 2 log 3 n ). Other applications of the tree-packing approach are new, nearly tight bounds on the number of near-minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner.", "ref_function": ["background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2012287357", "abstract": "We present an algorithm that finds the edge connectivity ? of a graph having n vectices and m edges. The running time is O(? m log(n2 m)) for directed graphs and slightly less for undirected graphs, O(m+?2n log(n ?)). This improves the previous best time bounds, O(min mn, ?2n2 ) for directed graphs and O(?n2) for undirected graphs. We present an algorithm that finds k edge-disjoint arborescences on a directed graph in time O((kn)2). This improves the previous best time bound, O(kmn + k3n2). Unlike previous work, our approach is based on two theorems of Edmonds that link these two problems and show how they can be solved.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["A recent development uses low-conductance cuts to find the minimum cut in an undirected unweighted graph.", "This technique was introduced by Kawarabayashi and Thorup @cite_2 , who achieve near-linear deterministic time (estimated to be @math ).", "This was improved by Henzinger, Rao, and Wang @cite_31 , who achieve deterministic runtime @math .", "Although the algorithm of is more efficient than Karger's algorithm @cite_16 on unweighted graphs, the procedure, as well as the one it was based on @cite_2 are quite involved, thus making them largely impractical for implementation purposes."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Explaining the inadequacies of previous studies"], "target_paper": "We consider the minimum cut problem in undirected, weighted graphs. We give a simple algorithm to find a minimum cut that @math -respects (cuts two edges of) a spanning tree @math of a graph @math . This procedure can be used in place of the complicated subroutine given in Karger's near-linear time minimum cut algorithm (J. ACM, 2000). We give a self-contained version of Karger's algorithm with the new procedure, which is easy to state and relatively simple to implement. It produces a minimum cut on an @math -edge, @math -vertex graph in @math time with high probability. This performance matches that achieved by Karger, thereby matching the current state of the art.", "reference": {"@cite_31": {"mid": "2569104968", "abstract": "We study the problem of computing a minimum cut in a simple, undirected graph and give a deterministic O(m log2 n log log2 n) time algorithm. This improves both on the best previously known deterministic running time of O(m log12 n) (Kawarabayashi and Thorup [12]) and the best previously known randomized running time of O(m log3 n) (Karger [11]) for this problem, though Karger's algorithm can be further applied to weighted graphs. Our approach is using the Kawarabayashi and Thorup graph compression technique, which repeatedly finds low-conductance cuts. To find these cuts they use a diffusion-based local algorithm. We use instead a flow-based local algorithm and suitably adjust their framework to work with our flow-based subroutine. Both flow and diffusion based methods have a long history of being applied to finding low conductance cuts. Diffusion algorithms have several variants that are naturally local while it is more complicated to make flow methods local. Some prior work has proven nice properties for local flow based algorithms with respect to improving or cleaning up low conductance cuts. Our flow subroutine, however, is the first that is both local and produces low conductance cuts. Thus, it may be of independent interest.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "1964510837", "abstract": "We significantly improve known time bounds for solving the minimum cut problem on undirected graphs. We use a \"semiduality\" between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques. We give a randomized (Monte Carlo) algorithm that finds a minimum cut in an m -edge, n -vertex graph with high probability in O (m log 3 n ) time. We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O( m log 3 n ) time. This variant has an optimal RNC parallelization. Both variants improve on the previous best time bound of O ( n 2 log 3 n ). Other applications of the tree-packing approach are new, nearly tight bounds on the number of near-minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner.", "ref_function": ["background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2963972775", "abstract": "We present a deterministic algorithm that computes the edge-connectivity of a graph in near-linear time. This is for a simple undirected unweighted graph G with n vertices and m edges. This is the first o(mn) time deterministic algorithm for the problem. Our algorithm is easily extended to find a concrete minimum edge-cut. In fact, we can construct the classic cactus representation of all minimum cuts in near-linear time. The previous fastest deterministic algorithm by Gabow from STOC '91 took O(m+\u03bb2 n), where \u03bb is the edge connectivity, but \u03bb can be as big as n\u22121. Karger presented a randomized near-linear time Monte Carlo algorithm for the minimum cut problem at STOC\u201996, but the returned cut is only minimum with high probability. Our main technical contribution is a near-linear time algorithm that contracts vertex sets of a simple input graph G with minimum degree \u0394, producing a multigraph \u1e20 with O(m \u0394) edges, which preserves all minimum cuts of G with at least two vertices on each side. In our deterministic near-linear time algorithm, we will decompose the problem via low-conductance cuts found using PageRank a la Brin and Page (1998), as analyzed by Andersson, Chung, and Lang at FOCS\u201906. Normally, such algorithms for low-conductance cuts are randomized Monte Carlo algorithms, because they rely on guessing a good start vertex. However, in our case, we have so much structure that no guessing is needed.", "ref_function": ["background", "background", "method", "method", "method", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["Semantic segmentation of images has been the subject of many works in the past years.", "Recently, deep learning methods have largely outperformed previous ones.", "The method presented in @cite_16 was the first to propose an accurate end-to-end network for semantic segmentation.", "This method is based on an encoder in which each scale is used to compute the final segmentation.", "Only a few month later, the U-Net architecture @cite_20 was proposed for the semantic segmentation of medical images.", "This method is an encoder-decoder able to provide highly precise segmentation.", "These two methods have largely influenced recent works such as DeeplabV3+ @cite_11 that uses dilated convolutional layers and spatial pyramid pooling modules in an encoder-decoder structure to improve the quality of the prediction.", "Other approaches explore multi-scale architectures to produce and fuse segmentations performed at different scales @cite_15 @cite_7 .", "Most of these methods are able to produce very accurate results, on various types of images (medical, outdoor, indoor).", "The survey @cite_1 of CNNs methods for semantic segmentation provides a deep analysis of some recent techniques.", "This work demonstrates that a combination of various components would most likely improve segmentation results on wider classes of objects."], "label": ["General descriptions of the topic", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Explain the significance of references", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result"], "target_paper": "We propose LU-Net -- for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efficiently solves the problem as an image processing problem. We first extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can finally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efficient. In this way, we can exploit both the 3D nature of the data and the specificity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.", "reference": {"@cite_7": {"mid": "2611259176", "abstract": "We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2898743055", "abstract": "Majority of CNN architecture design is aimed at achieving high accuracy in public benchmarks by increasing the complexity. Typically, they are over-specified by a large margin and can be optimized by a factor of 10-100x with only a small reduction in accuracy. In spite of the increase in computational power of embedded systems, these networks are still not suitable for embedded deployment. There is a large need to optimize for hardware and reduce the size of the network by orders of magnitude for computer vision applications. This has led to a growing community which is focused on designing efficient networks. However, CNN architectures are evolving rapidly and efficient architectures seem to lag behind. There is also a gap in understanding the hardware architecture details and incorporating it into the network design. The motivation of this paper is to systematically summarize efficient design techniques and provide guidelines for an application developer. We also perform a case study by benchmarking various semantic segmentation algorithms for autonomous driving.", "ref_function": ["background", "background", "background", "background", "background", "background", "background", "objective", "objective"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2563705555", "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "1903029394", "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20 relative improvement to 62.2 mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "1901129140", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http: lmb.informatik.uni-freiburg.de people ronneber u-net .", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2964309882", "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89 and 82.1 without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: github.com tensorflow models tree master research deeplab.", "ref_function": ["background", "background", "objective", "method", "method", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Recently, SqueezeSeg, a novel approach for the semantic segmentation of a LiDAR point cloud represented as a spherical range-image @cite_14 , was proposed.", "This representation allows to perform the segmentation by using simple 2D convolutions, which lowers the computational cost while keeping good accuracy.", "The architecture is derived from the SqueezeNet image segmentation method @cite_13 .", "The intermediate layers are \"fire layers\", layers made of one squeeze module and one expansion module.", "Later on, the same authors improved this method in @cite_3 by adding a context aggregation module and by considering focal loss and batch normalization to improve the quality of the segmentation.", "A similar range-image approach was proposed in @cite_17 , where a Atrous Spatial Pyramid Pooling @cite_4 and squeeze reweighting layer @cite_8 are added.", "Finally, in @cite_10 , the authors offer to input a range-image directly to the U-Net architecture described in @cite_20 .", "This method achieves results that are comparable to the state of the art of range-image methods with a much simpler and more intuitive architecture.", "All these range-image methods succeed in real-time computation.", "However, their results often lack of accuracy which limits their usage in real scenarios."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Explaining the inadequacies of previous studies"], "target_paper": "We propose LU-Net -- for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efficiently solves the problem as an image processing problem. We first extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can finally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efficient. In this way, we can exploit both the 3D nature of the data and the specificity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.", "reference": {"@cite_14": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2412782625", "abstract": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First , we highlight convolution with upsampled filters, or \u2018atrous convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second , we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third , we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.", "ref_function": ["objective", "method", "method", "method", "method", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["extends"]}, "@cite_8": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["extends"]}, "@cite_10": {"mid": "2946747865", "abstract": "This paper proposes RIU-Net (for Range-Image U-Net), the adaptation of a popular semantic segmentation network for the semantic segmentation of a 3D LiDAR point cloud. The point cloud is turned into a 2D range-image by exploiting the topology of the sensor. This image is then used as input to a U-net. This architecture has already proved its efficiency for the task of semantic segmentation of medical images. We demonstrate how it can also be used for the accurate semantic segmentation of a 3D LiDAR point cloud and how it represents a valid bridge between image processing and 3D point cloud processing. Our model is trained on range-images built from KITTI 3D object detection dataset. Experiments show that RIU-Net, despite being very simple, offers results that are comparable to the state-of-the-art of range-image based methods. Finally, we demonstrate that this architecture is able to operate at 90fps on a single GPU, which enables deployment for real-time segmentation.", "ref_function": ["background", "method", "method", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2968557240", "abstract": "Earlier work demonstrates the promise of deep-learning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2. With an improved model structure, SqueezeSetV2 is more robust against dropout noises in LiDAR point cloud and therefore achieves significant accuracy improvement. Training models for point cloud segmentation requires large amounts of labeled data, which is expensive to obtain. To sidestep the cost of data collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. Existing domain-adaptation methods mainly focus on images and most of them cannot be directly applied to point clouds. We address this problem with a domain-adaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6 over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0 to 57.4 . Our source code and synthetic dataset are open sourced11https: github.com xuanyuzhou98 SqueezeSegV2", "ref_function": ["background", "objective", "method", "background", "result", "background", "background", "method", "method", "result", "other"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2279098554", "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_20": {"mid": "1901129140", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http: lmb.informatik.uni-freiburg.de people ronneber u-net .", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2884355388", "abstract": "In this paper, we propose PointSeg, a real-time end-to-end semantic segmentation method for road-objects based on spherical images. We take the spherical image, which is transformed from the 3D LiDAR point clouds, as input of the convolutional neural networks (CNNs) to predict the point-wise semantic map. To make PointSeg applicable on a mobile system, we build the model based on the light-weight network, SqueezeNet, with several improvements. It maintains a good balance between memory cost and prediction performance. Our model is trained on spherical images and label masks projected from the KITTI 3D object detection dataset. Experiments show that PointSeg can achieve competitive accuracy with 90fps on a single GPU 1080ti. which makes it quite compatible for autonomous driving applications.", "ref_function": ["background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["extends"]}}}
{"sentences": ["Several temporal logics have been proposed that make joint use of actions and propositions on states: ACTL* @cite_34 , RLTL @cite_0 , SE-LTL @cite_4 , TLR* @cite_25 .", "There are also transition structures with mixed ingredients: LKS @cite_4 , L2TS @cite_54 .", "Although all of them bring actions (or transitions) to the focus, none tries to be utterly egalitarian, as we do."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_4": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}, "@cite_54": {"mid": "2167352300", "abstract": "Three temporal logics are introduced which induce on labeled transition systems the same identifications as branching bisimulation. The first is an extension of Hennessy-Milner logic with a kind of unit operator. The second is another extension of Hennessy-Milner logic which exploits the power of backward modalities. The third is CTL* with the next-time operator interpreted over all paths, not just over maximal ones. A relevant side effect of the last characterization is that it sets a bridge between the state- and event-based approaches to the semantics of concurrent systems. >", "ref_function": ["background", "background", "method", "method", "other", "other"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2024215898", "abstract": "We study efficient translations of Regular Linear Temporal Logic ( ) into automata on infinite words. is a temporal logic that fuses Linear Temporal Logic (LTL) with regular expressions, extending its expressive power to all @math -regular languages. The first contribution of this paper is a novel bottom up translation from into alternating parity automata of linear size that requires only colors @math , @math and @math . Moreover, the resulting automata enjoy the stratified internal structure of hesitant automata. Our translation is defined inductively for every operator, and does not require an upfront transformation of the expression into a normal form. Our construction builds at every step two automata: one equivalent to the formula and another to its complement. Inspired by this construction, our second contribution is to extend with new operators, including universal sequential composition, that enrich the logic with duality laws and negation normal forms. The third contribution is a ranking translation of the resulting alternating automata into non-deterministic automata. To provide this efficient translation we introduce the notion of stratified rankings, and show how the translation is optimal for the LTL fragment of the logic.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "1602925513", "abstract": "A temporal logic based on actions rather than on states is presented and interpreted over labelled transition systems. It is proved that it has essentially the same power as CTL*, a temporal logic interpreted over Kripke structures. The relationship between the two logics is established by introducing two mappings from Kripke structures to labelled transition systems and viceversa and two transformation functions between the two logics which preserve truth. A branching time version of the action based logic is also introduced. This new logic for transition systems can play an important role as an intermediate between Hennessy-Milner Logic and the modal \u03bc-calculus. It is sufficiently expressive to describe safety and liveness properties but permits model checking in linear time.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "1599831144", "abstract": "This paper presents the temporal logic of rewriting @math . Syntactically, @math is a very simple extension of @math which just adds action atoms, in the form of spatial action patterns, to @math . Semantically and pragmatically, however, when used together with rewriting logic as a \"tandem\" of system specification and property specification logics, it has substantially more expressive power than purely state-based logics like @math , or purely action-based logics like A- @math . Furthermore, it avoids the system property mismatch problem experienced in state-based or action-based logics, which makes many useful properties inexpressible in those frameworks without unnatural changes to a system's specification. The advantages in expresiveness of @math are gained without losing the ability to use existing tools and algorithms to model check its properties: a faithful translation of models and formulas is given that allows verifying @math properties with @math model checkers.", "ref_function": ["background", "background", "background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Action recognition in the deep learning era has been successfully tackled with 2D @cite_12 @cite_1 and 3D CNNs @cite_31 @cite_29 @cite_0 @cite_28 @cite_40 @cite_15 @cite_30 .", "Most existing works focus on modeling of motion and temporal structures.", "@cite_12 the optical flow CNN is introduced to model short-term motion patterns.", "TSN @cite_7 models long-range temporal structures using a sparse segment sampling in the whole video during training.", "3D CNN based models @cite_31 @cite_0 @cite_28 @cite_15 tackle the temporal modeling using the added dimension of the convolution on the temporal axis, in the hope that the models will learn the hierarchical motion patters as in the image space.", "Several recent works have started to decouple the spatial and temporal convolution in 3D CNNs to achieve more explicit temporal modeling @cite_29 @cite_14 @cite_30 .", "In @cite_16 @cite_17 the temporal modeling is further improved by tracking feature points or body joints over time."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Action recognition has seen a dramatic performance improvement in the last few years. Most of the current state-of-the-art literature either aims at improving performance through changes to the backbone CNN network, or they explore different trade-offs between computational efficiency and performance, again through altering the backbone network. However, almost all of these works maintain the same last layers of the network, which simply consist of a global average pooling followed by a fully connected layer. In this work we focus on how to improve the representation capacity of the network, but rather than altering the backbone, we focus on improving the last layers of the network, where changes have low impact in terms of computational cost. In particular, we show that current architectures have poor sensitivity to finer details and we exploit recent advances in the fine-grained recognition literature to improve our model in this aspect. With the proposed approach, we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major large-scale action recognition benchmarks.", "reference": {"@cite_30": {"mid": "2883429621", "abstract": "Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level \u201csemantic\u201d features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).", "ref_function": ["background", "background", "background", "background", "objective", "method", "result", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_14": {"mid": "2963155035", "abstract": "In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block \"R(2+1)D\" which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2963315828", "abstract": "Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0 ), UCF101 (94.9 ), THUMOS14 (80.1 ), and ActivityNet v1.2 (89.6 ). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0 ) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}, "@cite_28": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}, "@cite_29": {"mid": "2883534172", "abstract": "In this paper, we aim to reduce the computational cost of spatio-temporal deep neural networks, making them run as fast as their 2D counterparts while preserving state-of-the-art accuracy on video recognition benchmarks. To this end, we present the novel Multi-Fiber architecture that slices a complex neural network into an ensemble of lightweight networks or fibers that run through the network. To facilitate information flow between fibers we further incorporate multiplexer modules and end up with an architecture that reduces the computational cost of 3D networks by an order of magnitude, while increasing recognition performance at the same time. Extensive experimental results show that our multi-fiber architecture significantly boosts the efficiency of existing convolution networks for both image and video recognition tasks, achieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics datasets. Our proposed model requires over 9 ( ) and 13 ( ) less computations than the I3D [1] and R(2+1)D [2] models, respectively, yet providing higher accuracy.", "ref_function": ["objective", "objective", "method", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_1": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_0": {"mid": "1983364832", "abstract": "We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_40": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2963524571", "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2 on HMDB-51 and 97.9 on UCF-101.", "ref_function": ["background", "objective", "background", "objective", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_16": {"mid": "2891446678", "abstract": "How to leverage the temporal dimension is a key question in video analysis. Recent work suggests an efficient approach to video feature learning, namely, factorizing 3D convolutions into separate components respectively for spatial and temporal convolutions. The temporal convolution, however, comes with an implicit assumption \u2013 the feature maps across time steps are well aligned so that the features at the same locations can be aggregated. This assumption may be overly strong in practical applications, especially in action recognition where the motion of people or objects is a crucial aspect. In this work, we propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the standard temporal convolution. This operation explicitly takes into account the location changes caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths. On two very large-scale action recognition datasets, namely, Something-Something and Kinetics, the proposed network architecture achieves notable improvement over strong baselines.", "ref_function": ["background", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2156303437", "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.", "ref_function": ["background", "objective", "objective", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_17": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["Plain TLR*, as described in @cite_25 , stays a step away from our goal, because transitions are given by proof terms, that univocally determine one origin state and one destination state for each transition.", "TLR* uses proof-term patterns (called ), that are used literally in temporal formulas.", "The problem is that, in this way, a TLR* formula is tied to a particular algebraic specification (one in which the pattern makes sense).", "In contrast, an LTL or CTL formula is meaningful by itself and can be used on any system specification by using atomic proposition definitions as interfaces.", "Notably, propositions on transitions have been added to plain TLR*, in some way or another, in all the implementations of model checkers for (the linear-time subset of) TLR* that we are aware of @cite_36 @cite_51 @cite_14 @cite_9 .", "None of them, however, tries to allow a same proposition to be defined both in states and in transitions, which we need for flexible synchronization."], "label": ["Explaining the objective relationship between own work and references", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Not sure", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_14": {"mid": "1986424898", "abstract": "This paper presents the linear temporal logic of rewriting (LTLR) model checker under localized fairness assumptions for the Maude system. The linear temporal logic of rewriting extends linear temporal logic (LTL) with spatial action patterns that describe patterns of rewriting events. Since LTLR generalizes and extends various state-based and event-based logics, mixed properties involving both state propositions and actions, such as fairness properties, can be naturally expressed in LTLR. However, often the needed fairness assumptions cannot even be expressed as propositional temporal logic formulas because they are parametric, that is, they correspond to universally quantified temporal logic formulas. Such universal quantification is succinctly captured by the notion of localized fairness; for example, fairness is localized to the object name parameter in object fairness conditions. We summarize the foundations, and present the language design and implementation of the Maude Fair LTLR model checker, developed at the C++ level within the Maude system by extending the existing Maude LTL model checker. Our tool provides not only an efficient LTLR model checking algorithm under parameterized fairness assumptions but also suitable specification languages as part of its user interface. The expressiveness and effectiveness of the Maude Fair LTLR model checker are illustrated by five case studies. This is the first tool we are aware of that can model check temporal logic properties under parameterized fairness assumptions. We develop the LTLR model checker under localized fairness assumptions.The linear temporal logic of rewriting (LTLR) extends LTL with action patterns.Localized fairness specifies parameterized fairness over generic system entities.We present the foundations, the language design, and the implementation of our tool.We illustrate the expressiveness and effectiveness of our tool with case studies.", "ref_function": ["background", "background", "background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_36": {"mid": "1583068981", "abstract": "This paper presents the foundation, design, and implementation of the Linear Temporal Logic of Rewriting model checker as an extension of the Maude system. The Linear Temporal Logic of Rewriting (LTLR) extends linear temporal logic with spatial action patterns which represent rewriting events. LTLR generalizes and extends various state-based and event-based logics and aims to avoid certain types of mismatches between a system and its temporal logic properties. We have implemented the LTLR model checker at the C++ level within the Maude system by extending the existing Maude LTL model checker. Our LTLR model checker provides very expressive methods to define event-related properties as well as state-related properties, or, more generally, properties involving both events and state predicates. This greater expressiveness is gained without compromising performance, because the LTLR implementation minimizes the extra costs involved in handling the events of systems.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_51": {"mid": "2151284417", "abstract": "This paper presents a model checker for LTLR, a subset of the temporal logic of rewriting TLR* extending linear temporal logic with spatial action patterns. Both LTLR and TLR* are very expressive logics generalizing well-known state-based and action-based logics. Furthermore, the semantics of TLR* is given in terms of rewrite theories, so that the concurrent systems on which the LTLR properties are model checked can be specified at a very high level with rewrite rules. This paper answers a nontrivial challenge, namely, to be able to build a model checker to model check LTLR formulas on rewrite theories with relatively little effort by reusing [email protected]?s LTL model checker for rewrite theories. For this, the reflective features of both rewriting logic and its Maude implementation have proved extremely useful.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "1599831144", "abstract": "This paper presents the temporal logic of rewriting @math . Syntactically, @math is a very simple extension of @math which just adds action atoms, in the form of spatial action patterns, to @math . Semantically and pragmatically, however, when used together with rewriting logic as a \"tandem\" of system specification and property specification logics, it has substantially more expressive power than purely state-based logics like @math , or purely action-based logics like A- @math . Furthermore, it avoids the system property mismatch problem experienced in state-based or action-based logics, which makes many useful properties inexpressible in those frameworks without unnatural changes to a system's specification. The advantages in expresiveness of @math are gained without losing the ability to use existing tools and algorithms to model check its properties: a faithful translation of models and formulas is given that allows verifying @math properties with @math model checkers.", "ref_function": ["background", "background", "background", "method", "result"], "cite_purpose": ["differences"]}}}
{"sentences": ["Our paper @cite_5 contains a first definition of the synchronous composition of rewrite systems.", "There, we proposed to synchronise the execution of rules from different systems based on the coincidence of (atomic) rule labels.", "This reflects the synchronisation of actions in process algebras and in automata, for example.", "We also proposed to synchronise states by agreement on the Boolean values of propositions defined on them.", "We implemented that concept of synchronisation on Maude.", "That proposal had the advantage that it used standard machinery already existing in Maude: rule labels are basic elements of Maude's syntax, and propositions are customarily defined and used to build LTL formulas to be used with Maude's model checker.", "Why is the present, much more involved paper needed?", "We refer the reader to .", "In short: Boolean-valued propositions are not enough to allow flexible synchronisation and value-passing; we need to give more substance to transitions; we want to be able to synchronise an action at one system with several consecutive ones at the other system.", "A complex realistic example like the one on the alternating bit protocol in @cite_33 would not be possible in our previous setting."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Not sure", "Signalling Transition", "Describing the motivation", "Explaining the inadequacies of previous studies"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_5": {"mid": "2521108378", "abstract": "We present a concept of module composition for rewrite systems that we call synchronous product, and also a corresponding concept for doubly labeled transition systems (as proposed by De Nicola and Vaandrager) used as semantics for the former. In both cases, synchronization happens on states and on transitions, providing in this way more flexibility and more natural specifications. We describe our implementation in Maude, a rewriting logic-based language and system. A series of examples shows their use for modular specification and hints at other possible uses, including modular verification.", "ref_function": ["background", "background", "method", "method"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2890370072", "abstract": "Our overall goal is compositional specification and verification in rewriting logic. In previous work, we described a way to compose system specifications using the operation we call synchronous composition. In this paper, we propose the use of parameterized programming to encapsulate and handle specifications: theories represent interfaces; modules parameterized by such theories instruct on how to assemble the parameter systems using the synchronous composition operation; the implementation of the whole system is then obtained by instantiating the parameters with implementations for the components. We show, and illustrate with examples, how this setting facilitates compositionality.", "ref_function": ["objective", "background", "method", "result"], "cite_purpose": ["differences"]}}}
{"sentences": ["In a different topic, the paper @cite_33 also describes the use of parameterised programming to add encapsulation to our setting.", "We have already mentioned it in .", "We outline it roughly refering to the example from , on two controlled trains.", "First, a so-called theory is used to state that a train is any system that defines a Boolean-valued property called \"isMoving\".", "Requirements for reckoners and controllers are similarly stated.", "These are our interface specifications.", "The composition is specified in a parameterised module, whose formal parameters are the theories (that is, the interfaces).", "Thus, the composition can only be specified using the formal names and the properties in the interfaces.", "The particular implementations of trains and the other components are written and the needed properties are defined.", "Finally, the parameters of the composition module are instantiated with the component implementations, producing an implementation of the complete system."], "label": ["Reference to single investigations in the past:  about objective", "Signalling Transition", "Not sure", "Other functional sentences", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_33": {"mid": "2890370072", "abstract": "Our overall goal is compositional specification and verification in rewriting logic. In previous work, we described a way to compose system specifications using the operation we call synchronous composition. In this paper, we propose the use of parameterized programming to encapsulate and handle specifications: theories represent interfaces; modules parameterized by such theories instruct on how to assemble the parameter systems using the synchronous composition operation; the implementation of the whole system is then obtained by instantiating the parameters with implementations for the components. We show, and illustrate with examples, how this setting facilitates compositionality.", "ref_function": ["objective", "background", "method", "result"], "cite_purpose": ["similarities"]}}}
{"sentences": ["Process algebras were initially designed as theoretical tools.", "They focus on actions and synchronisation, and do not provide any means to specify internal computations, or to handle complex data types.", "However, later developments have taken process algebras as a basis for practical modelling and verification tools.", "Examples are occam @cite_19 , SCEL @cite_18 , FSP+LTSA @cite_45 , CSP @math B @cite_27 , and LOTOS and the CADP tool @cite_49 ."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "General descriptions of the topic", "General reference to previous research or scholarship: research objective"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_18": {"mid": "24701999", "abstract": "SCEL (Service Component Ensemble Language) is a new language specifically designed to rigorously model and program autonomic components and their interaction, while supporting formal reasoning on their behaviors. SCEL brings together various programming abstractions that allow one to directly represent aggregations, behaviors and knowledge according to specific policies. It also naturally supports programming interaction, self-awareness, context-awareness, and adaptation. The solid semantic grounds of the language is exploited for developing logics, tools and methodologies for formal reasoning on system behavior to establish qualitative and quantitative properties of both the individual components and the overall systems.", "ref_function": ["background", "background", "method", "method"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2110425399", "abstract": "This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.", "ref_function": ["background", "background", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "1532163655", "abstract": "ProB is a model checking tool for the B Method. In this paper we present an extension of ProB that supports checking of specifications written in a combination of CSP and B. We explain how the notations are combined semantically and give an overview of the implementation of the combination. We illustrate the benefit that appropriate use of CSP, in conjunction with our tool, gives to B developments both for specification and for verification purposes.", "ref_function": ["background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_45": {"mid": "1995830301", "abstract": "Concurrency provides a thoroughly updatedapproach to the basic concepts and techniques behind concurrent programming. Concurrent programming is complex and demands a much more formal approach than sequential programming. In order to develop a thorough understanding of the topicMagee and Kramer present concepts, techniques and problems through a variety of forms: informal descriptions, illustrative examples, abstract models and concrete Java examples. These combineto provide problem patterns and associated solution techniqueswhich enablestudents torecognise problems and arrive at solutions. New features include: New chapters covering program verification and logical properties. More student exercises. Supporting website contains an updated version of the LTSA tool for modelling concurrency, model animation, and model checking. Website also includes the full set of state models, java examples, and demonstration programs and a comprehensive set of overhead slides for course presentation.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_49": {"mid": "2757839910", "abstract": "We revisit the early publications of Ed Brinksma devoted, on the one hand, to the definition of the formal description technique LOTOS (ISO International Standard 8807:1989) for specifying communication protocols and distributed systems, and, on the other hand, to two proposals (Extended LOTOS and Modular LOTOS) for making LOTOS a simpler and more expressive language. We examine how this scientific agenda has been dealt with during the last decades. We review the successive enhancements of LOTOS that led to the definition of three languages: E-LOTOS (ISO International Standard 15437:2001), then LOTOS NT, and finally LNT. We present the software implementations (compilers and translators) developed for these new languages and report about their use in various application domains.", "ref_function": ["background", "objective", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Typically, there are two ways to compose Petri nets.", "One is given by hierarchical nets, that is, nets in which a transition can represent a complete separate net, that is described independently.", "The second way is to identify, or , either places or transitions from two different nets.", "For example, the coffee machine and the scientist can be modelled and then composed by fusing transitions like this: Some approaches propose the introduction of interfaces, that allow to see each component net as a black box.", "That is the case of the recent work described in @cite_29 ."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Other reference purpose"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_29": {"mid": "2016355163", "abstract": "A quite fourishing research thread in the recent literature on component based system is concerned with the algebraic properties of different classes of connectors. In a recent paper, an algebra of stateless connectors was presented that consists of five kinds of basic connectors, namely symmetry, synchronization, mutual exclusion, hiding and inaction, plus their duals and it was shown how they can be freely composed in series and in parallel to model sophisticated \"glues\". In this paper we explore the expressiveness of stateful connectors obtained by adding one-place buffers or unbounded buffers to the stateless connectors. The main results are: i) we show how different classes of connectors exactly correspond to suitable classes of Petri nets equipped with compositional interfaces, called nets with boundaries; ii) we show that the difference between strong and weak semantics in stateful connectors is reflected in the semantics of nets with boundaries by moving from the classic step semantics (strong case) to a novel banking semantics (weak case), where a step can be executed by taking some \"debit\" tokens to be given back during the same step; iii) we show that the corresponding bisimilarities are congruences (w.r.t. composition of connectors in series and in parallel); iv) we show that suitable monoidality laws, like those arising when representing stateful connectors in the tile model, can nicely capture concurrency aspects; and v) as a side result, we provide a basic algebra, with a finite set of symbols, out of which we can compose all P T nets, fulfilling a long standing quest.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Tile logic was introduced in @cite_28 , and is closely related to rewriting logic.", "In short, tile logic is rewriting logic with side effects for composition.", "A tile is written as @math with @math being the condition for, and @math the effect of, rewriting @math to @math .", "The intuitive meaning of that tile is: the term @math is rewritten to the term @math , producing an effect @math , but the rewrite can only happen if the variables of @math (that represent as yet unspecified subcomponents) are rewritten with a cumulative effect @math .''", "Effects are given by terms of any complexity."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_28": {"mid": "1909063750", "abstract": "In this paper we introduce a model for a wide class of computational systems, whose behaviour can be described by certain rewriting rules. We gathered our inspiration both from the world of term rewriting, in particular from the rewriting logic framework Mes92 , and of concurrency theory: among the others, the structured operational semantics Plo81 , the context systems LX90 and the structured transition systems CM92 approaches. Our model recollects many properties of these sources: first, it provides a compositional way to describe both the states and the sequences of transitions performed by a given system, stressing their distributed nature. Second, a suitable notion of typed proof allows to take into account also those formalisms relying on the notions of synchronization and side-effects to determine the actual behaviour of a system. Finally, an equivalence relation over sequences of transitions is defined, equipping the system under analysis with a concurrent semantics, where each equivalence class denotes a family of computationally equivalent'''' behaviours, intended to correspond to the execution of the same set of (causally unrelated) events. As a further abstraction step, our model is conveniently represented using double-categories: its operational semantics is recovered with a free construction, by means of a suitable adjunction.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Connections between tile logic and rewriting logic have been drawn in @cite_47 and @cite_39 , mainly in the language of category theory."], "label": ["General reference to previous research or scholarship: research objective"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_47": {"mid": "1601458080", "abstract": "Rewriting logic extends to concurrent systems with state changes the body of theory developed within the algebraic semantics approach. It is both a foundational tool and the kernel language of several implementation efforts (Cafe, ELAN, Maude). Tile logic extends (unconditional) rewriting logic since it takes into account state changes with side effects and synchronization. It is especially useful for defining compositional models of computation of reactive systems, coordination languages, mobile calculi, and causal and located concurrent systems. In this paper, the two logics are defined and compared using a recently developed algebraic specification methodology, membership equational logic. Given a theory T, the rewriting logic of T is the free monoidal 2-category, and the tile logic of T is the free monoidal double category, both generated by T. An extended version of monoidal 2-categories, called 2VH-categories, is also defined, able to include in an appropriate sense the structure of monoidal double categories. We show that 2VH-categories correspond to an extended version of rewriting logic, which is able to embed tile logic, and which can be implemented in the basic version of rewriting logic using suitable internal strategies. These strategies can be significantly simpler when the theory is uniform. A uniform theory is provided in the paper for CCS, and it is conjectured that uniform theories exist for most process algebras.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_39": {"mid": "2026741712", "abstract": "We propose a modular high-level approach to the specification of transactions in rewriting logic, where the operational and the abstract views are related by suitable adjunctions between categories of tile theories and of rewrite theories.", "ref_function": ["background"], "cite_purpose": ["background"]}}}
{"sentences": ["The goal of coordination is to make different components work together.", "The components may have been coded in different languages, reside in different servers, with different architectures.", "The paper @cite_20 is a comprehensive reference, though old."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "Other functional sentences"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_20": {"mid": "1500859230", "abstract": "A new class of models, formalisms and mechanisms has recently evolved for describing concurrent and distributed computations based on the concept of coordination''''. The purpose of a coordination model and associated language is to provide a means of integrating a number of possibly heterogeneous components together, by interfacing with each component in such a way that the collective set forms a single application that can execute on and take advantage of parallel and distributed systems. In this chapter we initially define and present in sufficient detail the fundamental concepts of what constitutes a coordination model or language. We then go on to classify these models and languages as either data-driven'''' or control-driven'''' (also called process-'''' or task-oriented''''). Next, the main existing coordination models and languages are described in sufficient detail to let the reader appreciate their features and put them into perspective with respect to each other. The chapter ends with a discussion comparing the various models and some conclusions.", "ref_function": ["background", "objective", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Recent progress on multi-label image classification relies on the combination of object localization and deep learning techniques @cite_28 @cite_10 .", "Generally, they introduced object proposals @cite_22 that were assumed to contain all possible foreground objects in the image and aggregated features extracted from all these proposals to incorporate local information.", "Although these methods achieved notable performance improvement, the step of region candidate localization usually incurred redundant computation cost and prevented the model from end-to-end training with deep neural networks.", "@cite_14 further utilized a learning based region proposal network and integrated it with deep neural networks.", "Although this method could be jointly optimized, it required additional annotations of bounding boxes to train the proposal generation component.", "To solve this issue, some other works @cite_29 @cite_20 @cite_29 resorted to attention mechanism to locate the informative regions, and these methods could be trained with image level annotations in an end-to-end manner.", "For example, @cite_20 introduced spatial transformer to adaptively search semantic-aware regions and then aggregated features from these regions to identify multiple labels.", "However, due to the lack of supervision and guidance, these methods could merely locate the regions roughly."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5 , 2.6 , 6.7 , and 3.1 on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at this https URL.", "reference": {"@cite_14": {"mid": "2560096627", "abstract": "Deep convolution neural networks (CNNs) have demonstrated advanced performance on single-label image classification, and various progress also has been made to apply CNN methods on multilabel image classification, which requires annotating objects, attributes, scene categories, etc., in a single shot. Recent state-of-the-art approaches to the multilabel image classification exploit the label dependencies in an image, at the global level, largely improving the labeling capacity. However, predicting small objects and visual concepts is still challenging due to the limited discrimination of the global visual features. In this paper, we propose a regional latent semantic dependencies model (RLSD) to address this problem. The utilized model includes a fully convolutional localization architecture to localize the regions that may contain multiple highly dependent labels. The localized regions are further sent to the recurrent neural networks to characterize the latent semantic dependencies at the regional level. Experimental results on several benchmark datasets show that our proposed model achieves the best performance compared to the state-of-the-art models, especially for predicting small objects occurring in the images. Also, we set up an upper bound model (RLSD+ft-RPN) using bounding-box coordinates during training, and the experimental results also show that our RLSD can approach the upper bound without using the bounding-box annotations, which is more realistic in the real world.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "7746136", "abstract": "The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box\u2019s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96 object recall at overlap threshold of 0.5 and over 75 recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.", "ref_function": ["background", "method", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_28": {"mid": "1567302070", "abstract": "Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground-truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and or redundant hypotheses; 3) the shared CNN is flexible and can be well pre-trained with a large-scale single-label image dataset, e.g., ImageNet; and 4) it may naturally output multi-label prediction results. Experimental results on Pascal VOC 2007 and VOC 2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 90.5 by HCP only and 93.2 after the fusion with our complementary result in [12] based on hand-crafted features on the VOC 2012 dataset.", "ref_function": ["background", "background", "objective", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_29": {"mid": "2592628593", "abstract": "Multi-label image classification is a fundamental but challenging task in computer vision. Great progress has been achieved by exploiting semantic relations between labels in recent years. However, conventional approaches are unable to model the underlying spatial relations between labels in multi-label images, because spatial annotations of the labels are generally not provided. In this paper, we propose a unified deep neural network that exploits both semantic and spatial relations between labels with only image-level supervisions. Given a multi-label image, our proposed Spatial Regularization Network (SRN) generates attention maps for all labels and captures the underlying relations between them via learnable convolutions. By aggregating the regularized classification results with original results by a ResNet-101 network, the classification performance can be consistently improved. The whole deep neural network is trained end-to-end with only image-level annotations, thus requires no additional efforts on image annotations. Extensive evaluations on 3 public datasets with different types of labels show that our approach significantly outperforms state-of-the-arts and has strong generalization capability. Analysis of the learned SRN model demonstrates that it can effectively capture both semantic and spatial relations of labels for improving classification performance.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2410641892", "abstract": "Convolutional neural networks (CNNs) have shown great performance as general feature representations for object recognition applications. However, for multi-label images that contain multiple objects from different categories, scales and locations, global CNN features are not optimal. In this paper, we incorporate local information to enhance the feature discriminative power. In particular, we first extract object proposals from each image. With each image treated as a bag and object proposals extracted from it treated as instances, we transform the multi-label recognition problem into a multi-class multi-instance learning problem. Then, in addition to extracting the typical CNN feature representation from each proposal, we propose to make use of ground-truth bounding box annotations (strong labels) to add another level of local information by using nearest-neighbor relationships of local regions to form a multi-view pipeline. The proposed multi-view multiinstance framework utilizes both weak and strong labels effectively, and more importantly it has the generalization ability to even boost the performance of unseen categories by partial strong labels from other categories. Our framework is extensively compared with state-of-the-art handcrafted feature based methods and CNN based methods on two multi-label benchmark datasets. The experimental results validate the discriminative power and the generalization ability of the proposed framework. With strong labels, our framework is able to achieve state-of-the-art results in both datasets.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2963300078", "abstract": "This paper proposes a novel deep architecture to address multi-label image recognition, a fundamental and practical task towards general visual understanding. Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting in redundant computation and sub-optimal performance. In this work, we achieve the interpretable and contextualized multi-label image classification by developing a recurrent memorized-attention module. This module consists of two alternately performed components: i) a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way and ii) an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions. The LSTM also output the parameters for computing the spatial transformer. On large-scale benchmarks of multi-label image classification (e.g., MS-COCO and PASCAL VOC 07), our approach demonstrates superior performances over other existing state-of-the-arts in both accuracy and efficiency.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["BIP stands for , , ---the three of a composed specification, as proposed by the authors.", "The behaviour of atomic components is specified by automata (of a special kind) some of whose actions are also taken as port names for communication.", "These automata are a specification of requirements on the component, whose real implementation can be made using any language or tool.", "Interaction is performed through connectors linking ports in potentially complex ways.", "Among the interactions that are allowed at any given time, the one with the highest priority is chosen and performed.", "Interaction and priority together implement control.", "The paper @cite_17 has a good overview.", "Several implementations exist that allow to use the BIP framework within programming languages like Java and C++."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Other functional sentences", "Reference to current state of knowledge"], "target_paper": "Rewriting logic is naturally concurrent: several subterms of the state term can be rewritten simultaneously. But state terms are global, which makes compositionality difficult to achieve. Compositionality here means being able to decompose a complex system into its functional components and code each as an isolated and encapsulated system. Our goal is to help bringing compositionality to system specification in rewriting logic. The base of our proposal is the operation that we call synchronous composition. We discuss the motivations and implications of our proposal, formalize it for rewriting logic and also for transition structures, to be used as semantics, and show the power of our approach with some examples. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).", "reference": {"@cite_17": {"mid": "2133038101", "abstract": "We present a methodology for modeling heterogeneous real-time components. Components are obtained as the superposition of three layers : Behavior, specified as a set of transitions; Interactions between transitions of the behavior; Priorities, used to choose amongst possible interactions. A parameterized binary composition operator is used to compose components layer by layer. We present the BIP language for the description and composition of layered components as well as associated tools for executing and analyzing components on a dedicated platform. The language provides a powerful mechanism for structuring interactions involving rendezvous and broadcast. We show that synchronous and timed systems are particular classes of components. Finally, we provide examples and compare the BIP framework to existing ones for heterogeneous component-based modeling.", "ref_function": ["objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Finding the best graph structure for a given problem is a question that recently sparked a lot of interest in the literature @cite_7 @cite_11 @cite_12 .", "In many cases, a good solution consists in using the covariance matrix (or its inverse) as the adjacency matrix of the graph.", "Following this lead, in this paper, we make use of a semi-geometric graph.", "It combines the geometry of images captured through a regular grid graph with the covariance of the pixels measured on different frames.", "As such, the support of the graph remains the same as that of the grid graph, but the weights are replaced by the covariance between corresponding pixels.", "Therefore, this graph takes into account both the data structure and the data distribution."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "Describing used methods", "Describing used methods", "Describing used methods", "Describing used methods"], "target_paper": "Predicting the future of Graph-supported Time Series (GTS) is a key challenge in many domains, such as climate monitoring, finance or neuroimaging. Yet it is a highly difficult problem as it requires to account jointly for time and graph (spatial) dependencies. To simplify this process, it is common to use a two-step procedure in which spatial and time dependencies are dealt with separately. In this paper, we are interested in comparing various linear spatial representations, namely structure-based ones and data-driven ones, in terms of how they help predict the future of GTS. To that end, we perform experiments with various datasets including spontaneous brain activity and raw videos.", "reference": {"@cite_12": {"mid": "2299462150", "abstract": "Stationarity is a cornerstone property that facilitates the analysis and processing of random signals in the time domain. Although time-varying signals are abundant in nature, in many practical scenarios, the information of interest resides in more irregular graph domains. This lack of regularity hampers the generalization of the classical notion of stationarity to graph signals. This paper proposes a definition of weak stationarity for random graph signals that takes into account the structure of the graph where the random process takes place, while inheriting many of the meaningful properties of the classical time domain definition. Provided that the topology of the graph can be described by a normal matrix, stationary graph processes can be modeled as the output of a linear graph filter applied to a white input. This is shown equivalent to requiring the correlation matrix to be diagonalized by the graph Fourier transform; a fact that is leveraged to define a notion of power spectral density (PSD). Properties of the graph PSD are analyzed and a number of methods for its estimation are proposed. This includes generalizations of nonparametric approaches such as periodograms, window-based average periodograms, and filter banks, as well as parametric approaches, using moving-average, autoregressive, and ARMA processes. Graph stationarity and graph PSD estimation are investigated numerically for synthetic and real-world graph signals.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2964012239", "abstract": "The construction of a meaningful graph plays a crucial role in the success of many graph-based representations and algorithms for handling structured data, especially in the emerging field of graph signal processing. However, a meaningful graph is not always readily available from the data, nor easy to define depending on the application domain. In particular, it is often desirable in graph signal processing applications that a graph is chosen such that the data admit certain regularity or smoothness on the graph. In this paper, we address the problem of learning graph Laplacians, which is equivalent to learning graph topologies, such that the input data form graph signals with smooth variations on the resulting topology. To this end, we adopt a factor analysis model for the graph signals and impose a Gaussian probabilistic prior on the latent variables that control these signals. We show that the Gaussian prior leads to an efficient representation that favors the smoothness property of the graph signals. We then propose an algorithm for learning graphs that enforces such property and is based on minimizing the variations of the signals on the learned graph. Experiments on both synthetic and real world data demonstrate that the proposed graph learning framework can efficiently infer meaningful graph topologies from signal observations under the smoothness prior.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Oher compression methods are focusing on minimizing the model size and are very complex (silicon area) to implement in hardware.", "One such method, deep compression @cite_48 , combines pruning, trained clustering-based quantization, and Huffman coding.", "Most of these steps cannot be applied to the intermediate feature maps, which change for every inference as opposed to the weights which are static and can be optimized off-line.", "Furthermore, applying Huffman coding---while being optimal in terms of compression rate and given a specification of input symbols and their statistics---implies storing a very large dictionary: encoding a 16 ,bit word requires a table with @math k entries, but effectively multiple values would have to be encoded jointly in order to exploit their joint distribution (e.g.", "the smoothness), immediately increasing the dictionary size to @math G even for just two values.", "Similar issues arise when using Lempel-Ziv-Welch (LZW) coding @cite_38 @cite_42 as present in e.g.", "the ZIP compression scheme, where the dictionary is encoded in the compressed data stream.", "This makes it unsuitable for a lightweight and energy-efficient VLSI implementation @cite_0 @cite_10 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "General reference to previous research or scholarship: about results", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: about results"], "target_paper": "In the wake of the success of convolutional neural networks in image classification, object recognition, speech recognition, etc., the demand for deploying these compute-intensive ML models on embedded and mobile systems with tight power and energy constraints at low cost, as well as for boosting throughput in data centers, is growing rapidly. This has sparked a surge of research into specialized hardware accelerators. Their performance is typically limited by I O bandwidth, power consumption is dominated by I O transfers to off-chip memory, and on-chip memories occupy a large part of the silicon area. We introduce and evaluate a novel, hardware-friendly and lossless compression scheme for the feature maps present within convolutional neural networks. Its hardware implementation fits into 2.8 kGE and 1.7 kGE of silicon area for the compressor and decompressor, respectively. We show that an average compression ratio of 5.1x for AlexNet, 4x for VGG-16, 2.4x for ResNet-34 and 2.2x for MobileNetV2 can be achieved---a gain of 45--70 over existing methods. Our approach also works effectively for various number formats, has a low frame-to-frame variance on the compression ratio, and achieves compression factors for gradient map compression during training that are even better than for inference.", "reference": {"@cite_38": {"mid": "1990653637", "abstract": "", "ref_function": [], "cite_purpose": [""]}, "@cite_48": {"mid": "2964299589", "abstract": "Abstract: Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.", "ref_function": ["background", "method", "method", "method", "method", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_42": {"mid": "2122962290", "abstract": "Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These encoders can operate in a variable-rate mode as well as a fixed-rate one, and they allow for any finite-state scheme of variable-length-to-variable-length coding. For every individual infinite sequence x a quantity (x) is defined, called the compressibility of x , which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved for x by any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite and practical data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of (x) allows a different machine for each different sequence to be compressed, the constructive coding theorem leads to a universal algorithm that is asymptotically optimal for all sequences.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": [""]}, "@cite_0": {"mid": "2162310490", "abstract": "In this paper, we propose a new two-stage hardware architecture that combines the features of both parallel dictionary LZW (PDLZW) and an approximated adaptive Huffman (AH) algorithms. In this architecture, an ordered list instead of the tree-based structure is used in the AH algorithm for speeding up the compression data rate. The resulting architecture shows that it not only outperforms the AH algorithm at the cost of only one-fourth the hardware resource but it is also competitive to the performance of LZW algorithm (compress). In addition, both compression and decompression rates of the proposed architecture are greater than those of the AH algorithm even in the case realized by software", "ref_function": ["background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2487148512", "abstract": "LZW algorithm is one of the most famous dictionary-based compression and decompression algorithms. The main contribution of this paper is to present a hardware LZW decompression algorithm and to implement it in an FPGA. The experimental results show that one proposed module on Virtex-7 family FPGA XC7VX485T-2 runs up to 2.16 times faster than sequential LZW decompression on a single CPU, where the frequency of FPGA is 301.02MHz. Since the proposed module is compactly designed and uses a few resources of the FPGA, we have succeeded to implement 150 identical modules which works in parallel on the FPGA, where the frequency of FPGA is 245.4MHz. In other words, our implementation runs up to 264 times faster than a sequential implementation on a single CPU.", "ref_function": ["background", "objective", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Few more methods exist by changing the CNN's structure in order to compress the weights @cite_14 @cite_37 or the feature maps @cite_1 @cite_50 @cite_18 .", "However, they require altering the CNN's model and or retraining, and they introduce some accuracy loss.", "Furthermore, they can only be used to compress a few feature maps at specific points within the network and introduce additional compute effort, such as applying a Fourier transform to the feature maps."], "label": ["General reference to previous research or scholarship: research objective", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "In the wake of the success of convolutional neural networks in image classification, object recognition, speech recognition, etc., the demand for deploying these compute-intensive ML models on embedded and mobile systems with tight power and energy constraints at low cost, as well as for boosting throughput in data centers, is growing rapidly. This has sparked a surge of research into specialized hardware accelerators. Their performance is typically limited by I O bandwidth, power consumption is dominated by I O transfers to off-chip memory, and on-chip memories occupy a large part of the silicon area. We introduce and evaluate a novel, hardware-friendly and lossless compression scheme for the feature maps present within convolutional neural networks. Its hardware implementation fits into 2.8 kGE and 1.7 kGE of silicon area for the compressor and decompressor, respectively. We show that an average compression ratio of 5.1x for AlexNet, 4x for VGG-16, 2.4x for ResNet-34 and 2.2x for MobileNetV2 can be achieved---a gain of 45--70 over existing methods. Our approach also works effectively for various number formats, has a low frame-to-frame variance on the compression ratio, and achieves compression factors for gradient map compression during training that are even better than for inference.", "reference": {"@cite_18": {"mid": "2946521496", "abstract": "We present a theoretical and experimental investigation of the quantization problem for artificial neural networks. We provide a mathematical definition of quantized neural networks and analyze their approximation capabilities, showing in particular that any Lipschitz-continuous map defined on a hypercube can be uniformly approximated by a quantized neural network. We then focus on the regularization effect of additive noise on the arguments of multi-step functions inherent to the quantization of continuous variables. In particular, when the expectation operator is applied to a non-differentiable multi-step random function, and if the underlying probability density is differentiable (in either classical or weak sense), then a differentiable function is retrieved, with explicit bounds on its Lipschitz constant. Based on these results, we propose a novel gradient-based training algorithm for quantized neural networks that generalizes the straight-through estimator, acting on noise applied to the network's parameters. We evaluate our algorithm on the CIFAR-10 and ImageNet image classification benchmarks, showing state-of-the-art performance on AlexNet and MobileNetV2 for ternary networks.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_37": {"mid": "2732044853", "abstract": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_14": {"mid": "2172166488", "abstract": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2787697768", "abstract": "In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Unlike previous works, the proposed method is based on converting fixed-point activations into vectors over the smallest GF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers embedded into a DNN. Such an end-to-end learned representation finds more compact feature maps by exploiting quantization redundancies within the fixed-point activations along the channel or spatial dimensions. We apply the proposed network architecture to the tasks of ImageNet classification and PASCAL VOC object detection. Compared to prior approaches, the conducted experiments show a factor of 2 decrease in memory requirements with minor degradation in accuracy while adding only bitwise computations.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_50": {"mid": "2741232386", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["The most directly comparable approach, cDMA @cite_25 , describes a hardware-friendly compression scheme to reduce the data size of intermediate feature maps.", "Their target application differs in that their main goal is to allow faster temporary offloading of the feature maps from GPU to CPU memory through the PCIe bandwidth bottleneck during training, thereby enabling larger batch sizes and deeper and wider networks without sacrificing performance.", "They propose to use , which takes a block of 32 activation values, and generates a 32-bit mask where only the bits to the non-zero values are set.", "The non-zero values are transferred after the mask.", "This provides the main advantage over Zero-RLE that the resulting data volume is independent of how the values of the feature maps are serialized while also providing small compression ratio advantages.", "Note that this is a special case of Zero-RLE with a maximum zero burst length of 1."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method"], "target_paper": "In the wake of the success of convolutional neural networks in image classification, object recognition, speech recognition, etc., the demand for deploying these compute-intensive ML models on embedded and mobile systems with tight power and energy constraints at low cost, as well as for boosting throughput in data centers, is growing rapidly. This has sparked a surge of research into specialized hardware accelerators. Their performance is typically limited by I O bandwidth, power consumption is dominated by I O transfers to off-chip memory, and on-chip memories occupy a large part of the silicon area. We introduce and evaluate a novel, hardware-friendly and lossless compression scheme for the feature maps present within convolutional neural networks. Its hardware implementation fits into 2.8 kGE and 1.7 kGE of silicon area for the compressor and decompressor, respectively. We show that an average compression ratio of 5.1x for AlexNet, 4x for VGG-16, 2.4x for ResNet-34 and 2.2x for MobileNetV2 can be achieved---a gain of 45--70 over existing methods. Our approach also works effectively for various number formats, has a low frame-to-frame variance on the compression ratio, and achieves compression factors for gradient map compression during training that are even better than for inference.", "reference": {"@cite_25": {"mid": "2962821792", "abstract": "Popular deep learning frameworks require users to fine-tune their memory usage so that the training data of a deep neural network (DNN) fits within the GPU physical memory. Prior work tries to address this restriction by virtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be utilized for memory allocations. Despite its merits, virtualizing memory can incur significant performance overheads when the time needed to copy data back and forth from CPU memory is higher than the latency to perform DNN computations. We introduce a high-performance virtualization strategy based on a \"compressing DMA engine\" (cDMA) that drastically reduces the size of the data structures that are targeted for CPU-side allocations. The cDMA engine offers an average 2.6x (maximum 13.8x) compression ratio by exploiting the sparsity inherent in offloaded data, improving the performance of virtualized DNNs by an average 53 (maximum 79 ) when evaluated on an NVIDIA Titan Xp.", "ref_function": ["background", "background", "background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["For this work, we build on a method known in the area of texture compression for GPUs, @cite_44 , fuse it with sparsity-focused compression methods, and evaluate the resulting compression algorithm on intermediate feature maps and gradient maps to show compression ratios of 5.1 (8 ,bit AlexNet), 4 (VGG-16), 2.4 (ResNet-34), 2.8 (SqueezeNet), and 2.2 (MobileNetV2)."], "label": ["Describing used methods"], "target_paper": "In the wake of the success of convolutional neural networks in image classification, object recognition, speech recognition, etc., the demand for deploying these compute-intensive ML models on embedded and mobile systems with tight power and energy constraints at low cost, as well as for boosting throughput in data centers, is growing rapidly. This has sparked a surge of research into specialized hardware accelerators. Their performance is typically limited by I O bandwidth, power consumption is dominated by I O transfers to off-chip memory, and on-chip memories occupy a large part of the silicon area. We introduce and evaluate a novel, hardware-friendly and lossless compression scheme for the feature maps present within convolutional neural networks. Its hardware implementation fits into 2.8 kGE and 1.7 kGE of silicon area for the compressor and decompressor, respectively. We show that an average compression ratio of 5.1x for AlexNet, 4x for VGG-16, 2.4x for ResNet-34 and 2.2x for MobileNetV2 can be achieved---a gain of 45--70 over existing methods. Our approach also works effectively for various number formats, has a low frame-to-frame variance on the compression ratio, and achieves compression factors for gradient map compression during training that are even better than for inference.", "reference": {"@cite_44": {"mid": "2516109628", "abstract": "As key applications become more data-intensive and the computational throughput of processors increases, the amount of data to be transferred in modern memory subsystems grows. Increasing physical bandwidth to keep up with the demand growth is challenging, however, due to strict area and energy limitations. This paper presents a novel and lightweight compression algorithm, Bit-Plane Compression (BPC), to increase the effective memory bandwidth. BPC aims at homogeneously-typed memory blocks, which are prevalent in many-core architectures, and applies a smart data transformation to both improve the inherent data compressibility and to reduce the complexity of compression hardware. We demonstrate that BPC provides superior compression ratios of 4.1:1 for integer benchmarks and reduces memory bandwidth requirements significantly.", "ref_function": ["background", "background", "objective", "objective", "result"], "cite_purpose": ["extends"]}}}
{"sentences": ["Semantic parsing models can be trained to produce gold logical forms using an encoder-decoder approach @cite_26 or by filling templates @cite_22 @cite_18 @cite_30 .", "When gold logical forms are not available, they are typically treated as latent variables or hidden states and the answers or denotations are used to search for correct logical forms @cite_31 @cite_29 @cite_34 .", "In some cases, feedback from query execution is used as a reward signal for updating the model through reinforcement learning @cite_5 @cite_6 @cite_32 @cite_16 or for refining parts of the query @cite_7 .", "In our work, we do not use logical forms or RL, which can be hard to train, but simplify the training process by directly matching questions to table cells."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Describing used methods"], "target_paper": "We present a novel approach to answering sequential questions based on structured objects such as knowledge bases or tables without using a logical form as an intermediate representation. We encode tables as graphs using a graph neural network model based on the Transformer architecture. The answers are then selected from the encoded graph using a pointer network. This model is appropriate for processing conversations around structured data, where the attention mechanism that selects the answers to a question can also be used to resolve conversational references. We demonstrate the validity of this approach with competitive results on the Sequential Question Answering (SQA) task (, 2017).", "reference": {"@cite_30": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_18": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2963868320", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2768409085", "abstract": "Synthesizing SQL queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the problem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the SQL queries to be serialized. Since the same SQL query may have multiple equivalent serializations, training a sequence-to-sequence-style model is sensitive to the choice from one of them. This phenomenon is documented as the \"order-matters\" problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited. In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter. In particular, we employ a sketch-based approach where the sketch contains a dependency graph, so that one prediction can be done by taking into consideration only the previous predictions that it depends on. In addition, we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch. By combining all these novel techniques, we show that SQLNet can outperform the prior art by 9 to 13 on the WikiSQL task.", "ref_function": ["background", "background", "background", "background", "result", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2891000242", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_29": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2891991579", "abstract": "This paper presents MAPO: a novel policy optimization formulation that incorporates a memory buffer of promising trajectories to reduce the variance of policy gradient estimates for deterministic environments with discrete actions. The formulation expresses the expected return objective as a weighted sum of two terms: an expectation over a memory of trajectories with high rewards, and a separate expectation over the trajectories outside the memory. We propose 3 techniques to make an efficient training algorithm for MAPO: (1) distributed sampling from inside and outside memory with an actor-learner architecture; (2) a marginal likelihood constraint over the memory to initiate training; (3) systematic exploration to discover new high reward trajectories. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with a sparse reward. We evaluate MAPO on weakly supervised program synthesis from natural language semantic parsing. On the WikiTableQuestions benchmark we improve the state-of-the-art by 2.5 , achieving an accuracy of 46.2 , and on the WikiSQL benchmark, MAPO achieves an accuracy of 74.9 with only weak supervision, outperforming the state-of-the-art with full supervision.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2917052767", "abstract": "We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms our alternative reward learning technique based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2 and 2.4 on WikiTableQuestions and WikiSQL datasets respectively.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2751448157", "abstract": "Relational databases store a significant amount of the worlds data. However, accessing this data currently requires users to understand a query language such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss. Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem. In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets. By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9 to 59.4 and logical form accuracy from 23.4 to 48.3 .", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2251079237", "abstract": "We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simplifies the semantic matching problem. By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outperforms previous methods substantially, and achieves an F1 measure of 52.5 on the WEBQUESTIONS dataset.", "ref_function": ["objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["The topic of learning a model when the input data points are provided by strategic sources has been the focus of a growing literature at the intersection of machine learning and game theory.", "A significant amount of work has been devoted to the setting in which Agents are interested in the outcome of the estimation process itself, e.g., when they are trying to sway the learned model closer to their own data points @cite_21 @cite_14 @cite_5 @cite_9 @cite_15 .", "Our setting is concerned with the fundamental question of eliciting accurate data when data acquisition is costly for the agents, or when they are not willing to share their data without some form of monetary compensation.", "Another line of work considers settings in which the Agents have to be compensated for their loss of privacy @cite_2 @cite_19 ."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: research objective", "Describing the objective", "General reference to previous research or scholarship: research objective"], "target_paper": "We consider a crowdsourcing data acquisition scenario, such as federated learning, where a Center collects data points from a set of rational Agents, with the aim of training a model. For linear regression models, we show how a payment structure can be designed to incentivize the agents to provide high-quality data as early as possible, based on a characterization of the influence that data points have on the loss function of the model. Our contributions can be summarized as follows: (a) we prove theoretically that this scheme ensures truthful data reporting as a game-theoretic equilibrium and further demonstrate its robustness against mixtures of truthful and heuristic data reports, (b) we design a procedure according to which the influence computation can be efficiently approximated and processed sequentially in batches over time, (c) we develop a theory that allows correcting the difference between the influence and the overall change in loss and (d) we evaluate our approach on real datasets, confirming our theoretical findings.", "reference": {"@cite_14": {"mid": "1983916796", "abstract": "We initiate the study of incentives in a general machine learning framework. We focus on a game-theoretic regression learning setting where private information is elicited from multiple agents with different, possibly conflicting, views on how to label the points of an input space. This conflict potentially gives rise to untruthfulness on the part of the agents. In the restricted but important case when every agent cares about a single point, and under mild assumptions, we show that agents are motivated to tell the truth. In a more general setting, we study the power and limitations of mechanisms without payments. We finally establish that, in the general setting, the VCG mechanism goes a long way in guaranteeing truthfulness and economic efficiency.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2435502378", "abstract": "We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples, taking a game-theoretic viewpoint. In our setting, samples are supplied by strategic agents, who wish to pull the estimate as close as possible to their own value. In this setting, the sample mean gives rise to manipulation opportunities, whereas the sample median does not. Our key question is whether the sample median is the best (in terms of mean squared error) truthful estimator of the population mean. We show that when the underlying distribution is symmetric, there are truthful estimators that dominate the median. Our main result is a characterization of worst-case optimal truthful estimators, which provably outperform the median, for possibly asymmetric distributions with bounded support.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2110135769", "abstract": "Abstract This paper introduces a whole class of estimators (clockwise repeated median estimators or CRM) for the simple regression model that are immune to strategic manipulation by the agents generating the data. We find that some well-known robust estimators proposed in the literature like the resistant line method are included in our family. Finally, we also undertake a Monte Carlo study to compare the distribution of some estimators that are robust to data manipulation with the OLS estimators under different scenarios.", "ref_function": ["background", "method", "method"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2767657742", "abstract": "We consider a data analyst's problem of purchasing data from strategic agents to compute an unbiased estimate of a statistic of interest. Agents incur private costs to reveal their data and the costs can be arbitrarily correlated with their data. Once revealed, data are verifiable. This paper focuses on linear unbiased estimators. We design an individually rational and incentive compatible mechanism that optimizes the worst-case mean-squared error of the estimation, where the worst-case is over the unknown correlation between costs and data, subject to a budget constraint in expectation. We characterize the form of the optimal mechanism in closed-form. We further extend our results to acquiring data for estimating a parameter in regression analysis, where private costs can correlate with the values of the dependent variable but not with the values of the independent variables.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2962877736", "abstract": "We consider the problem of fitting a linear model to data held by individuals who are concerned about their privacy. Incentivizing most players to truthfully report their data to the analyst constrains our design to mechanisms that provide a privacy guarantee to the participants; we use differential privacy to model individuals\u2019 privacy losses. This immediately poses a problem, as differentially private computation of a linear model necessarily produces a biased estimation, and existing approaches to design mechanisms to elicit data from privacy-sensitive individuals do not generalize well to biased estimators. We overcome this challenge through an appropriate design of the computation and payment scheme.", "ref_function": ["background", "background", "method", "method"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2054844093", "abstract": "The strategyproof classification problem deals with a setting where a decision maker must classify a set of input points with binary labels, while minimizing the expected error. The labels of the input points are reported by self-interested agents, who might lie in order to obtain a classifier that more closely matches their own labels, thereby creating a bias in the data; this motivates the design of truthful mechanisms that discourage false reports. In this paper we give strategyproof mechanisms for the classification problem in two restricted settings: (i) there are only two classifiers, and (ii) all agents are interested in a shared set of input points. We show that these plausible assumptions lead to strong positive results. In particular, we demonstrate that variations of a random dictator mechanism, that are truthful, can guarantee approximately optimal outcomes with respect to any family of classifiers. Moreover, these results are tight in the sense that they match the best possible approximation ratio that can be guaranteed by any truthful mechanism. We further show how our mechanisms can be used for learning classifiers from sampled data, and provide PAC-style generalization bounds on their expected error. Interestingly, our results can be applied to problems in the context of various fields beyond classification, including facility location and judgment aggregation.", "ref_function": ["background", "background", "method", "method", "method", "result", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2963668529", "abstract": "This paper is part of an emerging line of work at the intersection of machine learning and mechanism design, which aims to avoid noise in training data by correctly aligning the incentives of data sources. Specifically, we focus on the ubiquitous problem of linear regression, where strategyproof mechanisms have previously been identified in two dimensions. In our setting, agents have single-peaked preferences and can manipulate only their response variables. Our main contribution is the discovery of a family of group strategyproof linear regression mechanisms in any number of dimensions, which we call generalized resistant hyperplane mechanisms. The game-theoretic properties of these mechanisms --- and, in fact, their very existence --- are established through a connection to a discrete version of the Ham Sandwich Theorem.", "ref_function": ["background", "background", "background", "objective", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Our ideas are closely related to the literature of mechanisms @cite_8 and @cite_17 .", "The idea behind this literature is to extract high-quality information from individuals by comparing their reports against those of randomly chosen peers.", "This approach has been largely successful in eliciting information.", "The same principle applies to our case, where the payments are dependent on the improvement of the model and therefore agents are rewarded for providing information.", "Finally, jia2019towards [ jia2019towards ] recently considered a setting in which the value of the provided data information is determined via the .", "Their approach is inherently different from ours, but it is worth noting that they consider the influence approximation of @cite_1 for approximating the Shapley value."], "label": ["Explaining the objective relationship between own work and references", "Reference to single investigations in the past:  about objective", "General descriptions of the topic", "Explaining the method relationship between own work and references", "Reference to single investigations in the past:  about objective", "Explaining the method relationship between own work and references"], "target_paper": "We consider a crowdsourcing data acquisition scenario, such as federated learning, where a Center collects data points from a set of rational Agents, with the aim of training a model. For linear regression models, we show how a payment structure can be designed to incentivize the agents to provide high-quality data as early as possible, based on a characterization of the influence that data points have on the loss function of the model. Our contributions can be summarized as follows: (a) we prove theoretically that this scheme ensures truthful data reporting as a game-theoretic equilibrium and further demonstrate its robustness against mixtures of truthful and heuristic data reports, (b) we design a procedure according to which the influence computation can be efficiently approximated and processed sequentially in batches over time, (c) we develop a theory that allows correcting the difference between the influence and the overall change in loss and (d) we evaluate our approach on real datasets, confirming our theoretical findings.", "reference": {"@cite_1": {"mid": "2597603852", "abstract": "How can we explain the predictions of a black-box model? In this paper, we use influence functions \u2014 a classic technique from robust statistics \u2014 to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["differences"]}, "@cite_17": {"mid": "2267283990", "abstract": "Crowdsourcing is widely proposed as a method to solve a large variety of judgment tasks, such as classifying website content, peer grading in online courses, or collecting real-world data. As the data reported by workers cannot be verified, there is a tendency to report random data without actually solving the task. This can be countered by making the reward for an answer depend on its consistency with answers given by other workers, an approach called peer consistency. However, it is obvious that the best strategy in such schemes is for all workers to report the same answer without solving the task. Dasgupta and Ghosh [2013] show that, in some cases, exerting high effort can be encouraged in the highest-paying equilibrium. In this article, we present a general mechanism that implements this idea and is applicable to most crowdsourcing settings. Furthermore, we experimentally test the novel mechanism, and validate its theoretical properties.", "ref_function": ["background", "background", "method", "method", "result", "method", "result"], "cite_purpose": ["similarities"]}, "@cite_8": {"mid": "2757853533", "abstract": "Abstract Intelligent systems often depend on data provided by information agents, for example, sensor data or crowdsourced human computation. Providing accurate and relevant data requires costly effort that agents may not always be willing to provide. Thus, it becomes important not only to verify the correctness of data, but also to provide incentives so that agents that provide high-quality data are rewarded while those that do not are discouraged by low rewards. We cover different settings and the assumptions they admit, including sensing, human computation, peer grading, reviews, and predictions. We survey different incentive mechanisms, including proper scoring rules, prediction markets and peer prediction, Bayesian Truth Serum, Peer Truth Serum, Correlated Agreement, and the settings where each of them would be suitable. As an alternative, we also consider reputation mechanisms. We complement the game-theoretic analysis with practical examples of applications in prediction platforms, community sensin...", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["similarities"]}}}
{"sentences": ["Frequency Oracle One basic mechanism in LDP is to estimate frequencies of values.", "There have been several mechanisms @cite_34 @cite_30 @cite_2 @cite_31 @cite_5 @cite_0 proposed for this task.", "Among them, @cite_31 introduces , which achieves low estimation errors and low communication costs.", "The application of is crucial for the utility of other application such as heavy hitter identification @cite_2 and frequent itemset mining @cite_4 @cite_11 .", "And one major contribution of this paper is to enable to enjoy the privacy amplification effect."], "label": ["Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about result", "Explain the significance of references", "Describing the results"], "target_paper": "When collecting information, local differential privacy (LDP) alleviates privacy concerns of users, as users' private information is randomized before being sent to the central aggregator. However, LDP results in loss of utility due to the amount of noise that is added. To address this issue, recent work introduced an intermediate server and with the assumption that this intermediate server did not collude with the aggregator. Using this trust model, one can add less noise to achieve the same privacy guarantee; thus improving the utility. In this paper, we investigate this multiple-party setting of LDP. We first analyze the threat model and identify potential adversaries. We then make observations about existing approaches and propose new techniques that achieve a better privacy-utility tradeoff than existing ones. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.", "reference": {"@cite_30": {"mid": "1986293063", "abstract": "We give efficient protocols and matching accuracy lower bounds for frequency estimation in the local model for differential privacy. In this model, individual users randomize their data themselves, sending differentially private reports to an untrusted server that aggregates them. We study protocols that produce a succinct histogram representation of the data. A succinct histogram is a list of the most frequent items in the data (often called \"heavy hitters\") along with estimates of their frequencies; the frequency of all other items is implicitly estimated as 0. If there are n users whose items come from a universe of size d, our protocols run in time polynomial in n and log(d). With high probability, they estimate the accuracy of every item up to error O(\u221a log(d) (e2n) ). Moreover, we show that this much error is necessary, regardless of computational efficiency, and even for the simple setting where only one item appears with significant frequency in the data set. Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) for this task either ran in time \u03a9(d) or had much worse error (about \u221a[6] log(d) (e2n) ), and the only known lower bound on error was \u03a9(1 \u221a n ). We also adapt a result of (2010) to the local setting. In a model with public coins, we show that each user need only send 1 bit to the server. For all known local protocols (including ours), the transformation preserves computational efficiency.", "ref_function": ["background", "background", "method", "method", "result", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2532967691", "abstract": "In local differential privacy (LDP), each user perturbs her data locally before sending the noisy data to a data collector. The latter then analyzes the data to obtain useful statistics. Unlike the setting of centralized differential privacy, in LDP the data collector never gains access to the exact values of sensitive data, which protects not only the privacy of data contributors but also the collector itself against the risk of potential data leakage. Existing LDP solutions in the literature are mostly limited to the case that each user possesses a tuple of numeric or categorical values, and the data collector computes basic statistics such as counts or mean values. To the best of our knowledge, no existing work tackles more complex data mining tasks such as heavy hitter discovery over set-valued data. In this paper, we present a systematic study of heavy hitter mining under LDP. We first review existing solutions, extend them to the heavy hitter estimation, and explain why their effectiveness is limited. We then propose LDPMiner, a two-phase mechanism for obtaining accurate heavy hitters with LDP. The main idea is to first gather a candidate set of heavy hitters using a portion of the privacy budget, and focus the remaining budget on refining the candidate set in a second phase, which is much more efficient budget-wise than obtaining the heavy hitters directly from the whole dataset. We provide both in-depth theoretical analysis and extensive experiments to compare LDPMiner against adaptations of previous solutions. The results show that LDPMiner significantly improves over existing methods. More importantly, LDPMiner successfully identifies the majority true heavy hitters in practical settings.", "ref_function": ["background", "background", "background", "background", "background", "result", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2964010583", "abstract": "We study the problem of estimating @math -ary distributions under @math -local differential privacy. @math samples are distributed across users who send privatized versions of their sample to a central server. All previously known sample optimal algorithms require linear (in @math ) communication from each user in the high privacy regime @math , and run in time that grows as @math , which can be prohibitive for large domain size @math . @PARASPLIT We propose Hadamard Response (HR), a local privatization scheme that requires no shared randomness and is symmetric with respect to the users. Our scheme has order optimal sample complexity for all @math , a communication of at most @math bits per user, and nearly linear running time of @math . @PARASPLIT Our encoding and decoding are based on Hadamard matrices and are simple to implement. The statistical performance relies on the coding theoretic aspects of Hadamard matrices, ie, the large Hamming distance between the rows. An efficient implementation of the algorithm using the Fast Walsh-Hadamard transform gives the computational gains. @PARASPLIT We compare our approach with Randomized Response (RR), RAPPOR, and subset-selection mechanisms (SS), both theoretically, and experimentally. For @math , our algorithm runs about 100x faster than SS, and RAPPOR.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2734833749", "abstract": "We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time -- TreeHist and Bitstogram. In both algorithms, server running time is @math and user running time is @math , hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring @math server time and @math user time. With a typically large number of participants in local algorithms ( @math in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.", "ref_function": ["background", "background", "method", "method"], "cite_purpose": ["background", "background"]}, "@cite_5": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2742225091", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}, "@cite_34": {"mid": "1981029888", "abstract": "Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["There also exist relaxed models that seem incompatible with the shuffler model, i.e., @cite_15 considers the inferring probability as the adversary's power; and @cite_3 utilizes the linkage between each user's sensitive and public attributes."], "label": ["General reference to previous research or scholarship: approaches taken"], "target_paper": "When collecting information, local differential privacy (LDP) alleviates privacy concerns of users, as users' private information is randomized before being sent to the central aggregator. However, LDP results in loss of utility due to the amount of noise that is added. To address this issue, recent work introduced an intermediate server and with the assumption that this intermediate server did not collude with the aggregator. Using this trust model, one can add less noise to achieve the same privacy guarantee; thus improving the utility. In this paper, we investigate this multiple-party setting of LDP. We first analyze the threat model and identify potential adversaries. We then make observations about existing approaches and propose new techniques that achieve a better privacy-utility tradeoff than existing ones. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.", "reference": {"@cite_15": {"mid": "2902114605", "abstract": "In large-scale statistical learning, data collection and model fitting are moving increasingly toward peripheral devices---phones, watches, fitness trackers---away from centralized data collection. Concomitant with this rise in decentralized data are increasing challenges of maintaining privacy while allowing enough information to fit accurate, useful statistical models. This motivates local notions of privacy---most significantly, local differential privacy, which provides strong protections against sensitive data disclosures---where data is obfuscated before a statistician or learner can even observe it, providing strong protections to individuals' data. Yet local privacy as traditionally employed may prove too stringent for practical use, especially in modern high-dimensional statistical and machine learning problems. Consequently, we revisit the types of disclosures and adversaries against which we provide protections, considering adversaries with limited prior information and ensuring that with high probability, ensuring they cannot reconstruct an individual's data within useful tolerances. By reconceptualizing these protections, we allow more useful data release---large privacy parameters in local differential privacy---and we design new (minimax) optimal locally differentially private mechanisms for statistical learning problems for privacy levels. We thus present practicable approaches to large-scale locally private model training that were previously impossible, showing theoretically and empirically that we can fit large-scale image classification and language models with little degradation in utility.", "ref_function": ["background", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2948055046", "abstract": "Multi-dimensional analytical (MDA) queries are often issued against a fact table with predicates on (categorical or ordinal) dimensions and aggregations on one or more measures. In this paper, we study the problem of answering MDA queries under local differential privacy (LDP). In the absence of a trusted agent, sensitive dimensions are encoded in a privacy-preserving (LDP) way locally before being sent to the data collector. The data collector estimates the answers to MDA queries, based on the encoded dimensions. We propose several LDP encoders and estimation algorithms, to handle a large class of MDA queries with different types of predicates and aggregation functions. Our techniques are able to answer these queries with tight error bounds and scale well in high-dimensional settings (i.e., error is polylogarithmic in dimension sizes). We conduct experiments on real and synthetic data to verify our theoretical results, and compare our solution with marginal-estimation based solutions.", "ref_function": ["background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Distributed DP In the distributed setting of DP, each data owner (or proxy) has access to a (disjoint) subset of users.", "For example, each patient's information is possessed by a hospital.", "The DP noise is added at the level of the intermediate data owners (e.g., @cite_27 ).", "A special case (two-party computation) is also considered @cite_18 @cite_10 .", "@cite_7 studies the limitation of two-party DP.", "@cite_16 , a distributed noise generation protocol was proposed to prevent some party from adding malicious noise.", "@cite_37 lays the theoretical foundation of the relationship among several kinds of computational DP definitions."], "label": ["Reference to current state of knowledge", "Other functional sentences", "Reference to current state of knowledge", "General reference to previous research or scholarship: research objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method"], "target_paper": "When collecting information, local differential privacy (LDP) alleviates privacy concerns of users, as users' private information is randomized before being sent to the central aggregator. However, LDP results in loss of utility due to the amount of noise that is added. To address this issue, recent work introduced an intermediate server and with the assumption that this intermediate server did not collude with the aggregator. Using this trust model, one can add less noise to achieve the same privacy guarantee; thus improving the utility. In this paper, we investigate this multiple-party setting of LDP. We first analyze the threat model and identify potential adversaries. We then make observations about existing approaches and propose new techniques that achieve a better privacy-utility tradeoff than existing ones. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.", "reference": {"@cite_18": {"mid": "2751501713", "abstract": "Private record linkage (PRL) is the problem of identifying pairs of records that are similar as per an input matching rule from databases held by two parties that do not trust one another. We identify three key desiderata that a PRL solution must ensure: (1) perfect precision and high recall of matching pairs, (2) a proof of end-to-end privacy, and (3) communication and computational costs that scale subquadratically in the number of input records. We show that all of the existing solutions for PRL? including secure 2-party computation (S2PC), and their variants that use non-private or differentially private (DP) blocking to ensure subquadratic cost -- violate at least one of the three desiderata. In particular, S2PC techniques guarantee end-to-end privacy but have either low recall or quadratic cost. In contrast, no end-to-end privacy guarantee has been formalized for solutions that achieve subquadratic cost. This is true even for solutions that compose DP and S2PC: DP does not permit the release of any exact information about the databases, while S2PC algorithms for PRL allow the release of matching records. In light of this deficiency, we propose a novel privacy model, called output constrained differential privacy, that shares the strong privacy protection of DP, but allows for the truthful release of the output of a certain function applied to the data. We apply this to PRL, and show that protocols satisfying this privacy model permit the disclosure of the true matching records, but their execution is insensitive to the presence or absence of a single non-matching record. We find that prior work that combine DP and S2PC techniques even fail to satisfy this end-to-end privacy model. Hence, we develop novel protocols that provably achieve this end-to-end privacy guarantee, together with the other two desiderata of PRL. Our empirical evaluation also shows that our protocols obtain high recall, scale near linearly in the size of the input databases and the output set of matching pairs, and have communication and computational costs that are at least 2 orders of magnitude smaller than S2PC baselines.", "ref_function": ["background", "background", "method", "method", "method", "result", "background", "objective", "method", "method", "method", "result"], "cite_purpose": [""]}, "@cite_37": {"mid": "1493407996", "abstract": "The definition of differential privacy has recently emerged as a leading standard of privacy guarantees for algorithms on statistical databases. We offer several relaxations of the definition which require privacy guarantees to hold only against efficient--i.e., computationally-bounded--adversaries. We establish various relationships among these notions, and in doing so, we observe their close connection with the theory of pseudodense sets by [1]. We extend the dense model theorem of to demonstrate equivalence between two definitions (indistinguishability- and simulatability-based) of computational differential privacy. Our computational analogues of differential privacy seem to allow for more accurate constructions than the standard information-theoretic analogues. In particular, in the context of private approximation of the distance between two vectors, we present a differentially-private protocol for computing the approximation, and contrast it with a substantially more accurate protocol that is only computationally differentially private.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2029905190", "abstract": "We study differential privacy in a distributed setting where two parties would like to perform analysis of their joint data while preserving privacy for both datasets. Our results imply almost tight lower bounds on the accuracy of such data analyses, both for specific natural functions (such as Hamming distance) and in general. Our bounds expose a sharp contrast between the two-party setting and the simpler client-server setting (where privacy guarantees are one-sided). In addition, those bounds demonstrate a dramatic gap between the accuracy that can be obtained by differentially private data analysis versus the accuracy obtainable when privacy is relaxed to a computational variant of differential privacy. The first proof technique we develop demonstrates a connection between differential privacy and deterministic extraction from Santha-Vazirani sources. A second connection we expose indicates that the ability to approximate a function by a low-error differentially private protocol is strongly related to the ability to approximate it by a low communication protocol. (The connection goes in both directions.)", "ref_function": ["background", "background", "method", "result", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["uses"]}, "@cite_16": {"mid": "2610910029", "abstract": "In this work we provide efficient distributed protocols for generating shares of random noise, secure against malicious participants. The purpose of the noise generation is to create a distributed implementation of the privacy-preserving statistical databases described in recent papers [14,4,13]. In these databases, privacy is obtained by perturbing the true answer to a database query by the addition of a small amount of Gaussian or exponentially distributed random noise. The computational power of even a simple form of these databases, when the query is just of the form \u03a3 i f(d i ), that is, the sum over all rows i in the database of a function f applied to the data in row i, has been demonstrated in [4]. A distributed implementation eliminates the need for a trusted database administrator. The results for noise generation are of independent interest. The generation of Gaussian noise introduces a technique for distributing shares of many unbiased coins with fewer executions of verifiable secret sharing than would be needed using previous approaches (reduced by a factor of n). The generation of exponentially distributed noise uses two shallow circuits: one for generating many arbitrarily but identically biased coins at an amortized cost of two unbiased random bits apiece, independent of the bias, and the other to combine bits of appropriate biases to obtain an exponential distribution.", "ref_function": ["background", "objective", "background", "method", "method", "result", "method", "method"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2943322967", "abstract": "Private record linkage protocols allow multiple parties to exchange matching records, which refer to the same entities or have similar values, while keeping the non-matching ones secret. Conventional protocols are based on computationally expensive cryptographic primitives and therefore do not scale. To address these scalability issues, hybrid protocols have been proposed that combine differential privacy techniques with secure multiparty computation techniques. However, a drawback of such protocols is that they disclose to the parties both the matching records and the differentially private synopses of the datasets involved in the linkage. Consequently, differential privacy is no longer always satisfied. To address this issue, we propose a novel framework that separates the private synopses from the matching records. The two parties do not access the synopses directly, but still use them to efficiently link records. We theoretically prove the security of our framework under the state-of-the-art privacy notion of differential privacy for record linkage (DPRL). In addition, we develop a simple but effective strategy for releasing private synopses. Extensive experimental results show that our framework is superior to the existing methods in terms of efficiency.", "ref_function": ["background", "background", "method", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": [""]}}}
{"sentences": ["DP by Trusted Hardware In this approach, a trusted hardware (e.g., SGX) is utilized to collect data, tally the data, and add the noise within the protected hardware.", "The result is then sent to the analyst.", "Google propose Prochlo @cite_29 that uses SGX.", "Note that the trusted hardware can be run by the server.", "Thus @cite_38 and @cite_1 designed oblivious DP algorithms to overcome the threat of side information (memory access pattern may be related to the underlying data).", "These proposals assume the trusted hardware is safe to use.", "However, using trusted hardware has potential risks (e.g., @cite_23 ).", "This paper considers the setting without trusted hardware."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: about results", "Explaining the inadequacies of previous studies", "Describing the objective"], "target_paper": "When collecting information, local differential privacy (LDP) alleviates privacy concerns of users, as users' private information is randomized before being sent to the central aggregator. However, LDP results in loss of utility due to the amount of noise that is added. To address this issue, recent work introduced an intermediate server and with the assumption that this intermediate server did not collude with the aggregator. Using this trust model, one can add less noise to achieve the same privacy guarantee; thus improving the utility. In this paper, we investigate this multiple-party setting of LDP. We first analyze the threat model and identify potential adversaries. We then make observations about existing approaches and propose new techniques that achieve a better privacy-utility tradeoff than existing ones. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.", "reference": {"@cite_38": {"mid": "2795267922", "abstract": "It is well-known that a program's memory access pattern can leak information about its input. To thwart such leakage, most existing works adopt the technique of oblivious RAM (ORAM) simulation. Such an obliviousness notion has stimulated much debate. Although ORAM techniques have significantly improved over the past few years, the concrete overheads are arguably still undesirable for real-world systems --- part of this overhead is in fact inherent due to a well-known logarithmic ORAM lower bound by Goldreich and Ostrovsky. To make matters worse, when the program's runtime or output length depend on secret inputs, it may be necessary to perform worst-case padding to achieve full obliviousness and thus incur possibly super-linear overheads. Inspired by the elegant notion of differential privacy, we initiate the study of a new notion of access pattern privacy, which we call \"(\u03f5, \u03b4)-differential obliviousness\". We separate the notion of (\u03f5, \u03b4)-differential obliviousness from classical obliviousness by considering several fundamental algorithmic abstractions including sorting small-length keys, merging two sorted lists, and range query data structures (akin to binary search trees). We show that by adopting differential obliviousness with reasonable choices of \u03f5 and \u03b4, not only can one circumvent several impossibilities pertaining to full obliviousness, one can also, in several cases, obtain meaningful privacy with little overhead relative to the non-private baselines (i.e., having privacy \"almost for free\"). On the other hand, we show that for very demanding choices of \u03f5 and \u03b4, the same lower bounds for oblivious algorithms would be preserved for (\u03f5, \u03b4)-differential obliviousness.", "ref_function": ["background", "background", "background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_29": {"mid": "2761138375", "abstract": "The large-scale monitoring of computer users' software activities has become commonplace, e.g., for application telemetry, error reporting, or demographic profiling. This paper describes a principled systems architecture---Encode, Shuffle, Analyze (ESA)---for performing such monitoring with high utility while also protecting user privacy. The ESA design, and its Prochlo implementation, are informed by our practical experiences with an existing, large deployment of privacy-preserving software monitoring. With ESA, the privacy of monitored users' data is guaranteed by its processing in a three-step pipeline. First, the data is encoded to control scope, granularity, and randomness. Second, the encoded data is collected in batches subject to a randomized threshold, and blindly shuffled, to break linkability and to ensure that individual data items get \"lost in the crowd\" of the batch. Third, the anonymous, shuffled data is analyzed by a specific analysis engine that further prevents statistical inference attacks on analysis results. ESA extends existing best-practice methods for sensitive-data analytics, by using cryptography and statistical techniques to make explicit how data is elided and reduced in precision, how only common-enough, anonymous data is analyzed, and how this is done for only specific, permitted purposes. As a result, ESA remains compatible with the established workflows of traditional database analysis. Strong privacy guarantees, including differential privacy, can be established at each processing step to defend against malice or compromise at one or more of those steps. Prochlo develops new techniques to harden those steps, including the Stash Shuffle, a novel scalable and efficient oblivious-shuffling algorithm based on Intel's SGX, and new applications of cryptographic secret sharing and blinding. We describe ESA and Prochlo, as well as experiments that validate their ability to balance utility and privacy.", "ref_function": ["background", "objective", "method", "method", "method", "method", "background", "method", "result", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2809701319", "abstract": "Differential privacy has emerged as the main definition for private data analysis and machine learning. The global model of differential privacy, which assumes that users trust the data collector, provides strong privacy guarantees and introduces small errors in the output. In contrast, applications of differential privacy in commercial systems by Apple, Google, and Microsoft, use the local model. Here, users do not trust the data collector, and hence randomize their data before sending it to the data collector. Unfortunately, local model is too strong for several important applications and hence is limited in its applicability. In this work, we propose a framework based on trusted processors and a new definition of differential privacy called Oblivious Differential Privacy, which combines the best of both local and global models. The algorithms we design in this framework show interesting interplay of ideas from the streaming algorithms, oblivious algorithms, and differential privacy.", "ref_function": ["background", "background", "background", "background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2889224812", "abstract": "Intel Software Guard Extensions (SGX) isolate security-critical code inside a protected memory area called enclave. Previous research on SGX has demonstrated that memory corruption vulnerabilities within enclave code can be exploited to extract secret keys and bypass remote attestation. However, these attacks require kernel privileges, and rely on frequently probing enclave code which results in many enclave crashes. Further, they assume a constant, not randomized memory layout. In this paper, we present novel exploitation techniques against SGX that do not require any enclave crashes and work in the presence of existing SGX randomization approaches such as SGX-Shield. A key contribution of our attacks is that they work under weak adversarial assumptions, e.g., not requiring kernel privileges. In fact, they can be applied to any enclave that is developed with the standard Intel SGX SDK on either Linux or Windows.", "ref_function": ["background", "background", "background", "background", "method", "method", "method"], "cite_purpose": ["motivation"]}}}
{"sentences": ["Aggregation using Secure Computation Our work shares similar threat models from work on secure aggregation such as the electronic voting @cite_12 and statistic aggregation @cite_25 .", "In particular, multiple users compute some information collaboratively without leaking information about their own data.", "Note that the primary goal of secure aggregation is different: the result must be deterministic and correct, while in , a significant amount of noise is necessary."], "label": ["Explaining the method relationship between own work and references", "General reference to previous research or scholarship: approaches taken", "Explaining the objective relationship between own work and references"], "target_paper": "When collecting information, local differential privacy (LDP) alleviates privacy concerns of users, as users' private information is randomized before being sent to the central aggregator. However, LDP results in loss of utility due to the amount of noise that is added. To address this issue, recent work introduced an intermediate server and with the assumption that this intermediate server did not collude with the aggregator. Using this trust model, one can add less noise to achieve the same privacy guarantee; thus improving the utility. In this paper, we investigate this multiple-party setting of LDP. We first analyze the threat model and identify potential adversaries. We then make observations about existing approaches and propose new techniques that achieve a better privacy-utility tradeoff than existing ones. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.", "reference": {"@cite_25": {"mid": "2599930814", "abstract": "This paper presents Prio, a privacy-preserving system for the collection of aggregate statistics. Each Prio client holds a private data value (e.g., its current location), and a small set of servers compute statistical functions over the values of all clients (e.g., the most popular location). As long as at least one server is honest, the Prio servers learn nearly nothing about the clients' private data, except what they can infer from the aggregate statistics that the system computes. To protect functionality in the face of faulty or malicious clients, Prio uses secret-shared non-interactive proofs (SNIPs), a new cryptographic technique that yields a hundred-fold performance improvement over conventional zero-knowledge approaches. Prio extends classic private aggregation techniques to enable the collection of a large class of useful statistics. For example, Prio can perform a least-squares regression on high-dimensional client-provided data without ever seeing the data in the clear.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["similarities"]}, "@cite_12": {"mid": "2128227627", "abstract": "We present new cryptographic protocols for multiauthority secret ballot elections that guarantee privacy, robustness, and universal verifiability. Application of some novel techniques, in particular the construction of witness hiding indistinguishable protocols from Cramer, Damgard and Schoenmakers, and the verifiable secret sharing scheme of Pedersen, reduce the work required by the voter or an authority to a linear number of cryptographic operations in the population size (compared to quadratic in previous schemes). Thus we get significantly closer to a practical election scheme.", "ref_function": ["background", "method", "result"], "cite_purpose": ["similarities"]}}}
{"sentences": ["Most of these methods treat action recognition as a video classification problem.", "These works tend to focus on how motion is captured by the networks and largely ignore what makes the actions unique.", "In this work, we provide insights specific to the nature of the action recognition problem itself, showing how it requires an increased sensitivity to finer details.", "Different from the methods above, our work is explicitly designed for fine-grained action classification.", "In particular, the proposed approach is inspired by recent advances in the fine-grained recognition literature, such as @cite_33 .", "We hope that this work will help draw the attention of the community on understanding generic action classes as a fine-grained recognition problem."], "label": ["General descriptions of the topic", "Explaining the inadequacies of previous studies", "Describing the objective", "Describing the objective", "Describing used methods", "Other functional sentences"], "target_paper": "Action recognition has seen a dramatic performance improvement in the last few years. Most of the current state-of-the-art literature either aims at improving performance through changes to the backbone CNN network, or they explore different trade-offs between computational efficiency and performance, again through altering the backbone network. However, almost all of these works maintain the same last layers of the network, which simply consist of a global average pooling followed by a fully connected layer. In this work we focus on how to improve the representation capacity of the network, but rather than altering the backbone, we focus on improving the last layers of the network, where changes have low impact in terms of computational cost. In particular, we show that current architectures have poor sensitivity to finer details and we exploit recent advances in the fine-grained recognition literature to improve our model in this aspect. With the proposed approach, we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major large-scale action recognition benchmarks.", "reference": {"@cite_33": {"mid": "2798365843", "abstract": "Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for fine-grained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional filters that capture class-specific discriminative patches without extra part or bounding box annotations. Such a filter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are provided to understand our approach.", "ref_function": ["background", "method", "method", "method", "result", "result"], "cite_purpose": [""]}}}
{"sentences": ["There are several other areas of study regarding how best to use training data that are related to this work.", "Re-weighting or re-ordering training examples is a well-studied and related area of supervised learning.", "Often examples are re-weighted according to some notion of difficulty, or model uncertainty @cite_24 .", "In particular, the internal uncertainty of the model is used as the basis for selecting how training examples are weighted.", "However, model uncertainty depends upon the original training data the model was trained on, while here we use an external measure of uncertainty."], "label": ["General descriptions of the topic", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references"], "target_paper": "Incorporating Item Response Theory (IRT) into NLP tasks can provide valuable information about model performance and behavior. Traditionally, IRT models are learned using human response pattern (RP) data, presenting a significant bottleneck for large data sets like those required for training deep neural networks (DNNs). In this work we propose learning IRT models using RPs generated from artificial crowds of DNN models. We demonstrate the effectiveness of learning IRT models using DNN-generated data through quantitative and qualitative analyses for two NLP tasks. Parameters learned from human and machine RPs for natural language inference and sentiment analysis exhibit medium to large positive correlations. We demonstrate a use-case for latent difficulty item parameters, namely training set filtering, and show that using difficulty to sample training data outperforms baseline methods. Finally, we highlight cases where human expectation about item difficulty does not match difficulty as estimated from the machine RPs.", "reference": {"@cite_24": {"mid": "2963476860", "abstract": "Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.", "ref_function": ["background", "objective", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["For the deterministic setting a lot of effort has been dedicated to the study of the feasibility of rendezvous, and to the time required to achieve this task, when feasible.", "For instance, deterministic rendezvous with agents equipped with tokens used to mark nodes was considered, e.g., in @cite_28 .", "Deterministic rendezvous of two agents that cannot mark nodes but have unique labels was discussed in @cite_17 @cite_9 .", "These papers are concerned with the time of rendezvous in arbitrary graphs.", "In @cite_17 the authors show a rendezvous algorithm polynomial in the size of the graph, in the length of the shorter label and in the delay between the starting time of the agents.", "In @cite_9 rendezvous time is polynomial in the first two of these parameters and independent of the delay."], "label": ["General descriptions of the topic", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective", "Summarize the above references", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result"], "target_paper": "A team of mobile agents, starting from different nodes of an unknown network, possibly at different times, have to meet at the same node and declare that they have all met. Agents have different labels and move in synchronous rounds along links of the network. The above task is known as gathering and was traditionally considered under the assumption that when some agents are at the same node then they can talk. In this paper we ask the question of whether this ability of talking is needed for gathering. The answer turns out to be no. Our main contribution are two deterministic algorithms that always accomplish gathering in a much weaker model. We only assume that at any time an agent knows how many agents are at the node that it currently occupies but agents do not see the labels of other co-located agents and cannot exchange any information with them. They also do not see other nodes than the current one. Our first algorithm works under the assumption that agents know a priori some upper bound N on the network size, and it works in time polynomial in N and in the length l of the smallest label. Our second algorithm does not assume any a priori knowledge about the network but its complexity is exponential in the network size and in the labels of agents. Its purpose is to show feasibility of gathering under this harsher scenario. As a by-product of our techniques we obtain, in the same weak model, the solution of the fundamental problem of leader election among agents. As an application of our result we also solve, in the same model, the well-known gossiping problem: if each agent has a message at the beginning, we show how to make all messages known to all agents, even without any a priori knowledge about the network. If agents know an upper bound N on the network size then our gossiping algorithm works in time polynomial in N, in l and in the length of the largest message.", "reference": {"@cite_28": {"mid": "2131887891", "abstract": "In the rendezvous search problem, two mobile agents must move along the n nodes of a network so as to minimize the time required to meet or rendezvous. When the mobile agents are identical and the network is anonymous, however, the resulting symmetry can make the problem impossible to solve. Symmetry is typically broken by having the mobile agents run either a randomized algorithm or different deterministic algorithms. We investigate the use of identical tokens to break symmetry so that the two mobile agents can run the same deterministic algorithm. After deriving the explicit conditions under which identical tokens can be used to break symmetry on the n node ring, we derive the lower and upper bounds for the time and memory complexity of the rendezvous search problem with various parameter sets. While these results suggest a possible tradeoff between the mobile agents' memory and the time complexity of the rendezvous search problem, we prove that this tradeoff is limited.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2623545498", "abstract": "We obtain several improved solutions for the deterministic rendezvous problem in general undirected graphs. Our solutions answer several problems left open in a recent paper by We also introduce an interesting variant of the rendezvous problem which we call the deterministic treasure hunt problem. Both the rendezvous and the treasure hunt problems motivate the study of universal traversal sequences and universal exploration sequences with some strengthened properties. We call such sequences strongly universal traversal (exploration) sequences. We give an explicit construction of strongly universal exploration sequences. The existence of strongly universal traversal sequences, as well as the solution of the most difficult variant of the deterministic treasure hunt problem, are left as intriguing open problems.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_17": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}}}
{"sentences": ["Memory required by two anonymous agents to achieve deterministic rendezvous has been studied in @cite_22 for trees and in @cite_0 for general graphs.", "Memory needed for randomized rendezvous in the ring is discussed, e.g., in @cite_25 ."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: research objective"], "target_paper": "A team of mobile agents, starting from different nodes of an unknown network, possibly at different times, have to meet at the same node and declare that they have all met. Agents have different labels and move in synchronous rounds along links of the network. The above task is known as gathering and was traditionally considered under the assumption that when some agents are at the same node then they can talk. In this paper we ask the question of whether this ability of talking is needed for gathering. The answer turns out to be no. Our main contribution are two deterministic algorithms that always accomplish gathering in a much weaker model. We only assume that at any time an agent knows how many agents are at the node that it currently occupies but agents do not see the labels of other co-located agents and cannot exchange any information with them. They also do not see other nodes than the current one. Our first algorithm works under the assumption that agents know a priori some upper bound N on the network size, and it works in time polynomial in N and in the length l of the smallest label. Our second algorithm does not assume any a priori knowledge about the network but its complexity is exponential in the network size and in the labels of agents. Its purpose is to show feasibility of gathering under this harsher scenario. As a by-product of our techniques we obtain, in the same weak model, the solution of the fundamental problem of leader election among agents. As an application of our result we also solve, in the same model, the well-known gossiping problem: if each agent has a message at the beginning, we show how to make all messages known to all agents, even without any a priori knowledge about the network. If agents know an upper bound N on the network size then our gossiping algorithm works in time polynomial in N, in l and in the length of the largest message.", "reference": {"@cite_0": {"mid": "1972775782", "abstract": "Two identical (anonymous) mobile agents start from arbitrary nodes in an a priori unknown graph and move synchronously from node to node with the goal of meeting. This rendezvous problem has been thoroughly studied, both for anonymous and for labeled agents, along with another basic task, that of exploring graphs by mobile agents. The rendezvous problem is known to be not easier than graph exploration. A well-known recent result on exploration, due to Reingold, states that deterministic exploration of arbitrary graphs can be performed in log-space, i.e., using an agent equipped with O(log n) bits of memory, where n is the size of the graph. In this paper we study the size of memory of mobile agents that permits us to solve the rendezvous problem deterministically. Our main result establishes the minimum size of the memory of anonymous agents that guarantees deterministic rendezvous when it is feasible. We show that this minimum size is \u0398(log n), where n is the size of the graph, regardless of the delay between the starting times of the agents. More precisely, we construct identical agents equipped with \u0398(log n) memory bits that solve the rendezvous problem in all graphs with at most n nodes, if they start with any delay \u03c4, and we prove a matching lower bound \u03a9(log n) on the number of memory bits needed to accomplish rendezvous, even for simultaneous start. In fact, this lower bound is achieved already on the class of rings. This shows a significant contrast between rendezvous and exploration: e.g., while exploration of rings (without stopping) can be done using constant memory, rendezvous, even with simultaneous start, requires logarithmic memory. As a by-product of our techniques introduced to obtain log-space rendezvous we get the first algorithm to find a quotient graph of a given unlabeled graph in polynomial time, by means of a mobile agent moving around the graph.", "ref_function": ["background", "background", "background", "method", "result", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "1535538796", "abstract": "We present a tradeoff between the expected time for two identical agents to rendez-vous on a synchronous, anonymous, oriented ring and the memory requirements of the agents. In particular, we show that there exists a 2t state agent, which can achieve rendez-vous on an n node ring in expected time O(n2 2t + 2t) and that any t 2 state agent requires expected time \u03a9(n2 2t). As a corollary we observe that \u0398(log log n) bits of memory are necessary and sufficient to achieve rendezvous in linear time.", "ref_function": ["background", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2081055073", "abstract": "The aim of rendezvous in a graph is meeting of two mobile agents at some node of an unknown anonymous connected graph. In this article, we focus on rendezvous in trees, and, analogously to the efforts that have been made for solving the exploration problem with compact automata, we study the size of memory of mobile agents that permits to solve the rendezvous problem deterministically. We assume that the agents are identical, and move in synchronous rounds. We first show that if the delay between the starting times of the agents is arbitrary, then the lower bound on memory required for rendezvous is \u03a9(log n) bits, even for the line of length n. This lower bound meets a previously known upper bound of O(log n) bits for rendezvous in arbitrary graphs of size at most n. Our main result is a proof that the amount of memory needed for rendezvous with simultaneous start depends essentially on the number e of leaves of the tree, and is exponentially less impacted by the number n of nodes. Indeed, we present two identical agents with O(log e + log log n) bits of memory that solve the rendezvous problem in all trees with at most n nodes and at most e leaves. Hence, for the class of trees with polylogarithmically many leaves, there is an exponential gap in minimum memory size needed for rendezvous between the scenario with arbitrary delay and the scenario with delay zero. Moreover, we show that our upper bound is optimal by proving that \u03a9(log e + log log n) bits of memory are required for rendezvous, even in the class of trees with degrees bounded by 3.", "ref_function": ["objective", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Apart from the synchronous model used in this paper, several authors have investigated asynchronous gathering in the plane @cite_34 @cite_35 and in network environments @cite_19 @cite_1 @cite_30 @cite_13 .", "In the latter scenario the agent chooses the edge which it decides to traverse but the adversary controls the speed of the agent.", "Under this assumption rendezvous in a node cannot be guaranteed even in very simple graphs and hence the rendezvous requirement is relaxed to permit the agents to meet inside an edge."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies"], "target_paper": "A team of mobile agents, starting from different nodes of an unknown network, possibly at different times, have to meet at the same node and declare that they have all met. Agents have different labels and move in synchronous rounds along links of the network. The above task is known as gathering and was traditionally considered under the assumption that when some agents are at the same node then they can talk. In this paper we ask the question of whether this ability of talking is needed for gathering. The answer turns out to be no. Our main contribution are two deterministic algorithms that always accomplish gathering in a much weaker model. We only assume that at any time an agent knows how many agents are at the node that it currently occupies but agents do not see the labels of other co-located agents and cannot exchange any information with them. They also do not see other nodes than the current one. Our first algorithm works under the assumption that agents know a priori some upper bound N on the network size, and it works in time polynomial in N and in the length l of the smallest label. Our second algorithm does not assume any a priori knowledge about the network but its complexity is exponential in the network size and in the labels of agents. Its purpose is to show feasibility of gathering under this harsher scenario. As a by-product of our techniques we obtain, in the same weak model, the solution of the fundamental problem of leader election among agents. As an application of our result we also solve, in the same model, the well-known gossiping problem: if each agent has a message at the beginning, we show how to make all messages known to all agents, even without any a priori knowledge about the network. If agents know an upper bound N on the network size then our gossiping algorithm works in time polynomial in N, in l and in the length of the largest message.", "reference": {"@cite_30": {"mid": "2050063944", "abstract": "Two mobile agents (robots) having distinct labels and located in nodes of an unknown anonymous connected graph have to meet. We consider the asynchronous version of this well-studied rendezvous problem and we seek fast deterministic algorithms for it. Since in the asynchronous setting, meeting at a node, which is normally required in rendezvous, is in general impossible, we relax the demand by allowing meeting of the agents inside an edge as well. The measure of performance of a rendezvous algorithm is its cost: for a given initial location of agents in a graph, this is the number of edge traversals of both agents until rendezvous is achieved. If agents are initially situated at a distance D in an infinite line, we show a rendezvous algorithm with cost O(D|Lmin|2) when D is known and O((D + |Lmax|)3) if D is unknown, where |Lmin| and |Lmax| are the lengths of the shorter and longer label of the agents, respectively. These results still hold for the case of the ring of unknown size, but then we also give an optimal algorithm of cost O(n|Lmin|), if the size n of the ring is known, and of cost O(n|Lmax|), if it is unknown. For arbitrary graphs, we show that rendezvous is feasible if an upper bound on the size of the graph is known and we give an optimal algorithm of cost O(D|Lmin|) if the topology of the graph and the initial positions are known to agents.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_35": {"mid": "1635699204", "abstract": "We consider a collection of robots which are identical (anonymous), have limited visibility of the environment, and no memory of the past (oblivious); furthermore, they are totally asynchronous in their actions, computations, and movements. We show that, even in such a totally asynchronous setting, it is possible for the robots to gather in the same location in finite time, provided they have a compass.", "ref_function": ["background", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2100580556", "abstract": "Two mobile agents (robots) with distinct labels have to meet in an arbitrary, possibly infinite, unknown connected graph or in an unknown connected terrain in the plane. Agents are modeled as points, and the route of each of them only depends on its label and on the unknown environment. The actual walk of each agent also depends on an asynchronous adversary that may arbitrarily vary the speed of the agent, stop it, or even move it back and forth, as long as the walk of the agent in each segment of its route is continuous, does not leave it and covers all of it. Meeting in a graph means that both agents must be at the same time in some node or in some point inside an edge of the graph, while meeting in a terrain means that both agents must be at the same time in some point of the terrain. Does there exist a deterministic algorithm that allows any two agents to meet in any unknown environment in spite of this very powerful adversary? We give deterministic rendezvous algorithms for agents starting at arbitrary nodes of any anonymous connected graph (finite or infinite) and for agents starting at any interior points with rational coordinates in any closed region of the plane with path-connected interior. While our algorithms work in a very general setting - agents can, indeed, meet almost everywhere - we show that none of the above few limitations imposed on the environment can be removed. On the other hand, our algorithm also guarantees the following approximate rendezvous for agents starting at arbitrary interior points of a terrain as above: agents will eventually get at an arbitrarily small positive distance from each other.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "1493636406", "abstract": "Two anonymous mobile agents (robots) moving in an asynchronous manner have to meet in an infinite grid of dimension \u03b4 > 0, starting from two arbitrary positions at distance at most d. Since the problem is clearly infeasible in such general setting, we assume that the grid is embedded in a \u03b4-dimensional Euclidean space and that each agent knows the Cartesian coordinates of its own initial position (but not the one of the other agent). We design an algorithm permitting the agents to meet after traversing a trajectory of length O(d\u03b4 polylog d). This bound for the case of 2D-grids subsumes the main result of [12]. The algorithm is almost optimal, since the \u03a9(d\u03b4) lower bound is straightforward. Further, we apply our rendezvous method to the following network design problem. The ports of the \u03b4-dimensional grid have to be set such that two anonymous agents starting at distance at most d from each other will always meet, moving in an asynchronous manner, after traversing a O(d\u03b4 polylog d) length trajectory. We can also apply our method to a version of the geometric rendezvous problem. Two anonymous agents move asynchronously in the \u03b4-dimensional Euclidean space. The agents have the radii of visibility of r1 and r2, respectively. Each agent knows only its own initial position and its own radius of visibility. The agents meet when one agent is visible to the other one. We propose an algorithm designing the trajectory of each agent, so that they always meet after traveling a total distance of O((d r)\u03b4 polylog(d r)), where r = min(r1, r2) and for r \u2265 1.", "ref_function": ["background", "method", "method", "method", "method", "result", "background", "background", "background", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "2087073465", "abstract": "Consider a set of @math identical mobile computational entities in the plane, called robots, operating in Look-Compute-Move cycles, without any means of direct communication. The Gathering Problem is the primitive task of all entities gathering in finite time at a point not fixed in advance, without any external control. The problem has been extensively studied in the literature under a variety of strong assumptions (e.g., synchronicity of the cycles, instantaneous movements, complete memory of the past, common coordinate system, etc.). In this paper we consider the setting without those assumptions, that is, when the entities are oblivious (i.e., they do not remember results and observations from previous cycles), disoriented (i.e., have no common coordinate system), and fully asynchronous (i.e., no assumptions exist on timing of cycles and activities within a cycle). The existing algorithmic contributions for such robots are limited to solutions for @math or for restricted sets of initial configura...", "ref_function": ["background", "background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "1582826078", "abstract": "Two mobile agents starting at different nodes of an unknown network have to meet. This task is known in the literature as rendezvous. Each agent has a different label which is a positive integer known to it but unknown to the other agent. Agents move in an asynchronous way: the speed of agents may vary and is controlled by an adversary. The cost of a rendezvous algorithm is the total number of edge traversals by both agents until their meeting. The only previous deterministic algorithm solving this problem has cost exponential in the size of the graph and in the larger label. In this paper we present a deterministic rendezvous algorithm with cost polynomial in the size of the graph and in the length of the smaller label. Hence, we decrease the cost exponentially in the size of the graph and doubly exponentially in the labels of agents. As an application of our rendezvous algorithm we solve several fundamental problems involving teams of unknown size larger than 1 of labeled agents moving asynchronously in...", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["A different asynchronous model for gathering in ring networks was considered in @cite_36 @cite_20 .", "In this model, agents were memoryless but they could perform look operations which gave them a snapshot of the entire network with the positions of all agents in it."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken"], "target_paper": "A team of mobile agents, starting from different nodes of an unknown network, possibly at different times, have to meet at the same node and declare that they have all met. Agents have different labels and move in synchronous rounds along links of the network. The above task is known as gathering and was traditionally considered under the assumption that when some agents are at the same node then they can talk. In this paper we ask the question of whether this ability of talking is needed for gathering. The answer turns out to be no. Our main contribution are two deterministic algorithms that always accomplish gathering in a much weaker model. We only assume that at any time an agent knows how many agents are at the node that it currently occupies but agents do not see the labels of other co-located agents and cannot exchange any information with them. They also do not see other nodes than the current one. Our first algorithm works under the assumption that agents know a priori some upper bound N on the network size, and it works in time polynomial in N and in the length l of the smallest label. Our second algorithm does not assume any a priori knowledge about the network but its complexity is exponential in the network size and in the labels of agents. Its purpose is to show feasibility of gathering under this harsher scenario. As a by-product of our techniques we obtain, in the same weak model, the solution of the fundamental problem of leader election among agents. As an application of our result we also solve, in the same model, the well-known gossiping problem: if each agent has a message at the beginning, we show how to make all messages known to all agents, even without any a priori knowledge about the network. If agents know an upper bound N on the network size then our gossiping algorithm works in time polynomial in N, in l and in the length of the largest message.", "reference": {"@cite_36": {"mid": "2400422553", "abstract": "Consider a set of mobile robots placed on distinct nodes of a discrete, anonymous, and bidirectional ring. Asynchronously, each robot takes a snapshot of the ring, determining the size of the ring and which nodes are either occupied by robots or empty. Based on the observed configuration, it decides whether to move to one of its adjacent nodes or not. In the first case, it performs the computed move, eventually. This model of computation is known as Look-Compute-Move. The computation depends on the required task. In this paper, we solve both the well-known Gathering and Exclusive Searching tasks. In the former problem, all robots must simultaneously occupy the same node, eventually. In the latter problem, the aim is to clear all edges of the graph. An edge is cleared if it is traversed by a robot or if both its endpoints are occupied. We consider the exclusive searching where it must be ensured that two robots never occupy the same node. Moreover, since the robots are oblivious, the clearing is perpetual, i.e., the ring is cleared infinitely often. In the literature, most contributions are restricted to a subset of initial configurations. Here, we design two different algorithms and provide a characterization of the initial configurations that permit the resolution of the problems under very weak assumptions. More precisely, we provide a full characterization (except for few pathological cases) of the initial configurations for which gathering can be solved. The algorithm relies on the necessary assumption of the local-weak multiplicity detection. This means that during the Look phase a robot detects also whether the node it occupies is occupied by other robots, without acquiring the exact number. For the exclusive searching, we characterize all (except for few pathological cases) aperiodic configurations from which the problem is feasible. We also provide some impossibility results for the case of periodic configurations.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "objective", "background", "background", "background", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2144182788", "abstract": "We consider the problem of gathering identical, memoryless, mobile robots in one node of an anonymous unoriented ring. Robots start from different nodes of the ring. They operate in Look-Compute-Move cycles and have to end up in the same node. In one cycle, a robot takes a snapshot of the current configuration (Look), makes a decision to stay idle or to move to one of its adjacent nodes (Compute), and in the latter case makes an instantaneous move to this neighbor (Move). Cycles are performed asynchronously for each robot. For an odd number of robots we prove that gathering is feasible if and only if the initial configuration is not periodic, and we provide a gathering algorithm for any such configuration. For an even number of robots we decide the feasibility of gathering except for one type of symmetric initial configurations, and provide gathering algorithms for initial configurations proved to be gatherable.", "ref_function": ["background", "background", "background", "method", "method", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["In @cite_32 , the authors considered the problem of network exploration by many agents that could not communicate between them.", "However, the information available to an agent in each round was much different than in the present paper.", "Indeed, in @cite_32 , agents were getting local traffic reports consisting of answers to three questions: Am I alone in the node?", "'', Did any agent enter this node in this round?", "'', Did any agent leave this node in this round?''.", "To see that this feedback cannot be derived from our present assumption of knowing the number of agents co-located with an agent in a given round, consider the situation when an agent @math stays at a node, and in a given round one other agent leaves the node and another agent enters it.", "In our present model, agent @math does not notice any change, while in the model from @cite_32 it gets reports about somebody leaving the node and somebody entering it."], "label": ["Reference to single investigations in the past:  about objective", "Explaining the objective relationship between own work and references", "Reference to single investigations in the past: about method", "Not sure", "Not sure", "Other functional sentences", "Describing used methods"], "target_paper": "A team of mobile agents, starting from different nodes of an unknown network, possibly at different times, have to meet at the same node and declare that they have all met. Agents have different labels and move in synchronous rounds along links of the network. The above task is known as gathering and was traditionally considered under the assumption that when some agents are at the same node then they can talk. In this paper we ask the question of whether this ability of talking is needed for gathering. The answer turns out to be no. Our main contribution are two deterministic algorithms that always accomplish gathering in a much weaker model. We only assume that at any time an agent knows how many agents are at the node that it currently occupies but agents do not see the labels of other co-located agents and cannot exchange any information with them. They also do not see other nodes than the current one. Our first algorithm works under the assumption that agents know a priori some upper bound N on the network size, and it works in time polynomial in N and in the length l of the smallest label. Our second algorithm does not assume any a priori knowledge about the network but its complexity is exponential in the network size and in the labels of agents. Its purpose is to show feasibility of gathering under this harsher scenario. As a by-product of our techniques we obtain, in the same weak model, the solution of the fundamental problem of leader election among agents. As an application of our result we also solve, in the same model, the well-known gossiping problem: if each agent has a message at the beginning, we show how to make all messages known to all agents, even without any a priori knowledge about the network. If agents know an upper bound N on the network size then our gossiping algorithm works in time polynomial in N, in l and in the length of the largest message.", "reference": {"@cite_32": {"mid": "2089187143", "abstract": "A team consisting of an unknown number of mobile agents starting from different nodes of an unknown network, possibly at different times, have to explore the network: Every node must be visited by at least one agent, and all agents must eventually stop. Agents are anonymous (identical), execute the same deterministic algorithm, and move in synchronous rounds along links of the network. They are silent: They cannot send any messages to other agents or mark visited nodes in any way. In the absence of any additional information, exploration with termination of an arbitrary network in this model, devoid of any means of communication between agents, is impossible. Our aim is to solve the exploration problem by giving to agents very restricted local traffic reports. Specifically, an agent that is at a node v in a given round is provided with three bits of information answering the following questions: Am I alone at vq Did any agent enter v in this roundq Did any agent exit v in this roundq We show that this small amount of information permits us to solve the exploration problem in arbitrary networks. More precisely, we give a deterministic terminating exploration algorithm working in arbitrary networks for all initial configurations that are not perfectly symmetric; that is, in which there are agents with different views of the network. The algorithm works in polynomial time in the (unknown) size of the network. A deterministic terminating exploration algorithm working for all initial configurations in arbitrary networks does not exist.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "method"], "cite_purpose": ["background", "background", "differences"]}}}
{"sentences": ["In @cite_5 , the problem of conveying bits of information using movements of robots was considered in a context much different from ours.", "Mobile robots were moving in the plane and they could periodically get snapshots of the entire configuration of robots."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method"], "target_paper": "A team of mobile agents, starting from different nodes of an unknown network, possibly at different times, have to meet at the same node and declare that they have all met. Agents have different labels and move in synchronous rounds along links of the network. The above task is known as gathering and was traditionally considered under the assumption that when some agents are at the same node then they can talk. In this paper we ask the question of whether this ability of talking is needed for gathering. The answer turns out to be no. Our main contribution are two deterministic algorithms that always accomplish gathering in a much weaker model. We only assume that at any time an agent knows how many agents are at the node that it currently occupies but agents do not see the labels of other co-located agents and cannot exchange any information with them. They also do not see other nodes than the current one. Our first algorithm works under the assumption that agents know a priori some upper bound N on the network size, and it works in time polynomial in N and in the length l of the smallest label. Our second algorithm does not assume any a priori knowledge about the network but its complexity is exponential in the network size and in the labels of agents. Its purpose is to show feasibility of gathering under this harsher scenario. As a by-product of our techniques we obtain, in the same weak model, the solution of the fundamental problem of leader election among agents. As an application of our result we also solve, in the same model, the well-known gossiping problem: if each agent has a message at the beginning, we show how to make all messages known to all agents, even without any a priori knowledge about the network. If agents know an upper bound N on the network size then our gossiping algorithm works in time polynomial in N, in l and in the length of the largest message.", "reference": {"@cite_5": {"mid": "2944251479", "abstract": "We investigate avenues for the exchange of information (explicit communication) among deaf and dumb mobile robots scattered in the plane. We introduce the use of movement-signals (analogously to flight signals and bees waggle) as a mean to transfer messages, enabling the use of distributed algorithms among robots. We propose one-to-one deterministic movement protocols that implement explicit communication among asynchronous robots. We first show how the movements of robots can provide implicit acknowledgment in asynchronous systems. We use this result to design one-to-one communication among a pair of robots. Then, we propose two one-to-one communication protocols for any system of n *** 2 robots. The former works for robots equipped with observable IDs that agree on a common direction (sense of direction). The latter enables one-to-one communication assuming robots devoid of any observable IDs or sense of direction. All three protocols (for either two or any number of robots) assume that no robot remains inactive forever. However, they cannot avoid that the robots move either away or closer of each others, by the way requiring robots with an infinite visibility. In this paper, we also present how to overcome these two disadvantages. These protocols enable the use of distributing algorithms based on message exchanges among swarms of Stigmergic robots. They also allow robots to be equipped with the means of communication to tolerate faults in their communication devices.", "ref_function": ["background", "method", "method", "method", "method", "result", "background", "background", "background", "background", "objective", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Traditional MVS methods focus on designing neighbor selection and photometric error measures for efficient and accurate reconstruction @cite_26 @cite_34 @cite_9 .", "Furukawa al @cite_22 adopted geometric structures to reconstruct textured regions and applied Markov random fields to recover per-view depth maps.", "Langguth al @cite_15 used the shading-aware mechanism to improve the robustness of view selection.", "Wu al @cite_29 utilized the lighting and shadows information to enhance the performance of the ill-posed region.", "Michael al @cite_18 chose images to match (both at a per-view and per-pixel level) for addressing the dramatic changes in lighting, scale, clutter, and other effects.", "Schonberger al @cite_11 proposed the COLMAP framework, which applied photometric and geometric priors to optimize the view selection and used geometric consistency to refine the depth map."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "The success of existing deep-learning based multi-view stereo (MVS) approaches greatly depends on the availability of large-scale supervision in the form of dense depth maps. Such supervision, while not always possible, tends to hinder the generalization ability of the learned models in never-seen-before scenarios. In this paper, we propose the first unsupervised learning based MVS network, which learns the multi-view depth maps from the input multi-view images and does not need ground-truth 3D training data. Our network is symmetric in predicting depth maps for all views simultaneously, where we enforce cross-view consistency of multi-view depth maps during both training and testing stages. Thus, the learned multi-view depth maps naturally comply with the underlying 3D scene geometry. Besides, our network also learns the multi-view occlusion maps, which further improves the robustness of our network in handling real-world occlusions. Experimental results on multiple benchmarking datasets demonstrate the effectiveness of our network and the excellent generalization ability.", "reference": {"@cite_18": {"mid": "2156116778", "abstract": "We present a multi-view stereo algorithm that addresses the extreme changes in lighting, scale, clutter, and other effects in large online community photo collections. Our idea is to intelligently choose images to match, both at a per-view and per-pixel level. We show that such adaptive view selection enables robust performance even with dramatic appearance variability. The stereo matching technique takes as input sparse 3D points reconstructed from structure-from-motion methods and iteratively grows surfaces from these points. Optimizing for surface normals within a photoconsistency measure significantly improves the matching results. While the focus of our approach is to estimate high-quality depth maps, we also show examples of merging the resulting depth maps into compelling scene reconstructions. We demonstrate our algorithm on standard multi-view stereo datasets and on casually acquired photo collections of famous scenes gathered from the Internet.", "ref_function": ["background", "objective", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2257063750", "abstract": "This tutorial presents a hands-on view of the field of multi-view stereo with a focus on practical algorithms. Multi-view stereo algorithms are able to construct highly detailed 3D models from images alone. They take a possibly very large set of images and construct a 3D plausible geometry that explains the images under some reasonable assumptions, the most important being scene rigidity. The tutorial frames the multiview stereo problem as an image geometry consistency optimization problem. It describes in detail its main two ingredients: robust implementations of photometric consistency measures, and efficient optimization algorithms. It then presents how these main ingredients are used by some of the most successful algorithms, applied into real applications, and deployed as products in the industry. Finally it describes more advanced approaches exploiting domain-specific knowledge such as structural priors, and gives an overview of the remaining challenges and future research directions.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_9": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_29": {"mid": "2098551034", "abstract": "Multi-view stereo methods reconstruct 3D geometry from images well for sufficiently textured scenes, but often fail to recover high-frequency surface detail, particularly for smoothly shaded surfaces. On the other hand, shape-from-shading methods can recover fine detail from shading variations. Unfortunately, it is non-trivial to apply shape-from-shading alone to multi-view data, and most shading-based estimation methods only succeed under very restricted or controlled illumination. We present a new algorithm that combines multi-view stereo and shading-based refinement for high-quality reconstruction of 3D geometry models from images taken under constant but otherwise arbitrary illumination. We have tested our algorithm on several scenes that were captured under several general and unknown lighting conditions, and we show that our final reconstructions rival laser range scans.", "ref_function": ["background", "background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2519384515", "abstract": "We present a novel multi-view reconstruction approach that effectively combines stereo and shape-from-shading energies into a single optimization scheme. Our method uses image gradients to transition between stereo-matching (which is more accurate at large gradients) and Lambertian shape-from-shading (which is more robust in flat regions). In addition, we show that our formulation is invariant to spatially varying albedo without explicitly modeling it. We show that the resulting energy function can be optimized efficiently using a smooth surface representation based on bicubic patches, and demonstrate that this algorithm outperforms both previous multi-view stereo algorithms and shading based refinement approaches on a number of datasets.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2519683295", "abstract": "This work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate state-of-the-art performance in terms of accuracy, completeness, and efficiency.", "ref_function": ["background", "objective", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Different from the above geometry-based methods, learning-based approaches adopt convolution operation which has powerful feature learning capability for better pair-wise patch matching @cite_32 @cite_7 @cite_10 .", "Ji al @cite_24 pre-warped the multi-view images to 3D space, then used CNNs to regularize the cost volume.", "Huang al @cite_37 proposed DeepMVS, which aggregates information through a set of unordered images.", "Abhishek al @cite_31 directly leveraged camera parameters as the projection operation to form the cost volume, and achieved an end-to-end network.", "Yao al @cite_17 adopted a variance-based cost metric to aggregate the cost volume, then applied 3D convolutions to regularize and regress the depth map.", "Im al @cite_35 applied a plane sweeping approach to build a cost volume from deep features, then regularized the cost volume via a context-aware aggregation to improve depth regression.", "Very recently, Yao al @cite_4 introduced a scalable MVS framework based on the recurrent neural network to reduce the memory-consuming."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "The success of existing deep-learning based multi-view stereo (MVS) approaches greatly depends on the availability of large-scale supervision in the form of dense depth maps. Such supervision, while not always possible, tends to hinder the generalization ability of the learned models in never-seen-before scenarios. In this paper, we propose the first unsupervised learning based MVS network, which learns the multi-view depth maps from the input multi-view images and does not need ground-truth 3D training data. Our network is symmetric in predicting depth maps for all views simultaneously, where we enforce cross-view consistency of multi-view depth maps during both training and testing stages. Thus, the learned multi-view depth maps naturally comply with the underlying 3D scene geometry. Besides, our network also learns the multi-view occlusion maps, which further improves the robustness of our network in handling real-world occlusions. Experimental results on multiple benchmarking datasets demonstrate the effectiveness of our network and the excellent generalization ability.", "reference": {"@cite_35": {"mid": "2909918887", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_37": {"mid": "2964153986", "abstract": "We present DeepMVS, a deep convolutional neural network (ConvNet) for multi-view stereo reconstruction. Taking an arbitrary number of posed images as input, we first produce a set of plane-sweep volumes and use the proposed DeepMVS network to predict high-quality disparity maps. The key contributions that enable these results are (1) supervised pretraining on a photorealistic synthetic dataset, (2) an effective method for aggregating information across a set of unordered images, and (3) integrating multi-layer feature activations from the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using the ETH3D Benchmark. Our results show that DeepMVS compares favorably against state-of-the-art conventional MVS algorithms and other ConvNet based methods, particularly for near-textureless regions and thin structures.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2952740189", "abstract": "Deep learning has recently demonstrated its excellent performance for multi-view stereo (MVS). However, one major limitation of current learned MVS approaches is the scalability: the memory-consuming cost volume regularization makes the learned MVS hard to be applied to high-resolution scenes. In this paper, we introduce a scalable multi-view stereo framework based on the recurrent neural network. Instead of regularizing the entire 3D cost volume in one go, the proposed Recurrent Multi-view Stereo Network (R-MVSNet) sequentially regularizes the 2D cost maps along the depth direction via the gated recurrent unit (GRU). This reduces dramatically the memory consumption and makes high-resolution reconstruction feasible. We first show the state-of-the-art performance achieved by the proposed R-MVSNet on the recent MVS benchmarks. Then, we further demonstrate the scalability of the proposed method on several large-scale scenarios, where previous learned approaches often fail due to the memory constraint. Code is available at this https URL.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result", "other"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2563100679", "abstract": "Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object boundary detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing. In this work, we introduce a large-scale synthetic dataset with 500K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks.", "ref_function": ["background", "background", "background", "background", "method", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_24": {"mid": "2964243776", "abstract": "This paper proposes an end-to-end learning framework for multiview stereopsis. We term the network SurfaceNet. It takes a set of images and their corresponding camera parameters as input and directly infers the 3D model. The key advantage of the framework is that both photo-consistency as well geometric relations of the surface structure can be directly learned for the purpose of multiview stereopsis in an end-to-end fashion. SurfaceNet is a fully 3D convolutional network which is achieved by encoding the camera parameters together with the images in a 3D voxel representation. We evaluate SurfaceNet on the large-scale DTU benchmark.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2963966978", "abstract": "We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2962793285", "abstract": "We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet.", "ref_function": ["background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Unsupervised learning has been developed in monocular depth estimation and binocular stereo matching by exploiting the photometric consistency and regularization.", "Xie al @cite_25 proposed Deep3D to automatically convert 2D videos and images to stereoscopic 3D format.", "Zhou al @cite_30 proposed an unsupervised monocular depth prediction method by minimizing the image reconstruction error.", "Mahjourian al @cite_23 explicitly considered the inferred 3D geometry of the whole scene, where consistency of the estimated 3D point clouds and ego-motion across consecutive frames are enforced.", "Zhong al @cite_13 @cite_14 used the image warping error as the loss function to derive the learning process for estimating the disparity map."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "The success of existing deep-learning based multi-view stereo (MVS) approaches greatly depends on the availability of large-scale supervision in the form of dense depth maps. Such supervision, while not always possible, tends to hinder the generalization ability of the learned models in never-seen-before scenarios. In this paper, we propose the first unsupervised learning based MVS network, which learns the multi-view depth maps from the input multi-view images and does not need ground-truth 3D training data. Our network is symmetric in predicting depth maps for all views simultaneously, where we enforce cross-view consistency of multi-view depth maps during both training and testing stages. Thus, the learned multi-view depth maps naturally comply with the underlying 3D scene geometry. Besides, our network also learns the multi-view occlusion maps, which further improves the robustness of our network in handling real-world occlusions. Experimental results on multiple benchmarking datasets demonstrate the effectiveness of our network and the excellent generalization ability.", "reference": {"@cite_30": {"mid": "2609883120", "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_14": {"mid": "2887123368", "abstract": "Deep Learning based stereo matching methods have shown great successes and achieved top scores across different benchmarks. However, like most data-driven methods, existing deep stereo matching networks suffer from some well-known drawbacks such as requiring large amount of labeled training data, and that their performances are fundamentally limited by the generalization ability. In this paper, we propose a novel Recurrent Neural Network (RNN) that takes a continuous (possibly previously unseen) stereo video as input, and directly predicts a depth-map at each frame without a pre-training process, and without the need of ground-truth depth-maps as supervision. Thanks to the recurrent nature (provided by two convolutional-LSTM blocks), our network is able to memorize and learn from its past experiences, and modify its inner parameters (network weights) to adapt to previously unseen or unfamiliar environments. This suggests a remarkable generalization ability of the net, making it applicable in an open world setting. Our method works robustly with changes in scene content, image statistics, and lighting and season conditions etc. By extensive experiments, we demonstrate that the proposed method seamlessly adapts between different scenarios. Equally important, in terms of the stereo matching accuracy, it outperforms state-of-the-art deep stereo approaches on standard benchmark datasets such as KITTI and Middlebury stereo.", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2963906250", "abstract": "We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures. We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists. We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself.1", "ref_function": ["background", "background", "method", "objective", "method", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2751625733", "abstract": "Exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals, which are however not always available in many situations. In this paper, we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs. Training is performed in an end-to-end fashion without the need of ground-truth disparity maps. The idea is to use image warping error (instead of disparity-map residuals) as the loss function to drive the learning process, aiming to find a depth-map that minimizes the warping error. While this is a simple concept well-known in stereo matching, to make it work in a deep-learning framework, many non-trivial challenges must be overcome, and in this work we provide effective solutions. Our network is self-adaptive to different unseen imageries as well as to different camera settings. Experiments on KITTI and Middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin, and at the same time significantly faster.", "ref_function": ["background", "objective", "method", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2336968928", "abstract": "As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2D videos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Recent years have seen an explosion of applications of the deep learning methods to medical imaging, including computer-aided diagnosis (CAD) in radiology and medical image analysis @cite_37 @cite_23 @cite_27 .", "The efficiency of deep learning models for cytometry @cite_2 has been widely recognised and applied to cell imaging @cite_30 , virtual staining with generative adversarial networks (GAN) @cite_19 @cite_26 , fluorescence microscopy @cite_9 and reconstructing cell cycles and disease progression @cite_42 .", "However, despite the wide popularity and maturity of the deep learning approach, very little has been done to estimate the effect of biological activity of neuronal cells induced by compounds and searching for drugs that may protect against neurodegeneration and Alzheimer's disease.", "Simm in @cite_22 suggested to re-purpose high-throughput images assay to predict biological activity in drug discovery, however this approach depends on the features extracted from CellProfiler @cite_4 and lacks the flexibility of the CNN models @cite_7 which learn features directly from raw pixels of images."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "Understanding the morphological changes of primary neuronal cells induced by chemical compounds is essential for drug discovery. Using the data from a single high-throughput imaging assay, a classification model for predicting the biological activity of candidate compounds was introduced. The image recognition model which is based on deep convolutional neural network (CNN) architecture with residual connections achieved accuracy of 99.6 @math on a binary classification task of distinguishing untreated and treated rodent primary neuronal cells with Amyloid- @math .", "reference": {"@cite_30": {"mid": "2751723768", "abstract": "Abstract Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.", "ref_function": ["background", "background", "objective"], "cite_purpose": ["background"]}, "@cite_37": {"mid": "2731899572", "abstract": "The use of machine learning (ML) has been increasing rapidly in the medical imaging field, including computer-aided diagnosis (CAD), radiomics, and medical image analysis. Recently, an ML area called deep learning emerged in the computer vision field and became very popular in many fields. It started from an event in late 2012, when a deep-learning approach based on a convolutional neural network (CNN) won an overwhelming victory in the best-known worldwide computer vision competition, ImageNet Classification. Since then, researchers in virtually all fields, including medical imaging, have started actively participating in the explosively growing field of deep learning. In this paper, the area of deep learning in medical imaging is overviewed, including (1) what was changed in machine learning before and after the introduction of deep learning, (2) what is the source of the power of deep learning, (3) two major deep-learning models: a massive-training artificial neural network (MTANN) and a convolutional neural network (CNN), (4) similarities and differences between the two models, and (5) their applications to medical imaging. This review shows that ML with feature input (or feature-based ML) was dominant before the introduction of deep learning, and that the major and essential difference between ML before and after deep learning is the learning of image data directly without object segmentation or feature extraction; thus, it is the source of the power of deep learning, although the depth of the model is an important attribute. The class of ML with image input (or image-based ML) including deep learning has a long history, but recently gained popularity due to the use of the new terminology, deep learning. There are two major models in this class of ML in medical imaging, MTANN and CNN, which have similarities as well as several differences. In our experience, MTANNs were substantially more efficient in their development, had a higher performance, and required a lesser number of training cases than did CNNs. \u201cDeep learning\u201d, or ML with image input, in medical imaging is an explosively growing, promising field. It is expected that ML with image input will be the mainstream area in the field of medical imaging in the next few decades.", "ref_function": ["background", "background", "background", "background", "result", "background", "background", "background", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2107554012", "abstract": "Biologists can now prepare and image thousands of samples per day using automation, enabling chemical screens and functional genomics (for example, using RNA interference). Here we describe the first free, open-source system designed for flexible, high-throughput cell image analysis, CellProfiler. CellProfiler can address a variety of biological questions quantitatively, including standard assays (for example, cell count, size, per-cell protein levels) and complex morphological assays (for example, cell organelle shape or subcellular patterns of DNA or protein staining).", "ref_function": ["background", "objective", "method"], "cite_purpose": ["motivation", "background"]}, "@cite_22": {"mid": "2794301983", "abstract": "Summary In both academia and the pharmaceutical industry, large-scale assays for drug discovery are expensive and often impractical, particularly for the increasingly important physiologically relevant model systems that require primary cells, organoids, whole organisms, or expensive or rare reagents. We hypothesized that data from a single high-throughput imaging assay can be repurposed to predict the biological activity of compounds in other assays, even those targeting alternate pathways or biological processes. Indeed, quantitative information extracted from a three-channel microscopy-based screen for glucocorticoid receptor translocation was able to predict assay-specific biological activity in two ongoing drug discovery projects. In these projects, repurposing increased hit rates by 50- to 250-fold over that of the initial project assays while increasing the chemical structure diversity of the hits. Our results suggest that data from high-content screens are a rich source of information that can be used to predict and replace customized biological assays.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_7": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["motivation", "background"]}, "@cite_9": {"mid": "2904591139", "abstract": "We present deep-learning-enabled super-resolution across different fluorescence microscopy modalities. This data-driven approach does not require numerical modeling of the imaging process or the estimation of a point-spread-function, and is based on training a generative adversarial network (GAN) to transform diffraction-limited input images into super-resolved ones. Using this framework, we improve the resolution of wide-field images acquired with low-numerical-aperture objectives, matching the resolution that is acquired using high-numerical-aperture objectives. We also demonstrate cross-modality super-resolution, transforming confocal microscopy images to match the resolution acquired with a stimulated emission depletion (STED) microscope. We further demonstrate that total internal reflection fluorescence (TIRF) microscopy images of subcellular structures within cells and tissues can be transformed to match the results obtained with a TIRF-based structured illumination microscope. The deep network rapidly outputs these super-resolved images, without any iterations or parameter search, and could serve to democratize super-resolution imaging.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_42": {"mid": "2750796620", "abstract": "We show that deep convolutional neural networks combined with nonlinear dimension reduction enable reconstructing biological processes based on raw image data. We demonstrate this by reconstructing the cell cycle of Jurkat cells and disease progression in diabetic retinopathy. In further analysis of Jurkat cells, we detect and separate a subpopulation of dead cells in an unsupervised manner and, in classifying discrete cell cycle stages, we reach a sixfold reduction in error rate compared to a recent approach based on boosting on image features. In contrast to previous methods, deep learning based predictions are fast enough for on-the-fly analysis in an imaging flow cytometer.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2794744817", "abstract": "The histological analysis of tissue samples, widely used for disease diagnosis, involves lengthy and laborious tissue preparation. Here, we show that a convolutional neural network trained using a generative adversarial-network model can transform wide-field autofluorescence images of unlabelled tissue sections into images that are equivalent to the bright-field images of histologically stained versions of the same samples. A blind comparison, by board-certified pathologists, of this virtual staining method and standard histological staining using microscopic images of human tissue sections of the salivary gland, thyroid, kidney, liver and lung, and involving different types of stain, showed no major discordances. The virtual-staining method bypasses the typically labour-intensive and costly histological staining procedures, and could be used as a blueprint for the virtual staining of tissue images acquired with other label-free imaging modalities. Deep learning can be used to virtually stain autofluorescence images of unlabelled tissue sections, generating images that are equivalent to the histologically stained versions.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_23": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2905502540", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["There has been growing interest in first-order algorithms for non-convex minimization problems with no constraints or simple constraints in both stochastic and deterministic settings.", "Initially, the research in this direction mainly focuses on problems with smooth objective functions @cite_31 @cite_55 @cite_0 @cite_8 @cite_52 @cite_57 @cite_9 @cite_66 @cite_42 @cite_67 .", "Recently, algorithms and theories have been developed for non-convex problems with non-smooth (but weakly convex) objective functions @cite_89 @cite_88 @cite_102 @cite_50 @cite_26 @cite_79 .", "These works tackle the non-smoothness by introducing the Moreau envelope of the objective function.", "However, for with sophisticated functional constraints, these methods are not applicable."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_67": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2963625269", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2570983198", "abstract": "We analyze a fast incremental aggregated gradient method for optimizing nonconvex problems of the form min\u03a3 i \u0192 i (x). Specifically, we analyze the SAGA algorithm within an Incremental First-order Oracle framework, and show that it converges to a stationary point provably faster than both gradient descent and stochastic gradient descent. We also discuss a Polyak's special class of nonconvex problems for which SAGA converges at a linear rate to the global optimum. Finally, we analyze the practically valuable regularized and minibatch variants of SAGA. To our knowledge, this paper presents the first analysis of fast convergence for an incremental aggregated gradient method for nonconvex problems.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_55": {"mid": "1987083649", "abstract": "In this paper, we generalize the well-known Nesterov's accelerated gradient (AG) method, originally designed for convex smooth optimization, to solve nonconvex and possibly stochastic optimization problems. We demonstrate that by properly specifying the stepsize policy, the AG method exhibits the best known rate of convergence for solving general nonconvex smooth optimization problems by using first-order information, similarly to the gradient descent method. We then consider an important class of composite optimization problems and show that the AG method can solve them uniformly, i.e., by using the same aggressive stepsize policy as in the convex case, even if the problem turns out to be nonconvex. We demonstrate that the AG method exhibits an optimal rate of convergence if the composite problem is convex, and improves the best known rate of convergence if the problem is nonconvex. Based on the AG method, we also present new nonconvex stochastic approximation methods and show that they can improve a few existing rates of convergence for nonconvex stochastic optimization. To the best of our knowledge, this is the first time that the convergence of the AG method has been established for solving nonconvex nonlinear programming in the literature.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2963763253", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_42": {"mid": "2460087882", "abstract": "We give a simple proof that the Frank-Wolfe algorithm obtains a stationary point at a rate of @math on non-convex objectives with a Lipschitz continuous gradient. Our analysis is affine invariant and is the first, to the best of our knowledge, giving a similar rate to what was already proven for projected gradient methods (though on slightly different measures of stationarity).", "ref_function": ["background", "method"], "cite_purpose": ["background"]}, "@cite_52": {"mid": "2963965485", "abstract": "We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we obtain nonasymptotic rates of convergence of SVRG for nonconvex optimization, showing that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants, showing (theoretical) linear speedup due to minibatching in parallel settings.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_102": {"mid": "2963534244", "abstract": "We consider global efficiency of algorithms for minimizing a sum of a convex function and a composition of a Lipschitz convex function with a smooth map. The basic algorithm we rely on is the prox-linear method, which in each iteration solves a regularized subproblem formed by linearizing the smooth map. When the subproblems are solved exactly, the method has efficiency ( O ( ^ -2 ) ), akin to gradient descent for smooth minimization. We show that when the subproblems can only be solved by first-order methods, a simple combination of smoothing, the prox-linear method, and a fast-gradient scheme yields an algorithm with complexity ( O ( ^ -3 ) ). We round off the paper with an inertial prox-linear method that automatically accelerates in presence of convexity.", "ref_function": ["background", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_89": {"mid": "2786313301", "abstract": "We prove that the projected stochastic subgradient method, applied to a weakly convex problem, drives the gradient of the Moreau envelope to zero at the rate @math .", "ref_function": ["background"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2337540838", "abstract": "Recently, stochastic momentum methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_57": {"mid": "2803240098", "abstract": "In this paper, we present new stochastic methods for solving two important classes of nonconvex optimization problems. We first introduce a randomized accelerated proximal gradient (RapGrad) method for solving a class of nonconvex optimization problems consisting of the sum of @math component functions, and show that it can significantly reduce the number of gradient computations especially when the condition number @math (i.e., the ratio between the Lipschitz constant and negative curvature) is large. More specifically, RapGrad can save up to @math gradient computations than existing deterministic nonconvex accelerated gradient methods. Moreover, the number of gradient computations required by RapGrad can be @math (at least @math ) times smaller than the best-known randomized nonconvex gradient methods when @math . Inspired by RapGrad, we also develop a new randomized accelerated proximal dual (RapDual) method for solving a class of multi-block nonconvex optimization problems coupled with linear constraints. We demonstrate that RapDual can also save up to a factor of @math projection subproblems than its deterministic counterpart, where @math denotes the number of blocks. To the best of our knowledge, all these complexity results associated with RapGrad and RapDual seem to be new in the literature. We also illustrate potential advantages of these algorithms through our preliminary numerical experiments.", "ref_function": ["background", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_79": {"mid": "2807821938", "abstract": "In this paper, we investigate the non-asymptotic stationary convergence behavior of Stochastic Mirror Descent (SMD) for nonconvex optimization. We focus on a general class of nonconvex nonsmooth stochastic optimization problems, in which the objective can be decomposed into a relatively weakly convex function (possibly non-Lipschitz) and a simple non-smooth convex regularizer. We prove that SMD, without the use of mini-batch, is guaranteed to converge to a stationary point in a convergence rate of @math . The efficiency estimate matches with existing results for stochastic subgradient method, but is evaluated under a stronger stationarity measure. Our convergence analysis applies to both the original SMD and its proximal version, as well as the deterministic variants, for solving relatively weakly convex problems.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_50": {"mid": "2735159666", "abstract": "In this paper, we introduce a stochastic projected subgradient method for weakly convex (i.e., uniformly prox-regular) nonsmooth, nonconvex functions---a wide class of functions which includes the additive and convex composite classes. At a high-level, the method is an inexact proximal point iteration in which the strongly convex proximal subproblems are quickly solved with a specialized stochastic projected subgradient method. The primary contribution of this paper is a simple proof that the proposed algorithm converges at the same rate as the stochastic gradient method for smooth nonconvex problems. This result appears to be the first convergence rate analysis of a stochastic (or even deterministic) subgradient method for the class of weakly convex functions.", "ref_function": ["background", "method", "objective", "result"], "cite_purpose": ["background"]}, "@cite_88": {"mid": "2790417304", "abstract": "We consider an algorithm that successively samples and minimizes stochastic models of the objective function. We show that under weak-convexity and Lipschitz conditions, the algorithm drives the expected norm of the gradient of the Moreau envelope to zero at the rate @math . Our result yields new complexity guarantees for the stochastic proximal point algorithm on weakly convex problems and for the stochastic prox-linear algorithm for minimizing compositions of convex functions with smooth maps. Moreover, our result also recovers the recently obtained complexity estimate for the stochastic proximal subgradient method on weakly convex problems.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2963470657", "abstract": "In this paper, we introduce a new stochastic approximation type algorithm, namely, the randomized stochastic gradient (RSG) method, for solving an important class of nonlinear (possibly nonconvex) stochastic programming problems. We establish the complexity of this method for computing an approximate stationary point of a nonlinear programming problem. We also show that this method possesses a nearly optimal rate of convergence if the problem is convex. We discuss a variant of the algorithm which consists of applying a postoptimization phase to evaluate a short list of solutions generated by several independent runs of the RSG method, and we show that such modification allows us to improve significantly the large-deviation properties of the algorithm. These methods are then specialized for solving a class of simulation-based optimization problems in which only stochastic zeroth-order information is available.", "ref_function": ["background", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_66": {"mid": "2962851402", "abstract": "We consider the fundamental problem in nonconvex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order nonconvex optimization remain to be full gradient descent that converges in O(1 e) iterations for smooth objectives, and stochastic gradient descent that converges in O(1 e2) iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an O(1 e) rate, and is faster than full gradient descent by \u03a9(n1 3). We demonstrate the effectiveness of our methods on empirical risk minimizations with nonconvex loss functions and training neural nets.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["When all constraint functions in are affine, a primal-dual Frank-Wolfe method is proposed in @cite_72 , and it finds an @math -stationary point with a complexity of @math in general and @math when there exists a strictly feasible solution.", "Compared to @cite_72 , this paper uses a different notion of @math -stationary point and our constraint functions can be nonlinear and non-convex."], "label": ["Reference to single investigations in the past: about method", "Describing used methods"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_72": {"mid": "2805494515", "abstract": "We study constrained stochastic programs where the decision vector at each time slot cannot be chosen freely but is tied to the realization of an underlying random state vector. The goal is to minimize a general objective function subject to linear constraints. A typical scenario where such programs appear is opportunistic scheduling over a network of time-varying channels, where the random state vector is the channel state observed, and the control vector is the transmission decision which depends on the current channel state. We consider a primal-dual type Frank-Wolfe algorithm that has a low complexity update during each slot and that learns to make efficient decisions without prior knowledge of the probability distribution of the random state vector. We establish convergence time guarantees for the case of both convex and non-convex objective functions. We also emphasize application of the algorithm to non-convex opportunistic scheduling and distributed non-convex stochastic optimization over a connected graph.", "ref_function": ["background", "objective", "background", "method", "method", "result"], "cite_purpose": ["background", "differences"]}}}
{"sentences": ["As a classical approach for solving constrained optimization , a penalty method finds an approximate solution by solving a sequence of unconstrained subproblems, where the violation of constraints is penalized by the positively weighted penalty terms in the objective function of the subproblems.", "Unconstrained optimization techniques are then applied to the subproblems along with an updating scheme for the weighting parameters.", "The computational complexity of penalty methods for convex problems has been well established @cite_17 @cite_53 @cite_7 .", "For non-convex problems, most existing studies of penalty methods focus on the asymptotic convergence to a stationary point @cite_70 @cite_97 @cite_27 @cite_99 @cite_100 @cite_90 @cite_10 @cite_91 @cite_69 @cite_33 @cite_34 @cite_78 @cite_23 @cite_60 .", "On the contrary, we analyze the finite complexity of penalty methods for finding a near-stationary point."], "label": ["Reference to current state of knowledge", "Reference to current state of knowledge", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "Describing used methods"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_99": {"mid": "1994126718", "abstract": "Consider the problem of finding the local constrained minimum @math of the function f on the set [ F = x R^n | _i (x) 0, _ j + k (x) = 0; i = 1,2, ,k; j = 1,2, ,l . ]One method of solution is to minimize the associated penalty function [ p_0 (x) = f(x) - _ i = 1 ^k ( 0, _i (x) ) + _ j = 1 ^l | _ j + k (x) | for x R^n , 0. ]Let @math be the minimum of this penalty function. It is known that, provided @math is sufficiently small, @math .However, until recently, a serious drawback to this particular penalty function was that its first order derivatives are not everywhere defined. Thus, well-known gradient type methods usually applied to unconstrained optimization problems were necessarily excluded.This paper presents a method that enables a modified form of the gradient type approaches to be applied to a perturbation of the penalt...", "ref_function": ["method", "method", "background", "method"], "cite_purpose": ["background"]}, "@cite_69": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_91": {"mid": "2018296860", "abstract": "In this paper a new continuously differentiable exact penalty function is introduced for the solution of nonlinear programming problems with compact feasible set. A distinguishing feature of the penalty function is that it is defined on a suitable bounded open set containing the feasible region and that it goes to infinity on the boundary of this set. This allows the construction of an implementable unconstrained minimization algorithm, whose global convergence towards Kuhn-Tucker points of the constrained problem can be established.", "ref_function": ["background", "background", "method"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2893640118", "abstract": "We develop two new proximal alternating penalty algorithms to solve a wide range class of constrained convex optimization problems. Our approach mainly relies on a novel combination of the classical quadratic penalty, alternating minimization, Nesterov\u2019s acceleration, adaptive strategy for parameters. The first algorithm is designed to solve generic and possibly nonsmooth constrained convex problems without requiring any Lipschitz gradient continuity or strong convexity, while achieving the best-known ( O ( 1 k ) )-convergence rate in a non-ergodic sense, where k is the iteration counter. The second algorithm is also designed to solve non-strongly convex, but semi-strongly convex problems. This algorithm can achieve the best-known ( O ( 1 k^2 ) )-convergence rate on the primal constrained problem. Such a rate is obtained in two cases: (1) averaging only on the iterate sequence of the strongly convex term, or (2) using two proximal operators of this term without averaging. In both algorithms, we allow one to linearize the second subproblem to use the proximal operator of the corresponding objective term. Then, we customize our methods to solve different convex problems, and lead to new variants. As a byproduct, these algorithms preserve the same convergence guarantees as in our main algorithms. We verify our theoretical development via different numerical examples and compare our methods with some existing state-of-the-art algorithms.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_70": {"mid": "1976086748", "abstract": "The non-linear programming problem seeks to maximize a function f(x) where the n component vector x must satisfy certain constraints gi(x) = 0, i = 1, \u2026, m1 and gi(z) \u2267 0, i = m1 + 1, \u2026, m. The algorithm presented in this paper solves the non-linear programming problem by transforming it into a sequence of unconstrained maximization problems. Essentially, a penalty is imposed whenever x does not satisfy the constraints. Although the algorithm appears most useful in the concave case, the convergence proof holds for non-concave functions as well. The algorithm is especially interesting in the concave case because the programming problem reduces to a single unconstrained maximization problem or, at most, to a finite sequence of unconstrained maximization problems. In addition, the paper presents a new class of dual problems, and the algorithm is shown to be a dual feasible method. Another property of the algorithm is that it appears particularly well suited for large-scale problems with a sizable number of c...", "ref_function": ["method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_97": {"mid": "2081956522", "abstract": "An algorithm for solving the problem: minimize @math (a convex function) subject to @math , @math , each @math a concave function, is presented. Specifically, the function [ P [ x,t,r_k ] f( x ) + r_k^ - 1 [ g_i ( x ) - t_i ] ^2 ] is minimized over all x, nonnegative t, for a strictly decreasing null sequence @math . This extends the work of T. Pietrzykowski [5]. It is proved that for every @math , there exists a finite point @math which minimizes P, and which solves the convex programming problem as @math . This algorithm is similar to the Sequential Unconstrained Minimization Technique (SUMT) [1] in that it solves the (Wolfe) dual programming problem [6]. It differs from SUMT in that (1) it approaches the optimum from the region of infeasibility (i.e., it is a relaxation technique), (2) it does not require a nonempty interior to the nonlinearly constrained region, (3) no separate feasibilit...", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_53": {"mid": "2605995072", "abstract": "In this paper we present a complete iteration complexity analysis of inexact first-order Lagrangian and penalty methods for solving cone-constrained convex problems that have or may not have optimal Lagrange multipliers that close the duality gap. We first assume the existence of optimal Lagrange multipliers and study primal\u2013dual first-order methods based on inexact information and augmented Lagrangian smoothing or Nesterov-type smoothing. For inexact (fast) gradient augmented Lagrangian methods, we derive an overall computational complexity of O(1 \u03f5) projections onto a simple primal set in order to attain an e-optimal solution of the conic convex problem. For the inexact fast gradient method combined with Nesterov-type smoothing, we derive computational complexity O(1 \u03f53 2) projections onto the same set. Then, we assume that optimal Lagrange multipliers might not exist for the cone-constrained convex problem, and analyse the fast gradient method for solving penalty reformulations of the problem. For the ...", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_90": {"mid": "1985514811", "abstract": "It is shown that the existence of a strict local minimum satisfying the constraint qualification of [16] or McCormick's [12] second order sufficient optimality condition implies the existence of a class of exact local penalty functions (that is ones with a finite value of the penalty parameter) for a nonlinear programming problem. A lower bound to the penalty parameter is given by a norm of the optimal Lagrange multipliers which is dual to the norm used in the penalty function.", "ref_function": ["background", "method"], "cite_purpose": ["background"]}, "@cite_78": {"mid": "2090523290", "abstract": "The convergence behaviour of a class of iterative methods for solving the constrained minimization problem is analysed. The methods are based on the sequential minimization of a simple differentiable penalty function. They are sufficiently general to ensure global convergence of the iterates to the solution of the problem at an asymptotic (two-step Q-) superlinear rate.", "ref_function": ["background", "method", "method"], "cite_purpose": ["background"]}, "@cite_60": {"mid": "2122587727", "abstract": "The global convergence properties of a class of penalty methods for nonlinear programming are analyzed. These methods include successive linear programming approaches and, more specifically, the successive linear-quadratic programming approach presented by [Math. Program., 100 (2004), pp. 27--48]. Every iteration requires the solution of two trust-region subproblems involving piecewise linear and quadratic models, respectively. It is shown that, for a fixed penalty parameter, the sequence of iterates approaches stationarity of the penalty function. A procedure for dynamically adjusting the penalty parameter is described, and global convergence results for it are established.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "2053661713", "abstract": "The main result of the paper consists of the theorem that under certain, natural assumptions the local conditional maximum @math of the function f on the set [ A = x R^n | _i (x) 0, _j (x) = 0,i = 1, ,k,j = 1, ,l ] is identical with the unconditional maximum of the potential function [ p(x, ) = f(x) + _ i = 1 ^k neg ( _i (x)) - _ j = 1 ^l | (x) | , x R^n , 0, ] for @math sufficiently small. There is also provided a draft of a modified gradient procedure for maximizing the potential @math since it is generally nonsmooth even for differentiable f, @math and @math .", "ref_function": ["background", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2008832947", "abstract": "In their seminal papers Eremin [Soviet Mathematics Doklady, 8 (1966), pp. 459\u2013462] and Zangwill [Management Science, 13 (1967), pp. 344\u2013358] introduce a notion of exact penalization for use in the development of algorithms for constrained optimization. Since that time, exact penalty functions have continued to play a key role in the theory of mathematical programming. In the present paper, this theory is unified by showing how the Eremin\u2013Zangwill exact penalty functions can be used to develop the foundations of the theory of constrained optimization for finite dimensions in an elementary and straightforward way. Regularity conditions, multiplier rules, second-order optimality conditions, and convex programming are all given interpretations relative to the Eremin\u2013Zangwill exact penalty functions. In conclusion, a historical review of those results associated with the existence of an exact penalty parameter is provided.", "ref_function": ["background", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_100": {"mid": "2032024979", "abstract": "This paper presents a multiplier method for solving optimization problems with equality and inequality constraints. The method realizes all the good features that were foreseen by R. Fletcher for this type of algorithm in the past, but which suffers from none of the drawbacks of the earlier attempts.", "ref_function": ["method", "method"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "1994707765", "abstract": "In this paper, a recursive quadratic programming algorithm for solving equality constrained optimization problems is proposed and studied. The line search functions used are approximations to Fletcher's differentiable exact penalty function. Global convergence and local superlinear convergence results are proved, and some numerical results are given.", "ref_function": ["objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2174004662", "abstract": "In this paper it is shown that, given a nonlinear programming problem with inequality constraints, it is possible to construct a continuously differentiable exact penalty function whose global or local unconstrained minimizers correspond to global or local solutions of the constrained problem.", "ref_function": ["background"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2144603975", "abstract": "This paper considers a special but broad class of convex programming problems whose feasible region is a simple compact convex set intersected with the inverse image of a closed convex cone under an affine transformation. It studies the computational complexity of quadratic penalty based methods for solving the above class of problems. An iteration of these methods, which is simply an iteration of Nesterov\u2019s optimal method (or one of its variants) for approximately solving a smooth penalization subproblem, consists of one or two projections onto the simple convex set. Iteration-complexity bounds expressed in terms of the latter type of iterations are derived for two quadratic penalty based variants, namely: one which applies the quadratic penalty method directly to the original problem and another one which applies the latter method to a perturbation of the original problem obtained by adding a small quadratic term to its objective function.", "ref_function": ["background", "method", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["On solving a problem with a non-convex objective and linear constraint, @cite_25 has developed a quadratic-penalty accelerated inexact proximal point method.", "That method can generate an @math -stationary point in the sense of with a complexity of @math .", "Our method is similar to that in @cite_25 by utilizing the techniques from both the proximal point method and the quadratic penalty method.", "Although we make a little stronger assumption than @cite_25 by requiring the boundedness of @math , our method and analysis apply to the problems with non-convex objectives and convex non-convex nonlinear constraint functions.", "When the constraints are convex (but possibly nonlinear), our method can find a nearly @math -stationary point with a complexity of @math that is a nearly @math improvement over the complexity in @cite_25 ."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Explaining the method relationship between own work and references", "Describing used methods", "Describing the results"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_25": {"mid": "2787445655", "abstract": "This paper analyzes the iteration-complexity of a quadratic penalty accelerated inexact proximal point method for solving linearly constrained nonconvex composite programs. More specifically, the objective function is of the form @math where @math is a differentiable function whose gradient is Lipschitz continuous and @math is a closed convex function with bounded domain. The method, basically, consists of applying an accelerated inexact proximal point method for solving approximately a sequence of quadratic penalized subproblems associated to the linearly constrained problem. Each subproblem of the proximal point method is in turn approximately solved by an accelerated composite gradient method. It is shown that the proposed scheme generates a @math -approximate stationary point in at most @math . Finally, numerical results showing the efficiency of the proposed method are also given.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background", "similarities", "differences", "differences"]}}}
{"sentences": ["Barrier methods @cite_54 @cite_83 @cite_56 @cite_74 @cite_16 @cite_11 @cite_3 @cite_36 @cite_13 are another traditional class of algorithms for constrained optimization.", "Similar to the penalty methods, they also solve a sequence of unconstrained subproblems with barrier functions added to objective function.", "The barrier functions will increase to infinity as the iterates approach the boundary of the feasible set, and thus enforce the iterates to stay in the interior of the feasible set.", "However, the convergence rate of barrier methods is only shown when the problem is convex @cite_3 @cite_36 @cite_13 @cite_5 , and only asymptotic convergence analysis is available for non-convex problems."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: about results", "Explaining the inadequacies of previous studies"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_36": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}, "@cite_54": {"mid": "1987953347", "abstract": "Interest in linear programming has been intensified recently by Karmarkar's publication in 1984 of an algorithm that is claimed to be much faster than the simplex method for practical problems. We review classical barrier-function methods for nonlinear programming based on applying a logarithmic transformation to inequality constraints. For the special case of linear programming, the transformed problem can be solved by a \"projected Newton barrier\" method. This method is shown to be equivalent to Karmarkar's projective method for a particular choice of the barrier parameter. We then present details of a specific barrier algorithm and its practical implementation. Numerical results are given for several non-trivial test problems, and the implications for future developments in linear programming are discussed.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2011430287", "abstract": "Many scientific and engineering applications feature nonsmooth convex minimization problems over convex sets. In this paper, we address an important instance of this broad class where we assume that the nonsmooth objective is equipped with a tractable proximity operator and that the convex constraint set affords a self-concordant barrier. We provide a new joint treatment of proximal and self-concordant barrier concepts and illustrate that such problems can be efficiently solved, without the need of lifting the problem dimensions, as in disciplined convex optimization approach. We propose an inexact path-following algorithmic framework and theoretically characterize the worst-case analytical complexity of this framework when the proximal subproblems are solved inexactly. To show the merits of our framework, we apply its instances to both synthetic and real-world applications, where it shows advantages over standard interior point methods. As a byproduct, we describe how our framework can obtain points on t...", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_56": {"mid": "2157590940", "abstract": "Interior methods for optimization were widely used in the 1960s, primarily in the form of barrier methods. However, they were not seriously applied to linear programming because of the dominance of the simplex method. Barrier methods fell from favour during the 1970s for a variety of reasons, including their apparent inefficiency compared with the best available alternatives. In 1984, Karmarkar's announcement of a fast polynomial-time interior method for linear programming caused tremendous excitement in the field of optimization. A formal connection can be shown between his method and classical barrier methods, which have consequently undergone a renaissance in interest and popularity. Most papers published since 1984 have concentrated on issues of computational complexity in interior methods for linear programming. During the same period, implementations of interior methods have displayed great efficiency in solving many large linear programs of ever-increasing size. Interior methods have also been applied with notable success to nonlinear and combinatorial problems. This paper presents a self-contained survey of major themes in both classical material and recent developments related to the theory and practice of interior methods.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_83": {"mid": "2014746566", "abstract": "Abstract : This report gives the most comprehensive and detailed treatment to date of some of the most powerful mathematical programming techniques currently known--sequential unconstrained methods for constrained minimization problems in Euclidean n-space--giving many new results not published elsewhere. It provides a fresh presentation of nonlinear programming theory, a detailed review of other unconstrained methods, and a development of the latest algorithms for unconstrained minimization. (Author)", "ref_function": ["background", "method", "other"], "cite_purpose": ["background"]}, "@cite_74": {"mid": "1964015691", "abstract": "In the Newton log-barrier method, Newton steps are taken for the log-barrier function for a fixed value of the barrier parameter until a certain convergence criterion is satisfied. The barrier parameter is then decreased and the Newton process is repeated. A naive analysis indicates that Newton\u2019s method does not exhibit superlinear convergence to the minimizer of each instance of the log-barrier function until it reaches a very small neighborhood, namely within O(\u03bc2) of the minimizer, where \u03bc is the barrier parameter. By analyzing the structure of the barrier Hessian and gradient in terms of the subspace of active constraint gradients and the associated null space, we show that this neighborhood is in fact much larger \u2013O(\u03bc\u03c3) for any \u03c3\u2208(1,2] \u2013 thus explaining why reasonably fast local convergence can be attained in practice. Moreover, we show that the overall convergence rate of the Newton log-barrier algorithm is superlinear in the number of function derivative evaluations, provided that the nonlinear program is formulated with a linear objective and that the schedule for decreasing the barrier parameter is related in a certain way to the step length and convergence criteria for each Newton process.", "ref_function": ["background", "background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2962970587", "abstract": "We propose a new proximal, path-following framework for a class of---possibly non-smooth---constrained convex problems. We consider settings where the non-smooth part is endowed with a proximity operator, and the constraint set is equipped with a self-concordant barrier. Our main contribution is a new re-parametrization of the optimality condition of the barrier problem, that allows us to process the objective function with its proximal operator within a new path following scheme. In particular, our approach relies on the following two main ideas. First, we re-parameterize the optimality condition as an auxiliary problem, such that a \"good\" initial point is available. Second, we combine the proximal operator of the objective and path-following ideas to design a single phase, proximal, path-following algorithm. Our method has several advantages. First, it allows handling non-smooth objectives via proximal operators, this avoids lifting the problem dimension via slack variables and additional constraints. Second, it consists of only a single phase as compared to a two-phase algorithm in [43] In this work, we show how to overcome this difficulty in the proximal setting and prove that our scheme has the same O(\u03bd\u221alog(1 e)) worst-case iteration-complexity with standard approaches [30, 33], but our method can handle nonsmooth objectives, where \u03bd is the barrier parameter and e is a desired accuracy. Finally, our framework allows errors in the calculation of proximal-Newton search directions, without sacrificing the worst-case iteration complexity. We demonstrate the merits of our algorithm via three numerical examples, where proximal operators play a key role to improve the performance over off-the-shelf interior-point solvers.", "ref_function": ["background", "background", "objective", "method", "method", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2069671092", "abstract": "Interior methods are an omnipresent, conspicuous feature of the constrained optimization landscape today, but it was not always so. Primarily in the form of barrier methods, interior-point techniques were popular during the 1960s for solving nonlinearly constrained problems. However, their use for linear programming was not even contemplated because of the total dominance of the simplex method. Vague but continuing anxiety about barrier methods eventually led to their abandonment in favor of newly emerging, apparently more efficient alternatives such as augmented Lagrangian and sequential quadratic programming methods. By the early 1980s, barrier methods were almost without exception regarded as a closed chapter in the history of optimization. This picture changed dramatically with Karmarkar's widely publicized announcement in 1984 of a fast polynomial-time interior method for linear programming; in 1985, a formal connection was established between his method and classical barrier methods. Since then, interior methods have advanced so far, so fast, that their influence has transformed both the theory and practice of constrained optimization. This article provides a condensed, selective look at classical material and recent research about interior methods for nonlinearly constrained optimization.", "ref_function": ["background", "background", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2157959686", "abstract": "In this paper we develop a new affine-invariant primal\u2013dual subgradient method for nonsmooth convex optimization problems. This scheme is based on a self-concordant barrier for the basic feasible set. It is suitable for finding approximate solutions with certain relative accuracy. We discuss some applications of this technique including fractional covering problem, maximal concurrent flow problem, semidefinite relaxations and nonlinear online optimization. For all these problems, the rate of convergence of our method does not depend on the problem\u2019s data.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_11": {"mid": "1566327188", "abstract": "We propose an algorithmic framework for convex minimization problems of composite functions with two terms: a self-concordant part and a possibly nonsmooth regularization part. Our method is a new proximal Newton algorithm with local quadratic convergence rate. As a specific problem instance, we consider sparse precision matrix estimation problems in graph learning. Via a careful dual formulation and a novel analytic stepsize selection, we instantiate an algorithm within our framework for graph learning that avoids Cholesky decompositions and matrix inversions, making it attractive for parallel and distributed implementations.", "ref_function": ["background", "method", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["The augmented Lagrange method (ALM) @cite_32 @cite_20 @cite_18 @cite_35 is another common choice for constrained problems.", "Different from the exact or quadratic penalty method, ALM estimates the primal solution together with the dual solution.", "At each iteration, it updates the primal variable by minimizing the augmented Lagrange function and then performs a dual gradient ascent step to update the dual variable.", "The iteration complexity of ALM has been established for convex problems @cite_17 @cite_49 @cite_38 @cite_2 @cite_53 .", "For non-convex problems, most of the existing studies on ALM only show its asymptotic convergence or local convergence rate @cite_41 @cite_61 @cite_19 @cite_30 @cite_64 @cite_82 .", "The computational complexities of ALM for finding an @math -stationary point (under various notions of stationarity) are obtained only for linearly constrained problems @cite_39 @cite_12 @cite_63 @cite_4 .", "One exception is @cite_29 where they essentially assume that the smallest singular value of the Jacobian matrix of the constraint functions is uniformly bounded away from zero at all feasible points.", "In this paper, we do not require that assumption but, instead, need an initial nearly feasible solution when the constraints are non-convex while @cite_29 does not need."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: about results", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Describing the objective"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_30": {"mid": "2962853966", "abstract": "In this paper, we analyze the convergence of the alternating direction method of multipliers (ADMM) for minimizing a nonconvex and possibly nonsmooth objective function, ( (x_0, ,x_p,y) ), subject to coupled linear equality constraints. Our ADMM updates each of the primal variables (x_0, ,x_p,y ), followed by updating the dual variable. We separate the variable y from (x_i )\u2019s as it has a special role in our analysis. The developed convergence guarantee covers a variety of nonconvex functions such as piecewise linear functions, ( _q ) quasi-norm, Schatten-q quasi-norm ( (0<q<1 )), minimax concave penalty (MCP), and smoothly clipped absolute deviation penalty. It also allows nonconvex constraints such as compact manifolds (e.g., spherical, Stiefel, and Grassman manifolds) and linear complementarity constraints. Also, the (x_0 )-block can be almost any lower semi-continuous function. By applying our analysis, we show, for the first time, that several ADMM algorithms applied to solve nonconvex models in statistical learning, optimization on manifold, and matrix decomposition are guaranteed to converge. Our results provide sufficient conditions for ADMM to converge on (convex or nonconvex) monotropic programs with three or more blocks, as they are special cases of our model. ADMM has been regarded as a variant to the augmented Lagrangian method (ALM). We present a simple example to illustrate how ADMM converges but ALM diverges with bounded penalty parameter ( ). Indicated by this example and other analysis in this paper, ADMM might be a better choice than ALM for some nonconvex nonsmooth problems, because ADMM is not only easier to implement, it is also more likely to converge for the concerned scenarios.", "ref_function": ["background", "method", "method", "method", "result", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_35": {"mid": "1669104078", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_64": {"mid": "2963178962", "abstract": "The alternating direction method with multipliers (ADMM) is one of the most powerful and successful methods for solving various composite problems. The convergence of the conventional ADMM (i.e., 2-block) for convex objective functions has been stated for a long time, and its convergence for nonconvex objective functions has, however, been established very recently. The multi-block ADMM, a natural extension of ADMM, is a widely used scheme and has also been found very useful in solving various nonconvex optimization problems. It is thus expected to establish the convergence of the multi-block ADMM under nonconvex frameworks. In this paper, we first justify the convergence of 3-block Bregman ADMM. We next extend these results to the N-block case (N \u2265 3), which underlines the feasibility of multi-block ADMM applications in nonconvex settings. Finally, we present a simulation study and a real-world application to support the correctness of the obtained theoretical assertions.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_41": {"mid": "1534354577", "abstract": "For optimization problems with nonlinear constraints, linearly constrained Lagrangian (LCL) methods solve a sequence of subproblems of the form \"minimize an augmented Lagrangian function subject to linearized constraints.\" Such methods converge rapidly near a solution but may not be reliable from arbitrary starting points. Nevertheless, the well-known software package MINOS has proved effective on many large problems. Its success motivates us to derive a related LCL algorithm that possesses three important properties: it is globally convergent, the subproblem constraints are always feasible, and the subproblems may be solved inexactly. The new algorithm has been implemented in MATLAB, with an option to use either MINOS or SNOPT (Fortran codes) to solve the linearly constrained subproblems. Only first derivatives are required. We present numerical results on a subset of the COPS, HS, and CUTE test problems, which include many large examples. The results demonstrate the robustness and efficiency of the stabilized LCL procedure.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_29": {"mid": "2955184355", "abstract": "We propose a practical inexact augmented Lagrangian method (iALM) for nonconvex problems with nonlinear constraints. We characterize the total computational complexity of our method subject to a verifiable geometric condition, which is closely related to the Polyak-Lojasiewicz and Mangasarian-Fromovitz conditions. In particular, when a first-order solver is used for the inner iterates, we prove that iALM finds a first-order stationary point with @math calls to the first-order oracle. If, in addition, the problem is smooth and a second-order solver is used for the inner iterates, iALM finds a second-order stationary point with @math calls to the second-order oracle. These complexity results match the known theoretical results in the literature. We also provide strong numerical evidence on large-scale machine learning problems, including the Burer-Monteiro factorization of semidefinite programs, and a novel nonconvex relaxation of the standard basis pursuit template. For these examples, we also show how to verify our geometric condition.", "ref_function": ["background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background", "differences"]}, "@cite_2": {"mid": "2969771825", "abstract": "Augmented Lagrangian method (ALM) has been popularly used for solving constrained optimization problems. Practically, subproblems for updating primal variables in the framework of ALM usually can only be solved inexactly. The convergence and local convergence speed of ALM have been extensively studied. However, the global convergence rate of the inexact ALM is still open for problems with nonlinear inequality constraints. In this paper, we work on general convex programs with both equality and inequality constraints. For these problems, we establish the global convergence rate of the inexact ALM and estimate its iteration complexity in terms of the number of gradient evaluations to produce a primal and or primal-dual solution with a specified accuracy. We first establish an ergodic convergence rate result of the inexact ALM that uses constant penalty parameters or geometrically increasing penalty parameters. Based on the convergence rate result, we then apply Nesterov\u2019s optimal first-order method on each primal subproblem and estimate the iteration complexity of the inexact ALM. We show that if the objective is convex, then (O( ^ -1 ) ) gradient evaluations are sufficient to guarantee a primal ( )-solution in terms of both primal objective and feasibility violation. If the objective is strongly convex, the result can be improved to (O( ^ - 1 2 | |) ). To produce a primal-dual ( )-solution, more gradient evaluations are needed for convex case, and the number is (O( ^ - 4 3 ) ), while for strongly convex case, the number is still (O( ^ - 1 2 | |) ). Finally, we establish a nonergodic convergence rate result of the inexact ALM that uses geometrically increasing penalty parameters. This result is established only for the primal problem. We show that the nonergodic iteration complexity result is in the same order as that for the ergodic result. Numerical experiments on quadratically constrained quadratic programming are conducted to compare the performance of the inexact ALM with different settings.", "ref_function": ["background", "background", "background", "background", "objective", "method", "result", "background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2057624533", "abstract": "The main purpose of this paper is to suggest a method for finding the minimum of a functionf(x) subject to the constraintg(x)=0. The method consists of replacingf byF=f+\u03bbg+1 2cg2, wherec is a suitably large constant, and computing the appropriate value of the Lagrange multiplier. Only the simplest algorithm is presented. The remaining part of the paper is devoted to a survey of known methods for finding unconstrained minima, with special emphasis on the various gradient techniques that are available. This includes Newton's method and the method of conjugate gradients.", "ref_function": ["objective", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_38": {"mid": "2768546550", "abstract": "First-order methods have been popularly used for solving large-scale problems. However, many existing works only consider unconstrained problems or those with simple constraint. In this paper, we develop two first-order methods for constrained convex programs, for which the constraint set is represented by affine equations and smooth nonlinear inequalities. Both methods are based on the classic augmented Lagrangian function. They update the multipliers in the same way as the augmented Lagrangian method (ALM) but employ different primal variable updates. The first method, at each iteration, performs a single proximal gradient step to the primal variable, and the second method is a block update version of the first one. For the first method, we establish its global iterate convergence as well as global sublinear and local linear convergence, and for the second method, we show a global sublinear convergence result in expectation. Numerical experiments are carried out on the basis pursuit denoising and a convex quadratically constrained quadratic program to show the empirical performance of the proposed methods. Their numerical behaviors closely match the established theoretical results.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2135779729", "abstract": "The theory of the proximal point algorithm for maximal monotone operators is applied to three algorithms for solving convex programs, one of which has not previously been formulated. Rate-of-convergence results for the \u201cmethod of multipliers,\u201d of the strong sort already known, are derived in a generalized form relevant also to problems beyond the compass of the standard second-order conditions for oplimality. The new algorithm, the \u201cproximal method of multipliers,\u201d is shown to have much the same convergence properties, but with some potential advantages.", "ref_function": ["background", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2951136802", "abstract": "In this paper we study the worst-case complexity of an inexact Augmented Lagrangian method for nonconvex inequality-constrained problems. Assuming that the penalty parameters are bounded, we prove a complexity bound of @math iterations for the referred algorithm generate an @math -approximate KKT point, for @math . When the penalty parameters are unbounded, we prove an iteration complexity bound of @math , where @math controls the rate of increase of the penalty parameters. For linearly constrained problems, these bounds yield to evaluation complexity bounds of @math and @math , respectively, when suitable @math -order methods ( @math ) are used to approximately solve the unconstrained subproblems at each iteration of our Augmented Lagrangian scheme.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_39": {"mid": "2341508215", "abstract": "In this paper, we propose a new decomposition approach named the proximal primal dual algorithm (Prox-PDA) for smooth nonconvex linearly constrained optimization problems. The proposed approach is primal-dual based, where the primal step minimizes certain approximation of the augmented Lagrangian of the problem, and the dual step performs an approximate dual ascent. The approximation used in the primal step is able to decompose the variable blocks, making it possible to obtain simple subproblems by leveraging the problem structures. Theoretically, we show that whenever the penalty parameter in the augmented Lagrangian is larger than a given threshold, the Prox-PDA converges to the set of stationary solutions, globally and in a sublinear manner (i.e., certain measure of stationarity decreases in the rate of @math , where @math is the iteration counter). Interestingly, when applying a variant of the Prox-PDA to the problem of distributed nonconvex optimization (over a connected undirected graph), the resulting algorithm coincides with the popular EXTRA algorithm [ 2014], which is only known to work in convex cases. Our analysis implies that EXTRA and its variants converge globally sublinearly to stationary solutions of certain nonconvex distributed optimization problem. There are many possible extensions of the Prox-PDA, and we present one particular extension to certain nonconvex distributed matrix factorization problem.", "ref_function": ["background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_49": {"mid": "2785315711", "abstract": "Stochastic gradient (SG) method has been popularly applied to solve optimization problems with objective that is stochastic or an average of many functions. Most existing works on SG assume that the underlying problem is unconstrained or has an easy-to-project constraint set. In this paper, we consider problems that have a stochastic objective and also many functional constraints. For such problems, it could be extremely expensive to project a point to the feasible set, or even compute subgradient and or function value of all constraint functions. To find solutions of these problems, we propose a novel SG method based on the augmented Lagrangian function. Within every iteration, it inquires a stochastic subgradient of the objective, a subgradient and function value of one randomly sampled constraint function, and function value of another sampled constraint function. Hence, the per-iteration complexity is low. We establish its convergence rate for convex and also strongly convex problems. It can achieve the optimal @math convergence rate for convex case and nearly optimal @math rate for strongly convex case. Numerical experiments on quadratically constrained quadratic programming are conducted to demonstrate its efficiency.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2144603975", "abstract": "This paper considers a special but broad class of convex programming problems whose feasible region is a simple compact convex set intersected with the inverse image of a closed convex cone under an affine transformation. It studies the computational complexity of quadratic penalty based methods for solving the above class of problems. An iteration of these methods, which is simply an iteration of Nesterov\u2019s optimal method (or one of its variants) for approximately solving a smooth penalization subproblem, consists of one or two projections onto the simple convex set. Iteration-complexity bounds expressed in terms of the latter type of iterations are derived for two quadratic penalty based variants, namely: one which applies the quadratic penalty method directly to the original problem and another one which applies the latter method to a perturbation of the original problem obtained by adding a small quadratic term to its objective function.", "ref_function": ["background", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_19": {"mid": "1830979757", "abstract": "In this paper, we consider augmented Lagrangian AL algorithms for solving large-scale nonlinear optimization problems that execute adaptive strategies for updating the penalty parameter. Our work is motivated by the recently proposed adaptive AL trust region method by [An adaptive augmented Lagrangian method for large-scale constrained optimization, Math. Program. 152 2015, pp. 201\u2013245.]. The first focal point of this paper is a new variant of the approach that employs a line search rather than a trust region strategy, where a critical algorithmic feature for the line search strategy is the use of convexified piecewise quadratic models of the AL function for computing the search directions. We prove global convergence guarantees for our line search algorithm that are on par with those for the previously proposed trust region method. A second focal point of this paper is the practical performance of the line search and trust region algorithm variants in Matlab software, as well as that of an adaptive penalty parameter updating strategy incorporated into the Lancelot software. We test these methods on problems from the CUTEst and COPS collections, as well as on challenging test problems related to optimal power flow. Our numerical experience suggests that the adaptive algorithms outperform traditional AL methods in terms of efficiency and reliability. As with traditional AL algorithms, the adaptive methods are matrix-free and thus represent a viable option for solving large-scale problems.", "ref_function": ["background", "other", "other", "other", "other", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2619916648", "abstract": "This paper establishes the iteration-complexity of a Jacobi-type non-Euclidean proximal alternating direction method of multipliers (ADMM) for solving multi-block linearly constrained nonconvex programs. The subproblems of this ADMM variant can be solved in parallel and hence the method has great potential to solve large scale multi-block linearly constrained nonconvex programs. Moreover, our analysis allows the Lagrange multiplier to be updated with a relaxation parameter in the interval (0, 2).", "ref_function": ["background", "method", "method"], "cite_purpose": ["background"]}, "@cite_82": {"mid": "2076940249", "abstract": "We establish local convergence and rate of convergence of the classical augmented Lagrangian algorithm under the sole assumption that the dual starting point is close to a multiplier satisfying the second-order sufficient optimality condition. In particular, no constraint qualifications of any kind are needed. Previous literature on the subject required, in addition, the linear independence constraint qualification and either the strict complementarity assumption or a stronger version of the second-order sufficient condition. That said, the classical results allow the initial multiplier estimate to be far from the optimal one, at the expense of proportionally increasing the threshold value for the penalty parameters. Although our primary goal is to avoid constraint qualifications, if the stronger assumptions are introduced, then starting points far from the optimal multiplier are allowed within our analysis as well. Using only the second-order sufficient optimality condition, for penalty parameters large ...", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_61": {"mid": "1923817890", "abstract": "The alternating direction method with multipliers (ADMM) has been one of most powerful and successful methods for solving various convex or nonconvex composite problems that arise in the fields of image & signal processing and machine learning. In convex settings, numerous convergence results have been established for ADMM as well as its varieties. However, due to the absence of convexity, the convergence analysis of nonconvex ADMM is generally very difficult. In this paper we study the Bregman modification of ADMM (BADMM), which includes the conventional ADMM as a special case and often leads to an improvement of the performance of the algorithm. Under certain assumptions, we prove that the iterative sequence generated by BADMM converges to a stationary point of the associated augmented Lagrangian function. The obtained results underline the feasibility of ADMM in applications under nonconvex settings.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_53": {"mid": "2605995072", "abstract": "In this paper we present a complete iteration complexity analysis of inexact first-order Lagrangian and penalty methods for solving cone-constrained convex problems that have or may not have optimal Lagrange multipliers that close the duality gap. We first assume the existence of optimal Lagrange multipliers and study primal\u2013dual first-order methods based on inexact information and augmented Lagrangian smoothing or Nesterov-type smoothing. For inexact (fast) gradient augmented Lagrangian methods, we derive an overall computational complexity of O(1 \u03f5) projections onto a simple primal set in order to attain an e-optimal solution of the conic convex problem. For the inexact fast gradient method combined with Nesterov-type smoothing, we derive computational complexity O(1 \u03f53 2) projections onto the same set. Then, we assume that optimal Lagrange multipliers might not exist for the cone-constrained convex problem, and analyse the fast gradient method for solving penalty reformulations of the problem. For the ...", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_63": {"mid": "2592519230", "abstract": "This paper establishes convergence rate bounds for a variant of the proximal alternating direction method of multipliers (ADMM) for solving nonconvex linearly constrained optimization problems. The variant of the proximal ADMM allows the inclusion of an over-relaxation stepsize parameter belonging to the interval @math . To the best of our knowledge, all related papers in the literature only consider the case where the over-relaxation parameter lies in the interval @math .", "ref_function": ["background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["While preparing this paper, we notice two recently posted papers @cite_86 @cite_76 on the problems with non-convex constraints.", "The algorithms in both works are based on the proximal point method.", "Different from our approach, they solve subproblems with strongly convex objective and also strongly convex constraints by adding proximal terms to the objective and constraints.", "Their analysis requires the uniform boundedness of the dual solutions of all subproblems and, to ensure this requirement is satisfied, @cite_48 assume that a uniform Slater's condition holds while @cite_86 assume that the Mangasarian-Fromovitz constraint qualification holds at the limiting points of the generated iterates.", "However, neither assumptions can be easily verified.", "As pointed out in @cite_86 , their assumptions can be implied by a sufficient feasibility assumption, which is an even stronger assumption.", "On the contrary, our analysis in the non-convex constrained case does not depend on the boundness of the dual variables, and thus does not need the aforementioned assumptions by @cite_86 @cite_76 ."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about result", "Describing used methods"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_86": {"mid": "2964862314", "abstract": "Nonconvex optimization is becoming more and more important in machine learning and operations research. In spite of recent progresses, the development of provably efficient algorithm for optimization with nonconvex functional constraints remains open. Such problems have potential applications in risk-averse machine learning, semisupervised learning and robust optimization among others. In this paper, we introduce a new proximal point type method for solving this important class of nonconvex problems by transforming them into a sequence of convex constrained subproblems. We establish the convergence and rate of convergence of this algorithm to the KKT point under different types of constraint qualifications. In particular, we prove that our algorithm will converge to an @math -KKT point in @math iterations under a properly defined condition. For practical use, we present inexact variants of this approach, in which approximate solutions of the subproblems are computed by either primal or primal-dual type algorithms, and establish their associated rate of convergence. To the best of our knowledge, this is the first time that proximal point type method is developed for nonlinear programing with nonconvex functional constraints, and most of the convergence and complexity results seem to be new in the literature.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["uses", "background", "background", "differences"]}, "@cite_76": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["uses", "differences"]}, "@cite_48": {"mid": "2964772384", "abstract": "Optimization models with non-convex constraints arise in many tasks in machine learning, e.g., learning with fairness constraints or Neyman-Pearson classification with non-convex loss. Although many efficient methods have been developed with theoretical convergence guarantees for non-convex unconstrained problems, it remains a challenge to design provably efficient algorithms for problems with non-convex functional constraints. This paper proposes a class of subgradient methods for constrained optimization where the objective function and the constraint functions are are weakly convex. Our methods solve a sequence of strongly convex subproblems, where a proximal term is added to both the objective function and each constraint function. Each subproblem can be solved by various algorithms for strongly convex optimization. Under a uniform Slater's condition, we establish the computation complexities of our methods for finding a nearly stationary point.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In addition to the methods above, algorithms that utilize Hessian information have been developed to find the second-order @math -stationary point of linearly constrained smooth non-convex optimization @cite_87 @cite_71 @cite_15 .", "Different from these works, we focus on finding an approximate first-order stationary point for nonlinear constrained non-convex optimization using only gradient information."], "label": ["General reference to previous research or scholarship: approaches taken", "Describing used methods"], "target_paper": "Non-convex optimization problems arise from various areas in science and engineering. Although many numerical methods and theories have been developed for unconstrained non-convex problems, the parallel development for constrained non-convex problems remains limited. That restricts the practices of mathematical modeling and quantitative decision making in many disciplines. In this paper, an inexact proximal-point penalty method is proposed for constrained optimization problems where both the objective function and the constraint can be non-convex. The proposed method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-type method. The theoretical property of the proposed method is analyzed in two different cases. In the first case, the objective function is non-convex but the constraint functions are assumed to be convex, while in the second case, both the objective function and the constraint are non-convex. For both cases, we give the complexity results in terms of the number of function value and gradient evaluations to produce near-stationary points. Due to the different structures, different definitions of near-stationary points are given for the two cases. The complexity for producing a nearly @math -stationary point is @math for the first case while it becomes @math for the second case.", "reference": {"@cite_15": {"mid": "2959708829", "abstract": "This paper proposes low-complexity algorithms for finding approximate second-order stationary points (SOSPs) of problems with smooth non-convex objective and linear constraints. While finding (approximate) SOSPs is computationally intractable, we first show that generic instances of the problem can be solved efficiently. More specifically, for a generic problem instance, certain strict complementarity (SC) condition holds for all Karush-Kuhn-Tucker (KKT) solutions (with probability one). The SC condition is then used to establish an equivalence relationship between two different notions of SOSPs, one of which is computationally easy to verify. Based on this particular notion of SOSP, we design an algorithm named the Successive Negative-curvature grAdient Projection (SNAP), which successively performs either conventional gradient projection or some negative curvature based projection steps to find SOSPs. SNAP and its first-order extension SNAP @math , require @math iterations to compute an @math -SOSP, and their per-iteration computational complexities are polynomial in the number of constraints and problem dimension. To our knowledge, this is the first time that first-order algorithms with polynomial per-iteration complexity and global sublinear rate have been designed to find SOSPs of the important class of non-convex problems with linear constraints.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_71": {"mid": "2895571900", "abstract": "We consider the problem of finding an approximate second-order stationary point of a constrained non-convex optimization problem. We first show that, unlike the unconstrained scenario, the vanilla projected gradient descent algorithm may converge to a strict saddle point even when there is only a single linear constraint. We then provide a hardness result by showing that checking ( , )-second order stationarity is NP-hard even in the presence of linear constraints. Despite our hardness result, we identify instances of the problem for which checking second order stationarity can be done efficiently. For such instances, we propose a dynamic second order Frank--Wolfe algorithm which converges to ( , )-second order stationary points in O ( ^ -2 , ^ -3 ) iterations. The proposed algorithm can be used in general constrained non-convex optimization as long as the constrained quadratic sub-problem can be solved efficiently.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_87": {"mid": "2788706426", "abstract": "In this work, we study two first-order primal-dual based algorithms, the Gradient Primal-Dual Algorithm (GPDA) and the Gradient Alternating Direction Method of Multipliers (GADMM), for solving a class of linearly constrained non-convex optimization problems. We show that with random initialization of the primal and dual variables, both algorithms are able to compute second-order stationary solutions (ss2) with probability one. This is the first result showing that primal-dual algorithm is capable of finding ss2 when only using first-order information, it also extends the existing results for first-order, but primal-only algorithms. An important implication of our result is that it also gives rise to the first global convergence result to the ss2, for two classes of unconstrained distributed non-convex learning problems over multi-agent networks.", "ref_function": ["background", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Semantic parsing-based approaches translate questions into formal queries using bottom up parsing @cite_5 or staged query graph generation @cite_12 .", "gAnswer @cite_15 @cite_1 builds up semantic query graph for question analysis and utilize subgraph matching for disambiguation.", "Recent studies combine parsing based approaches with neural networks, to enhance the ability for structure disambiguation.", "ConstraintQG ( ConstraintQG ), CQAEMNLP ( CQAEMNLP ) and SQG ( SQG ) build query graphs by staged query generation, and follow an encode-and-compare framework to rank candidate queries with neural networks.", "These approaches try to learn entire representations for questions with different query structures by using a single network.", "Thus, they may suffer from the lack of training data, especially for questions with rarely appeared structures.", "By contrast, our approach utilizes multiple networks to learn predictors for different query substructures, which can gain a stable performance with limited training data.", "Also, our approach does not require manually-written rules, and performs stably with noisy linking results."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "General descriptions of the topic", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Describing used methods", "Describing the results"], "target_paper": "Formal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for complex questions. Also, it achieves promising performance with limited training data and noisy entity relation linking results.", "reference": {"@cite_5": {"mid": "2252136820", "abstract": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.", "ref_function": ["background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2011992920", "abstract": "RDF question answering (Q A) allows users to ask questions in natural languages over a knowledge base represented by RDF. To answer a national language question, the existing work takes a two-stage approach: question understanding and query evaluation. Their focus is on question understanding to deal with the disambiguation of the natural language phrases. The most common technique is the joint disambiguation, which has the exponential search space. In this paper, we propose a systematic framework to answer natural language questions over RDF repository (RDF Q A) from a graph data-driven perspective. We propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, RDF Q A is reduced to subgraph matching problem. More importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found. The cost of disambiguation is saved if there are no matching found. We compare our method with some state-of-the-art RDF Q A systems in the benchmark dataset. Extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly.", "ref_function": ["background", "background", "objective", "method", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2766317792", "abstract": "RDF question answering (Q A) allows users to ask questions in natural languages over a knowledge base represented by RDF. To answer a natural language question, the existing work takes a two-stage approach: question understanding and query evaluation. Their focus is on question understanding to deal with the disambiguation of the natural language phrases. The most common technique is the joint disambiguation, which has the exponential search space. In this paper, we propose a systematic framework to answer natural language questions over RDF repository (RDF Q A) from a graph data-driven perspective. We propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, RDF Q A is reduced to subgraph matching problem. More importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found. The cost of disambiguation is saved if there are no matching found. More specifically, we propose two different frameworks to build the semantic query graph, one is relation (edge)-first and the other one is node-first. We compare our method with some state-of-the-art RDF Q A systems in the benchmark dataset. Extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly.", "ref_function": ["background", "background", "objective", "method", "objective", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2251079237", "abstract": "We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simplifies the semantic matching problem. By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outperforms previous methods substantially, and achieves an F1 measure of 52.5 on the WEBQUESTIONS dataset.", "ref_function": ["objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Dictionary learning has been widely used in computer vision to obtain basic components and sparse representations of images @cite_7 .", "Recently, in order to optimize the learned dictionary for a specific task, people proposed supervised dictionary learning @cite_1 .", "Some methods learn discriminative dictionaries for different classes @cite_8 @cite_14 , or use label information to prune the learned dictionary by unsupervised dictionary learning @cite_5 .", "They actually separate the dictionary learning from the supervised learning part and may lead to inferior results.", "Another group of methods combine dictionary learning and supervised learning @cite_1 @cite_11 , but fail to consider the spatial temporal property for specific problems.", "Hence, we propose to do dictionary learning and supervised learning iteratively, and spatial and temporal regularization are added to improve the interpretation of results."], "label": ["General descriptions of the topic", "General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Describing used methods"], "target_paper": "In the face of growing needs for water and energy, a fundamental understanding of the environmental impacts of human activities becomes critical for managing water and energy resources, remedying water pollution, and making regulatory policy wisely. Among activities that impact the environment, oil and gas production, wastewater transport, and urbanization are included. In addition to the occurrence of anthropogenic contamination, the presence of some contaminants (e.g., methane, salt, and sulfate) of natural origin is not uncommon. Therefore, scientists sometimes find it difficult to identify the sources of contaminants in the coupled natural and human systems. In this paper, we propose a technique to simultaneously conduct source detection and prediction, which outperforms other approaches in the interdisciplinary case study of the identification of potential groundwater contamination within a region of high-density shale gas development.", "reference": {"@cite_14": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2536599074", "abstract": "We propose in this paper to unify two different approaches to image restoration: On the one hand, learning a basis set (dictionary) adapted to sparse signal descriptions has proven to be very effective in image reconstruction and classification tasks. On the other hand, explicitly exploiting the self-similarities of natural images has led to the successful non-local means approach to image restoration. We propose simultaneous sparse coding as a framework for combining these two approaches in a natural manner. This is achieved by jointly decomposing groups of similar signals on subsets of the learned dictionary. Experimental results in image denoising and demosaicking tasks with synthetic and real noise show that the proposed method outperforms the state of the art, making it possible to effectively restore raw images from digital cameras at a reasonable speed and memory cost.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2032768707", "abstract": "Face recognition (FR) is an active yet challenging topic in computer vision applications. As a powerful tool to represent high dimensional data, recently sparse representation based classification (SRC) has been successfully used for FR. This paper discusses the metaface learning (MFL) of face images under the framework of SRC. Although directly using the training samples as dictionary bases can achieve good FR performance, a well learned dictionary matrix can lead to higher FR rate with less dictionary atoms. An SRC oriented unsupervised MFL algorithm is proposed in this paper and the experimental results on benchmark face databases demonstrated the improvements brought by the proposed MFL algorithm over original SRC.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2128638419", "abstract": "It is now well established that sparse signal models are well suited for restoration tasks and can be effectively learned from audio, image, and video data. Recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones. This paper proposes a new step in that direction, with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and discriminative class models. The linear version of the proposed model admits a simple probabilistic interpretation, while its most general variant admits an interpretation in terms of kernels. An optimization framework for learning all the components of the proposed model is presented, along with experimental results on standard handwritten digit and texture classification tasks.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background", "motivation", "background"]}, "@cite_5": {"mid": "2151768982", "abstract": "We present an approach to determine the category and location of objects in images. It performs very fast categorization of each pixel in an image, a brute-force approach made feasible by three key developments: First, our method reduces the size of a large generic dictionary (on the order of ten thousand words) to the low hundreds while increasing classification performance compared to k-means. This is achieved by creating a discriminative dictionary tailored to the task by following the information bottleneck principle. Second, we perform feature-based categorization efficiently on a dense grid by extending the concept of integral images to the computation of local histograms. Third, we compute SIFT descriptors densely in linear time. We compare our method to the state of the art and find that it excels in accuracy and simplicity, performing better while assuming less.", "ref_function": ["objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["motivation", "background"]}}}
{"sentences": ["Temporal alignment is a necessary pre-processing step for most dynamic 3D reconstruction methods.", "Current video synchronization or image sequencing @cite_42 @cite_44 @cite_27 @cite_18 @cite_29 @cite_19 rely on the image 2D features, foregoing the recovery of the 3D structure.", "Feature-based sequencing methods like @cite_42 @cite_27 @cite_16 make different assumptions on the underlying imaging geometry.", "For example, while @cite_42 favors an approximately static imaging geometry, @cite_27 prefers viewing configurations with large baselines.", "@cite_44 overcomes the limitation of static cameras and improves accuracy by leveraging the temporal info of frames in individual cameras.", "@cite_18 determines spatio-temporal alignment among a partially order set of observation by framing the problem as mapping of @math observations into a single line in @math , which explicitly imposes a total ordering.", "Unlike previous methods, @cite_20 propose a synchronization algorithm without tracking corresponding feature between video sequences.", "Instead, they synchronize two videos by the relative motion between two rigid objects.", "@cite_25 determined sequencing based on the approximate 3D intersections of viewing rays under an affine reference frame.", "@cite_30 jointly synchronize a pair of video sequences and reconstruct their commonly observed dense 3D structure by maximizing the spatio-temporal consistency of two-view pixel correspondences across video sequences."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: about results", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "We present a general paradigm for dynamic 3D reconstruction from multiple independent and uncontrolled image sources having arbitrary temporal sampling density and distribution. Our graph-theoretic formulation models the Spatio-temporal relationships among our observations in terms of the joint estimation of their 3D geometry and its discrete Laplace operator. Towards this end, we define a tri-convex optimization framework that leverages the geometric properties and dependencies found among a Euclideanshape-space and the discrete Laplace operator describing its local and global topology. We present a reconstructability analysis, experiments on motion capture data and multi-view image datasets, as well as explore applications to geometry-based event segmentation and data association.", "reference": {"@cite_30": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2167747244", "abstract": "In this paper, we consider the problem of estimating the spatiotemporal alignment between N unsynchronized video sequences of the same dynamic 3D scene, captured from distinct viewpoints. Unlike most existing methods, which work for N = 2 and rely on a computationally intensive search in the space of temporal alignments, we present a novel approach that reduces the problem for general N to the robust estimation of a single line in RN. This line captures all temporal relations between the sequences and can be computed without any prior knowledge of these relations. Considering that the spatial alignment is captured by the parameters of fundamental matrices, an iterative algorithm is used to refine simultaneously the parameters representing the temporal and spatial relations between the sequences. Experimental results with real-world and synthetic sequences show that our method can accurately align the videos even when they have large misalignments (e.g., hundreds of frames), when the problem is seemingly ambiguous (e.g., scenes with roughly periodic motion), and when accurate manual alignment is difficult (e.g., due to slow-moving objects).", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_29": {"mid": "1886695513", "abstract": "We present a novel algorithm for temporally synchronizing multiple videos capturing the same dynamic scene. Our algorithm relies on general image features and it does not require explicitly tracking any specific object, making it applicable to general scenes with complex motion. This is facilitated by our new trajectory filtering and matching schemes that correctly identifies matching pairs of trajectories (inliers) from a large set of potential candidate matches, of which many are outliers. We find globally optimal synchronization parameters by using a stable RANSAC-based optimization approach. For multi-video synchronization, the algorithm identifies an informative subset of video pairs which prevents the RANSAC algorithm from being biased by outliers. Experiments on two-camera and multi-camera synchronization demonstrate the performance of our algorithm.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_42": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background", "background"]}, "@cite_44": {"mid": "2152136819", "abstract": "Photo-sequencing is the problem of recovering the temporal order of a set of still images of a dynamic event, taken asynchronously by a set of uncalibrated cameras. Solving this problem is a first, crucial step for analyzing (or visualizing) the dynamic content of the scene captured by a large number of freely moving spectators. We propose a geometric based solution, followed by rank aggregation to the photo-sequencing problem. Our algorithm trades spatial certainty for temporal certainty. Whereas the previous solution proposed by [4] relies on two images taken from the same static camera to eliminate uncertainty in space, we drop the static-camera assumption and replace it with temporal information available from images taken from the same (moving) camera. Our method thus overcomes the limitation of the static-camera assumption, and scales much better with the duration of the event and the spread of cameras in space. We present successful results on challenging real data sets and large scale synthetic data (250 images).", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_19": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_27": {"mid": "1901796002", "abstract": "We present an algorithm that synchronizes two short video sequences where an object undergoes ballistic motion against stationary scene points. The object\u2019s motion and epipolar geometry are exploited to guide the algorithm to the correct synchronization in an iterative manner. Our algorithm accurately synchronizes videos recorded at different frame rates, and takes few iterations to converge to sub-frame accuracy. We use synthetic data to analyze our algorithm\u2019s accuracy under the influence of noise. We demonstrate that it accurately synchronizes real video sequences, and evaluate its performance against manual synchronization.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background", "background", "background"]}, "@cite_16": {"mid": "2126784706", "abstract": "This paper presents a method of synchronizing video sequences that exploits the non-rigidity of sets of 3D point features (e.g., anatomical joint locations) within the scene. The theory is developed for homography, perspective and affine projection models within a unified rank constraint framework that is computationally cheap. An efficient method is then presented that recovers potential frame correspondences, estimates possible synchronization parameters via the Hough transform and refines these parameters using non-linear optimization methods in order to recover synchronization to sub-frame accuracy, even for sequences of unknown and different frame rates. The method is evaluated quantitatively using synthetic data and demonstrated qualitatively on several real sequences.", "ref_function": ["method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2097446893", "abstract": "We present a novel method for automatically synchronizing two video sequences of the same event. Unlike previously proposed methods, we do not put any restrictive constraints on the scene nor on the camera motions: our method can deal with independently moving cameras, wide baseline conditions, and general 3D scenes. It starts from five point correspondences throughout the video sequences, that are provided using wide baseline matching and tracking techniques. It is efficient, in that it can be implemented in a non-combinatorial way. The feasibility of the method is demonstrated by preliminary experimental results.", "ref_function": ["method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "1850003624", "abstract": "In this work, a method that synchronizes two video sequences is proposed. Unlike previous methods, which require the existence of correspondences between features tracked in the two sequences, and or that the cameras are static or jointly moving, the proposed approach does not impose any of these constraints. It works when the cameras move independently, even if different features are tracked in the two sequences. The assumptions underlying the proposed strategy are that the intrinsic parameters of the cameras are known and that two rigid objects, with independent motions on the scene, are visible in both sequences. The relative motion between these objects is used as clue for the synchronization. The extrinsic parameters of the cameras are assumed to be unknown. A new synchronization algorithm for static or jointly moving cameras that see (possibly) different parts of a common rigidly moving object is also proposed. Proof-of-concept experiments that illustrate the performance of these methods are presented, as well as a comparison with a state-of-the-art approach.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["@cite_33 present a membership inference attack in which they infer the presence of an individual's genotype within a complex genomic DNA mixture.", "@cite_2 improve on the attack using correlation statistics of just a few hundreds SNPs, while @cite_16 rely on regression coefficients.", "Shringarpure and Bustamante @cite_30 perform membership inference against the Beacon network.", "Beacons are web servers that answer questions e.g.", "does your dataset include a genome that has a specific nucleotide at a specific genomic coordinate?''", "to which the Beacon responds yes or no, without referring to a specific individual; see: https: github.com ga4gh-beacon specification .", "They use a likelihood-ratio test to predict whether an individual is present in the Beacon, detecting membership within a Beacon with 1,000 individuals using 5,000 queries.", "Also, Von @cite_9 reduce the number of queries to less than 0.5 best performing attack uses a high-order Markov chain to model the SNP correlations, as described in @cite_11 .", "Note that, as part of the attacks described in this paper, we use inference methods from @cite_11 as our baseline inference methods."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Other functional sentences", "Other functional sentences", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Describing used methods"], "target_paper": "Due to its hereditary nature, genomic data is not only linked to its owner but to that of close relatives as well. As a result, its sensitivity does not really degrade over time; in fact, the relevance of a genomic sequence is likely to be longer than the security provided by encryption. This prompts the need for specialized techniques providing long-term security for genomic data, yet the only available tool for this purpose is GenoGuard (, 2015). By relying on Honey Encryption, GenoGuard is secure against an adversary that can brute force all possible keys; i.e., whenever an attacker tries to decrypt using an incorrect password, she will obtain an incorrect but plausible looking decoy sequence. In this paper, we set to analyze the real-world security guarantees provided by GenoGuard; specifically, assess how much more information does access to a ciphertext encrypted using GenoGuard yield, compared to one that was not. Overall, we find that, if the adversary has access to side information in the form of partial information from the target sequence, the use of GenoGuard does appreciably increase her power in determining the rest of the sequence. We show that, in the case of a sequence encrypted using an easily guessable (low-entropy) password, the adversary is able to rule out most decoy sequences, and obtain the target sequence with just 2.5 of it available as side information. In the case of a harder-to-guess (high-entropy) password, we show that the adversary still obtains, on average, better accuracy in guessing the rest of the target sequences than using state-of-the-art genomic sequence inference methods, obtaining up to 15 improvement in accuracy.", "reference": {"@cite_30": {"mid": "1838635991", "abstract": "The human genetics community needs robust protocols that enable secure sharing of genomic data from participants in genetic research. Beacons are web servers that answer allele-presence queries\u2014such as \u201cDo you have a genome that has a specific nucleotide (e.g., A) at a specific genomic position (e.g., position 11,272 on chromosome 1)?\u201d\u2014with either \u201cyes\u201d or \u201cno.\u201d Here, we show that individuals in a beacon are susceptible to re-identification even if the only data shared include presence or absence information about alleles in a beacon. Specifically, we propose a likelihood-ratio test of whether a given individual is present in a given genetic beacon. Our test is not dependent on allele frequencies and is the most powerful test for a specified false-positive rate. Through simulations, we showed that in a beacon with 1,000 individuals, re-identification is possible with just 5,000 queries. Relatives can also be identified in the beacon. Re-identification is possible even in the presence of sequencing errors and variant-calling differences. In a beacon constructed with 65 European individuals from the 1000 Genomes Project, we demonstrated that it is possible to detect membership in the beacon with just 250 SNPs. With just 1,000 SNP queries, we were able to detect the presence of an individual genome from the Personal Genome Project in an existing beacon. Our results show that beacons can disclose membership and implied phenotypic information about participants and do not protect privacy a priori. We discuss risk mitigation through policies and standards such as not allowing anonymous pings of genetic beacons and requiring minimum beacon sizes.", "ref_function": ["background", "background", "method", "method", "result", "background", "background", "background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2040228409", "abstract": "We use high-density single nucleotide polymorphism (SNP) genotyping microarrays to demonstrate the ability to accurately and robustly determine whether individuals are in a complex genomic DNA mixture. We first develop a theoretical framework for detecting an individual's presence within a mixture, then show, through simulations, the limits associated with our method, and finally demonstrate experimentally the identification of the presence of genomic DNA of specific individuals within a series of highly complex genomic mixtures, including mixtures where an individual contributes less than 0.1 of the total genomic DNA. These findings shift the perceived utility of SNPs for identifying individual trace contributors within a forensics mixture, and suggest future research efforts into assessing the viability of previously sub-optimal DNA sources due to sample contamination. These findings also suggest that composite statistics across cohorts, such as allele frequency or genotype counts, do not mask identity within genome-wide association studies. The implications of these findings are discussed.", "ref_function": ["background", "method", "result", "result", "other"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2952306472", "abstract": "Genomic datasets are often associated with sensitive phenotypes. Therefore, the leak of membership information is a major privacy risk. Genomic beacons aim to provide a secure, easy to implement, and standardized interface for data sharing by only allowing yes no queries on the presence of specific alleles in the dataset. Previously deemed secure against re-identification attacks, beacons were shown to be vulnerable despite their stringent policy. Recent studies have demonstrated that it is possible to determine whether the victim is in the dataset, by repeatedly querying the beacon for his her single nucleotide polymorphisms (SNPs). In this work, we propose a novel re-identification attack and show that the privacy risk is more serious than previously thought. Using the proposed attack, even if the victim systematically hides informative SNPs (i.e., SNPs with very low minor allele frequency -MAF-), it is possible to infer the alleles at positions of interest as well as the beacon query results with very high confidence. Our method is based on the fact that alleles at different loci are not necessarily independent. We use the linkage disequilibrium and a high-order Markov chain-based algorithm for the inference. We show that in a simulated beacon with 65 individuals from the CEU population, we can infer membership of individuals with 95 confidence with only 5 queries, even when SNPs with MAF less than 0.05 are hidden. This means, we need less than 0.5 of the number of queries that existing works require, to determine beacon membership under the same conditions. We further show that countermeasures such as hiding certain parts of the genome or setting a query budget for the user would fail to protect the privacy of the participants under our adversary model.", "ref_function": ["background", "background", "objective", "background", "method", "result", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2141481372", "abstract": "Genome-wide association studies (GWAS) aim at discovering the association between genetic variations, particularly single-nucleotide polymorphism (SNP), and common diseases, which is well recognized to be one of the most important and active areas in biomedical research. Also renowned is the privacy implication of such studies, which has been brought into the limelight by the recent attack proposed by Homer's attack demonstrates that it is possible to identify a GWAS participant from the allele frequencies of a large number of SNPs. Such a threat, unfortunately, was found in our research to be significantly understated. In this paper, we show that individuals can actually be identified from even a relatively small set of statistics, as those routinely published in GWAS papers. We present two attacks. The first one extends Homer's attack with a much more powerful test statistic, based on the correlations among different SNPs described by coefficient of determination (r2). This attack can determine the presence of an individual from the statistics related to a couple of hundred SNPs. The second attack can lead to complete disclosure of hundreds of participants' SNPs, through analyzing the information derived from published statistics. We also found that those attacks can succeed even when the precisions of the statistics are low and part of data is missing. We evaluated our attacks on the real human genomes and concluded that such threats are completely realistic.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2134493890", "abstract": "Recent advances in genome-scale, system-level measurements of quantitative phenotypes (transcriptome, metabolome, and proteome) promise to yield unprecedented biological insights. In this environment, broad dissemination of results from genome-wide association studies (GWASs) or deep-sequencing efforts is highly desirable. However, summary results from case-control studies (allele frequencies) have been withdrawn from public access because it has been shown that they can be used for inferring participation in a study if the individual's genotype is available. A natural question that follows is how much private information is contained in summary results from quantitative trait GWAS such as regression coefficients or p values. We show that regression coefficients for many SNPs can reveal the person's participation and for participants his or her phenotype with high accuracy. Our power calculations show that regression coefficients contain as much information on individuals as allele frequencies do, if the person's phenotype is rather extreme or if multiple phenotypes are available as has been increasingly facilitated by the use of multiple-omics data sets. These findings emphasize the need to devise a mechanism that allows data sharing that will facilitate scientific progress without sacrificing privacy protection.", "ref_function": ["background", "background", "background", "background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "1526729971", "abstract": "As genomic data becomes widely used, the problem of genomic data privacy becomes a hot interdisciplinary research topic among geneticists, bioinformaticians and security and privacy experts. Practical attacks have been identified on genomic data, and thus break the privacy expectations of individuals who contribute their genomic data to medical research, or simply share their data online. Frustrating as it is, the problem could become even worse. Existing genomic privacy breaches rely on low-order SNV (Single Nucleotide Variant) correlations. Our work shows that far more powerful attacks can be designed if high-order correlations are utilized. We corroborate this concern by making use of different SNV correlations based on various genomic data models and applying them to an inference attack on individuals' genotype data with hidden SNVs. We also show that low-order models behave very differently from real genomic data and therefore should not be relied upon for privacy-preserving solutions.", "ref_function": ["background", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background", "uses"]}}}
{"sentences": ["Progress in genomics research is dependent on collaboration and data sharing among different institutions.", "Given the sensitive nature of the data, as well as regulatory and ethics constraints, this often proves to be a challenging task.", "@cite_4 propose the use of secret sharing to distribute data among several entities and, using secure multi-party computations, support privacy-friendly computations across multiple entities.", "@cite_3 present GENSETS, a genome-wide, privacy-preserving similar patients querying system using genomic edit distance approximation and private set difference protocols.", "Then, @cite_21 use Software Guard Extensions (SGX) to build a privacy-preserving international collaboration tool; this enables secure and distributed computations over encrypted data, thus supporting the analysis of rare disease genetic data across different continents.", "Finally, Oprisanu and De Cristofaro @cite_14 present a framework ( AnoniMME'') geared supporting anonymous queries within the Matchmaker Exchange platform, which allows researchers to perform queries for rare genetic disease discovery over multiple federated databases."], "label": ["General descriptions of the topic", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Due to its hereditary nature, genomic data is not only linked to its owner but to that of close relatives as well. As a result, its sensitivity does not really degrade over time; in fact, the relevance of a genomic sequence is likely to be longer than the security provided by encryption. This prompts the need for specialized techniques providing long-term security for genomic data, yet the only available tool for this purpose is GenoGuard (, 2015). By relying on Honey Encryption, GenoGuard is secure against an adversary that can brute force all possible keys; i.e., whenever an attacker tries to decrypt using an incorrect password, she will obtain an incorrect but plausible looking decoy sequence. In this paper, we set to analyze the real-world security guarantees provided by GenoGuard; specifically, assess how much more information does access to a ciphertext encrypted using GenoGuard yield, compared to one that was not. Overall, we find that, if the adversary has access to side information in the form of partial information from the target sequence, the use of GenoGuard does appreciably increase her power in determining the rest of the sequence. We show that, in the case of a sequence encrypted using an easily guessable (low-entropy) password, the adversary is able to rule out most decoy sequences, and obtain the target sequence with just 2.5 of it available as side information. In the case of a harder-to-guess (high-entropy) password, we show that the adversary still obtains, on average, better accuracy in guessing the rest of the target sequences than using state-of-the-art genomic sequence inference methods, obtaining up to 15 improvement in accuracy.", "reference": {"@cite_14": {"mid": "2951951981", "abstract": "Motivation: Advances in genome sequencing and genomics research are bringing us closer to a new era of personalized medicine, where healthcare can be tailored to the individual\u2019s genetic makeup, and to more effective diagnosis and treatment of rare genetic diseases. Much of this progress depends on collaborations and access to genomes, and thus a number of initiatives have been introduced to support seamless data sharing. Among these, the Global Alliance for Genomics and Health runs a popular platform, called Matchmaker Exchange, which allows researchers to perform queries for rare genetic disease discovery over multiple federated databases. Queries include gene variations which are linked to rare diseases, and the ability to find other researchers that have seen or have interest in those variations is extremely valuable. Nonetheless, in some cases, researchers may be reluctant to use the platform since the queries they make (thus, what they are working on) are revealed to other researchers, and this creates concerns with privacy and competitive advantage. Contributions: We present AnoniMME, a novel framework geared to enable anonymous queries within the Matchmaker Exchange platform. The framework, building on a cryptographic primitive called Reverse Private Information Retrieval (PIR), let researchers anonymously query the federated platform, in a multi-server setting. Specifically, they write their query, along with a public encryption key, anonymously in a public database. Responses are also supported, so that other researchers can respond to queries by providing their encrypted contact details. Availability and Implementation: https: github.com bristena-op AnoniMME.", "ref_function": ["background", "background", "background", "background", "background", "objective", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2568218703", "abstract": "We introduce PRINCESS, a privacy-preserving international collaboration framework for analyzing rare disease genetic data that are distributed across different continents. PRINCESS leverages Software Guard Extensions (SGX) and hardware for trustworthy computation. Unlike a traditional international collaboration model, where individual-level patient DNA are physically centralized at a single site, PRINCESS performs a secure and distributed computation over encrypted data, fulfilling institutional policies and regulations for protected health information. To demonstrate PRINCESS' performance and feasibility, we conducted a family-based allelic association study for Kawasaki Disease, with data hosted in three different continents. The experimental results show that PRINCESS provides secure and accurate analyses much faster than alternative solutions, such as homomorphic encryption and garbled circuits (over 40 000\u00d7 faster). https: github.com achenfengb PRINCESS_opensource. shw070@ucsd.edu. Supplementary data are available at Bioinformatics online.", "ref_function": ["background", "background", "method", "method", "result", "other", "other", "other"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2166106997", "abstract": "Motivation: Increased availability of various genotyping techniques has initiated a race for finding genetic markers that can be used in diagnostics and personalized medicine. Although many genetic risk factors are known, key causes of common diseases with complex heritage patterns are still unknown. Identification of such complex traits requires a targeted study over a large collection of data. Ideally, such studies bring together data from many biobanks. However, data aggregation on such a large scale raises many privacy issues. Results: We show how to conduct such studies without violating privacy of individual donors and without leaking the data to third parties. The presented solution has provable security guarantees. Contact: [email protected] Supplementary information: Supplementary data are available at Bioinformatics online.", "ref_function": ["background", "background", "background", "background", "background", "result", "result", "other"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["Another line of work focuses on protecting privacy in the context of personal genomic testing, i.e., computational tests run on sequenced genomes to assess, e.g., genetic susceptibility to diseases, determining the best course of treatment, etc.", "@cite_31 assume that each individual keeps a copy of their data and consents to tests done in such a way that only the outcome is disclosed.", "They present a few cryptographic protocols allowing researchers to privately search mutations in specific genes.", "@cite_34 rely on a semi-trusted party to store an encrypted copy of the individual's genomic data: using additively homomorphic encryption and proxy re-encryption, they allow a Medical Center to privately perform disease susceptibility tests on patients' SNPs.", "@cite_6 introduce a new cryptographic primitive called Controlled Functional Encryption (CFE), which allows users to learn only certain functions of the (encrypted) data, using keys obtained from an authority; however, the client is required to send a fresh key request to the authority every time they want to evaluate a function on a ciphertext.", "Overall, for an overview of privacy-enhancing technologies applied to genetic testing, we refer the reader to @cite_13 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Signalling Transition"], "target_paper": "Due to its hereditary nature, genomic data is not only linked to its owner but to that of close relatives as well. As a result, its sensitivity does not really degrade over time; in fact, the relevance of a genomic sequence is likely to be longer than the security provided by encryption. This prompts the need for specialized techniques providing long-term security for genomic data, yet the only available tool for this purpose is GenoGuard (, 2015). By relying on Honey Encryption, GenoGuard is secure against an adversary that can brute force all possible keys; i.e., whenever an attacker tries to decrypt using an incorrect password, she will obtain an incorrect but plausible looking decoy sequence. In this paper, we set to analyze the real-world security guarantees provided by GenoGuard; specifically, assess how much more information does access to a ciphertext encrypted using GenoGuard yield, compared to one that was not. Overall, we find that, if the adversary has access to side information in the form of partial information from the target sequence, the use of GenoGuard does appreciably increase her power in determining the rest of the sequence. We show that, in the case of a sequence encrypted using an easily guessable (low-entropy) password, the adversary is able to rule out most decoy sequences, and obtain the target sequence with just 2.5 of it available as side information. In the case of a harder-to-guess (high-entropy) password, we show that the adversary still obtains, on average, better accuracy in guessing the rest of the target sequences than using state-of-the-art genomic sequence inference methods, obtaining up to 15 improvement in accuracy.", "reference": {"@cite_31": {"mid": "2087135382", "abstract": "Recent advances in DNA sequencing technologies have put ubiquitous availability of fully sequenced human genomes within reach. It is no longer hard to imagine the day when everyone will have the means to obtain and store one's own DNA sequence. Widespread and affordable availability of fully sequenced genomes immediately opens up important opportunities in a number of health-related fields. In particular, common genomic applications and tests performed in vitro today will soon be conducted computationally, using digitized genomes. New applications will be developed as genome-enabled medicine becomes increasingly preventive and personalized. However, this progress also prompts significant privacy challenges associated with potential loss, theft, or misuse of genomic data. In this paper, we begin to address genomic privacy by focusing on three important applications: Paternity Tests, Personalized Medicine, and Genetic Compatibility Tests. After carefully analyzing these applications and their privacy requirements, we propose a set of efficient techniques based on private set operations. This allows us to implement in in silico some operations that are currently performed via in vitro methods, in a secure fashion. Experimental results demonstrate that proposed techniques are both feasible and practical today.", "ref_function": ["background", "background", "background", "background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "2133711597", "abstract": "In this paper, we propose privacy-enhancing technologies for medical tests and personalized medicine methods that use patients' genomic data. Focusing on genetic disease-susceptibility tests, we develop a new architecture (between the patient and the medical unit) and propose a \"privacy-preserving disease susceptibility test\" (PDS) by using homomorphic encryption and proxy re-encryption. Assuming the whole genome sequencing to be done by a certified institution, we propose to store patients' genomic data encrypted by their public keys at a \"storage and processing unit\" (SPU). Our proposed solution lets the medical unit retrieve the encrypted genomic data from the SPU and process it for medical tests and personalized medicine methods, while preserving the privacy of patients' genomic data. We also quantify the genomic privacy of a patient (from the medical unit's point of view) and show how a patient's genomic privacy decreases with the genetic tests he undergoes due to (i) the nature of the genetic test, and (ii) the characteristics of the genomic data. Furthermore, we show how basic policies and obfuscation methods help to keep the genomic privacy of a patient at a high level. We also implement and show, via a complexity analysis, the practicality of PDS.", "ref_function": ["background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2885741165", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2056556714", "abstract": "Motivated by privacy and usability requirements in various scenarios where existing cryptographic tools (like secure multi-party computation and functional encryption) are not adequate, we introduce a new cryptographic tool called Controlled Functional Encryption (C-FE). As in functional encryption, C-FE allows a user (client) to learn only certain functions of encrypted data, using keys obtained from an authority. However, we allow (and require) the client to send a fresh key request to the authority every time it wants to evaluate a function on a ciphertext. We obtain efficient solutions by carefully combining CCA2 secure public-key encryption (or rerandomizable RCCA secure public-key encryption, depending on the nature of security desired) with Yao's garbled circuit. Our main contributions in this work include developing and for- mally defining the notion of C-FE; designing theoretical and practical constructions of C-FE schemes achieving these definitions for specific and general classes of functions; and evaluating the performance of our constructions on various application scenarios.", "ref_function": ["background", "background", "method", "method", "objective"], "cite_purpose": ["motivation", "background"]}}}
{"sentences": ["As the sensitivity of genomic data does not degrade over time, access to an individual's genome poses a threat to her descendants, even years after she has deceased.", "To the best of our knowledge, GenoGuard @cite_27 is the only attempt to provide long-term security.", "GenoGuard, reviewed in , relies on Honey Encryption @cite_35 , aiming to provide confidentiality in the presence of brute-force attacks; it only serves as a storage mechanism, i.e., it does not support selective retrieval or testing on encrypted data (as such, it is not composable'' with other techniques supporting privacy-preserving testing or data sharing).", "In this paper, we provide a security analysis of GenoGuard.", "In parallel to our work, @cite_15 recently propose attacks against probability model transforming encoders, and also evaluate them on GenoGuard.", "Using machine learning, they train a classifier to distinguish between the real and the decoy sequences, and exclude all decoy data for approximately 48"], "label": ["Reference to current state of knowledge", "Reference to single investigations in the past:  about objective", "Explaining the inadequacies of previous studies", "Describing the objective", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method"], "target_paper": "Due to its hereditary nature, genomic data is not only linked to its owner but to that of close relatives as well. As a result, its sensitivity does not really degrade over time; in fact, the relevance of a genomic sequence is likely to be longer than the security provided by encryption. This prompts the need for specialized techniques providing long-term security for genomic data, yet the only available tool for this purpose is GenoGuard (, 2015). By relying on Honey Encryption, GenoGuard is secure against an adversary that can brute force all possible keys; i.e., whenever an attacker tries to decrypt using an incorrect password, she will obtain an incorrect but plausible looking decoy sequence. In this paper, we set to analyze the real-world security guarantees provided by GenoGuard; specifically, assess how much more information does access to a ciphertext encrypted using GenoGuard yield, compared to one that was not. Overall, we find that, if the adversary has access to side information in the form of partial information from the target sequence, the use of GenoGuard does appreciably increase her power in determining the rest of the sequence. We show that, in the case of a sequence encrypted using an easily guessable (low-entropy) password, the adversary is able to rule out most decoy sequences, and obtain the target sequence with just 2.5 of it available as side information. In the case of a harder-to-guess (high-entropy) password, we show that the adversary still obtains, on average, better accuracy in guessing the rest of the target sequences than using state-of-the-art genomic sequence inference methods, obtaining up to 15 improvement in accuracy.", "reference": {"@cite_35": {"mid": "1892454167", "abstract": "We introduce honey encryption (HE), a simple, general approach to encrypting messages using low min-entropy keys such as passwords. HE is designed to produce a ciphertext which, when decrypted with any of a number of incorrect keys, yields plausible-looking but bogus plaintexts called honey messages. A key benefit of HE is that it provides security in cases where too little entropy is available to withstand brute-force attacks that try every key; in this sense, HE provides security beyond conventional brute-force bounds. HE can also provide a hedge against partial disclosure of high min-entropy keys.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "1714926069", "abstract": "Secure storage of genomic data is of great and increasing importance. The scientific community's improving ability to interpret individuals' genetic materials and the growing size of genetic database populations have been aggravating the potential consequences of data breaches. The prevalent use of passwords to generate encryption keys thus poses an especially serious problem when applied to genetic data. Weak passwords can jeopardize genetic data in the short term, but given the multi-decade lifespan of genetic data, even the use of strong passwords with conventional encryption can lead to compromise. We present a tool, called Geno Guard, for providing strong protection for genomic data both today and in the long term. Geno Guard incorporates a new theoretical framework for encryption called honey encryption (HE): it can provide information-theoretic confidentiality guarantees for encrypted data. Previously proposed HE schemes, however, can be applied to messages from, unfortunately, a very restricted set of probability distributions. Therefore, Geno Guard addresses the open problem of applying HE techniques to the highly non-uniform probability distributions that characterize sequences of genetic data. In Geno Guard, a potential adversary can attempt exhaustively to guess keys or passwords and decrypt via a brute-force attack. We prove that decryption under any key will yield a plausible genome sequence, and that Geno Guard offers an information-theoretic security guarantee against message-recovery attacks. We also explore attacks that use side information. Finally, we present an efficient and parallelized software implementation of Geno Guard.", "ref_function": ["background", "background", "background", "background", "method", "method", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2962983989", "abstract": "", "ref_function": [], "cite_purpose": ["similarities"]}}}
{"sentences": ["Product fit recommendation has only been researched very recently.", "The main challenge is to estimate the true size of a product and the best fitting size for a customer, and match them accordingly.", "This has been handled in a number of different ways.", "In @cite_18 the true size for customers and products is estimated using a latent factor model, and recommendations are made on a similarity-based approach.", "In @cite_11 an extension using a Bayesian model has been proposed.", "A hierarchical Bayesian approach can be found in @cite_9 .", "In @cite_6 the size recommendation problem is tackled by learning embeddings for customers and products.", "The embeddings are combined in a joint space, where metric learning and prototyping is applied in order to derive good representations for the different size classes.", "The authors of @cite_6 also published two datasets with their paper, which we utilize in our experiments."], "label": ["General descriptions of the topic", "General descriptions of the topic", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "One of the biggest hurdles for customers when purchasing fashion online, is the difficulty of finding products with the right fit. In order to provide a better online shopping experience, platforms need to find ways to recommend the right product sizes and the best fitting products to their customers. These recommendation systems, however, require customer feedback in order to estimate the most suitable sizing options. Such feedback is rare and often only available as natural text. In this paper, we examine the extraction of product fit feedback from customer reviews using natural language processing techniques. In particular, we compare traditional methods with more recent transfer learning techniques for text classification, and analyze their results. Our evaluation shows, that the transfer learning approach ULMFit is not only comparatively fast to train, but also achieves highest accuracy on this task. The integration of the extracted information with actual size recommendation systems is left for future work.", "reference": {"@cite_9": {"mid": "2894397262", "abstract": "We introduce a hierarchical Bayesian approach to tackle the challenging problem of size recommendation in e-commerce fashion. Our approach jointly models a size purchased by a customer, and its possible return event: 1. no return, 2. returned too small 3. returned too big. Those events are drawn following a multinomial distribution parameterized on the joint probability of each event, built following a hierarchy combining priors. Such a model allows us to incorporate extended domain expertise and article characteristics as prior knowledge, which in turn makes it possible for the underlying parameters to emerge thanks to sufficient data. Experiments are presented on real (anonymized) data from millions of customers along with a detailed discussion on the efficiency of such an approach within a large scale production system.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2749155890", "abstract": "We propose a novel latent factor model for recommending product size fits Small, Fit, Large to customers. Latent factors for customers and products in our model correspond to their physical true size, and are learnt from past product purchase and returns data. The outcome for a customer, product pair is predicted based on the difference between customer and product true sizes, and efficient algorithms are proposed for computing customer and product true size values that minimize two loss function variants. In experiments with Amazon shoe datasets, we show that our latent factor models incorporating personas, and leveraging return codes show a 17-21 AUC improvement compared to baselines. In an online A B test, our algorithms show an improvement of 0.49 in percentage of Fit transactions over control.", "ref_function": ["background", "background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2893160345", "abstract": "Product size recommendation and fit prediction are critical in order to improve customers' shopping experiences and to reduce product return rates. Modeling customers' fit feedback is challenging due to its subtle semantics, arising from the subjective evaluation of products, and imbalanced label distribution. In this paper, we propose a new predictive framework to tackle the product fit problem, which captures the semantics behind customers' fit feedback, and employs a metric learning technique to resolve label imbalance issues. We also contribute two public datasets collected from online clothing retailers.", "ref_function": ["background", "background", "objective", "method"], "cite_purpose": ["background", "uses"]}, "@cite_11": {"mid": "2788493241", "abstract": "Lack of calibrated product sizing in popular categories such as apparel and shoes leads to customers purchasing incorrect sizes, which in turn results in high return rates due to fi\u20act issues. We address the problem of product size recommendations based on customer purchase and return data. We propose a novel approach based on Bayesian logit and probit regression models with ordinal categories Small, Fit, Large to model size fits as a function of the difference between latent sizes of customers and products. We propose posterior computation based on mean-field variational inference, leveraging the Polya-Gamma augmentation for the logit prior, that results in simple updates, enabling our technique to efficiently handle large datasets. O\u201eur experiments with real-life shoe datasets show that our model outperforms the state of the art in 5 of 6 datasets and leads to an improvement of 17-26 in AUC over baselines when predicting size fit outcomes.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["To extend this notion of distance between @math and @math in , Kantorovich considered a relaxed version in @cite_4 @cite_20 .", "When an optimal transport map exists, the following second Wasserstein distance recovers .", "Further, @math is well-defined, even when an optimal transport map might not exist.", "In particular, it is defined as where @math denotes the set of all joint probability distributions (or equivalently, couplings) whose first and second marginals are @math and @math , respectively.", "Any coupling @math achieving the infimum is called the .", "eq:kantor_relax is also referred to as the primal formulation for Wasserstein- @math distance.", "Kantorovich also provided a dual formulation for eq:kantor_relax , well-known as the Kantorovich duality theorem [Theorem 1.3] villani2003topics , given by where @math denotes the constrained space of functions, defined as @math ."], "label": ["Reference to single investigations in the past:  about objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_4": {"mid": "2134670261", "abstract": "The following paper is reproduced from a Russian journal of the character of our own Proceedings of the National Academy of Sciences, Comptes Rendus (Doklady) de I'Academie des Sciences de I'URSS, 1942, Volume XXXVII, No. 7-8. The author is one of the most distinguished of Russian mathematicians. He has made very important contributions in pure mathematics in the theory of functional analysis, and has made equally important contributions to applied mathematics in numerical analysis and the theory and practice of computation. Although his exposition in this paper is quite terse and couched in mathematical language which may be difficult for some readers of Management Science to follow, it is thought that this presentation will: (1) make available to American readers generally an important work in the field of linear programming, (2) provide an indication of the type of analytic work which has been done and is being done in connection with rational planning in Russia, (3) through the specific examples mentioned indicate the types of interpretation which the Russians have made of the abstract mathematics (for example, the potential and field interpretations adduced in this country recently by W. Prager were anticipated in this paper). It is to be noted, however, that the problem of determining an effective method of actually acquiring the solution to a specific problem is not solved in this paper. In the category of development of such methods we seem to be, currently, ahead of the Russians.--A. Charnes, Northwestern Technological Institute and The Transportation Center.", "ref_function": ["background", "background", "background", "background", "objective", "method", "other", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2036476394", "abstract": "In 1942, I considered a general problem on the most profitable translocation of masses in a compact metric space. The problem is as follows: Assume that we are given two mass distributions determined by additive set functions \u03a6(e) and \u03a6\u2032(e) with \u03a6(R) = \u03a6\u2032(R) = 1. A translocation of masses is a function \u03a8(e, e\u2032) that determines the mass translocated from a set e to a set e\u2032 with [\u03a8(e, R) = \u03a6(e); \u03a8(R, e) = \u03a6\u2032(e)]. The translocation work is defined by the integral", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["As there is no easy way to ensure the feasibility of the constraints along the gradient updates, common approach is to translate the optimization into a tractable form, while sacrificing the original goal of finding the optimal transport @cite_25 .", "Concretely, an entropic or a quadratic regularizer is added to .", "This makes the dual an unconstrained problem, which can be numerically solved using Sinkhorn algorithm @cite_25 or stochastic gradient methods @cite_15 @cite_8 .", "The optimal transport can then be obtained from @math and @math , using the first-order optimality conditions of the Fenchel-Rockafellar's duality theorem."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_15": {"mid": "2962970351", "abstract": "Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different computational setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat the current state of the art finite dimensional OT solver (Sinkhorn's algorithm) ; (ii) when comparing a discrete distribution to a continuous density, a re-formulation (semi-discrete) of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, and is more efficient than discretizing beforehand the two densities. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2158131535", "abstract": "Optimal transport distances are a fundamental family of distances for probability measures and histograms of features. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost can quickly become prohibitive whenever the size of the support of these measures or the histograms' dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport problems from a maximum-entropy perspective. We smooth the classic optimal transport problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon classic optimal transport distances on the MNIST classification problem.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_8": {"mid": "2767358676", "abstract": "This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT plan and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.", "ref_function": ["objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In this paper, we take a different approach and aim to solve the dual problem, without introducing a regularization.", "This idea is also considered classically in @cite_12 and more recently in @cite_21 and @cite_11 .", "The classical approach relies on the exact knowledge of the density, which is not available in practice.", "The approach in @cite_21 relies on the discrete Brenier theory which is computationally expensive and not scalable.", "The most related work to ours is @cite_11 , which we provide a formal comparison in ."], "label": ["Describing used methods", "General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_21": {"mid": "2911275792", "abstract": "This work builds the connection between the regularity theory of optimal transportation map, Monge-Ampere equation and GANs, which gives a theoretic understanding of the major drawbacks of GANs: convergence difficulty and mode collapse. According to the regularity theory of Monge-Ampere equation, if the support of the target measure is disconnected or just non-convex, the optimal transportation mapping is discontinuous. General DNNs can only approximate continuous mappings. This intrinsic conflict leads to the convergence difficulty and mode collapse in GANs. We test our hypothesis that the supports of real data distribution are in general non-convex, therefore the discontinuity is unavoidable using an Autoencoder combined with discrete optimal transportation map (AE-OT framework) on the CelebA data set. The testing result is positive. Furthermore, we propose to approximate the continuous Brenier potential directly based on discrete Brenier theory to tackle mode collapse. Comparing with existing method, this method is more accurate and effective.", "ref_function": ["background", "background", "background", "background", "method", "result", "method", "result"], "cite_purpose": ["background", ""]}, "@cite_12": {"mid": "2292026763", "abstract": "We present a new, simple, and elegant algorithm for computing the optimal mapping for the Monge-Kantorovich problem with quadratic cost. The method arises from a reformulation of the dual problem into an unconstrained minimization of a convex, continuous functional, for which the derivative can be explicitly found. The Monge-Kantorovich problem has applications in many fields; examples from image warping and medical imaging are shown.", "ref_function": ["background", "method", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2917201408", "abstract": "We provide a framework to approximate the 2-Wasserstein distance and the optimal transport map, amenable to efficient training as well as statistical and geometric analysis. With the quadratic cost and considering the Kantorovich dual form of the optimal transportation problem, the Brenier theorem states that the optimal potential function is convex and the optimal transport map is the gradient of the optimal potential function. Using this geometric structure, we restrict the optimization problem to different parametrized classes of convex functions and pay special attention to the class of input-convex neural networks. We analyze the statistical generalization and the discriminative power of the resulting approximate metric, and we prove a restricted moment-matching property for the approximate optimal map. Finally, we discuss a numerical algorithm to solve the restricted optimization problem and provide numerical experiments to illustrate and compare the proposed approach with the established regularization-based approaches. We further discuss practical implications of our proposal in a modular and interpretable design for GANs which connects the generator training with discriminator computations to allow for learning an overall composite generator.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "similarities"]}}}
{"sentences": ["The idea of solving the semi-dual optimization problem is classically considered in @cite_12 , where the authors derive a formula for the functional derivative of the objective function with respect to @math and propose to solve the optimization problem with the gradient descent method.", "Their approach is based on the discretization of the space and knowledge of the explicit form of the probability density functions, that is not applicable to real-world high dimensional problems."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_12": {"mid": "2292026763", "abstract": "We present a new, simple, and elegant algorithm for computing the optimal mapping for the Monge-Kantorovich problem with quadratic cost. The method arises from a reformulation of the dual problem into an unconstrained minimization of a convex, continuous functional, for which the derivative can be explicitly found. The Monge-Kantorovich problem has applications in many fields; examples from image warping and medical imaging are shown.", "ref_function": ["background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["More recently, the authors in @cite_29 @cite_21 propose to learn the function @math in a semi-discrete setting, where one of the marginals is assumed to be a discrete distribution supported on a set of @math points @math , and the other marginal is assumed to have a continuous density with compact convex support @math .", "They show that the problem of learning the function @math is similar to the variational formulation of the Alexandrov problem: constructing a convex polytope with prescribed face normals and volumes.", "Moreover, they show that, in the semi-distrete setting, the optimal @math is of the form @math and simplify the problem of learning @math to the problem learning @math real numbers @math .", "However, the objective function involves computing polygonal partition of @math into @math convex cells, induced by the function @math , which is computationally challenging.", "Moreover, the learned optimal transport map @math , transports the probability distribution from each convex cell to a single point @math , which results in generalization issues.", "Additionally, the proposed approach is semi-discrete, and as a result, does not scale with the number of samples."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: about results", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_29": {"mid": "2766665711", "abstract": "In this work, we show the intrinsic relations between optimal transportation and convex geometry, especially the variational approach to solve Alexandrov problem: constructing a convex polytope with prescribed face normals and volumes. This leads to a geometric interpretation to generative models, and leads to a novel framework for generative models. By using the optimal transportation view of GAN model, we show that the discriminator computes the Kantorovich potential, the generator calculates the transportation map. For a large class of transportation costs, the Kantorovich potential can give the optimal transportation map by a close-form formula. Therefore, it is sufficient to solely optimize the discriminator. This shows the adversarial competition can be avoided, and the computational architecture can be simplified. Preliminary experimental results show the geometric method outperforms WGAN for approximating probability measures with multiple clusters in low dimensional space.", "ref_function": ["background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2911275792", "abstract": "This work builds the connection between the regularity theory of optimal transportation map, Monge-Ampere equation and GANs, which gives a theoretic understanding of the major drawbacks of GANs: convergence difficulty and mode collapse. According to the regularity theory of Monge-Ampere equation, if the support of the target measure is disconnected or just non-convex, the optimal transportation mapping is discontinuous. General DNNs can only approximate continuous mappings. This intrinsic conflict leads to the convergence difficulty and mode collapse in GANs. We test our hypothesis that the supports of real data distribution are in general non-convex, therefore the discontinuity is unavoidable using an Autoencoder combined with discrete optimal transportation map (AE-OT framework) on the CelebA data set. The testing result is positive. Furthermore, we propose to approximate the continuous Brenier potential directly based on discrete Brenier theory to tackle mode collapse. Comparing with existing method, this method is more accurate and effective.", "ref_function": ["background", "background", "background", "background", "method", "result", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Statistical analysis of learning the optimal transport map through the semi-dual optimization problem is studied in @cite_17 @cite_23 , where the authors establish a minimax convergence rate with respect to number of samples for certain classes of regular probability distributions.", "They also propose a procedure that achieves the optimal convergence rate, that involves representing the function @math with span of wavelet basis functions up to a certain order, and also requiring the function @math to be convex.", "However, they do not provide a computational algorithm to implement the procedure."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_23": {"mid": "2810943841", "abstract": "Isotonic regression is a standard problem in shape-constrained estimation where the goal is to estimate an unknown nondecreasing regression function @math from independent pairs @math where @math . While this problem is well understood both statistically and computationally, much less is known about its uncoupled counterpart where one is given only the unordered sets @math and @math . In this work, we leverage tools from optimal transport theory to derive minimax rates under weak moments conditions on @math and to give an efficient algorithm achieving optimal rates. Both upper and lower bounds employ moment-matching arguments that are also pertinent to learning mixtures of distributions and deconvolution.", "ref_function": ["background", "background", "method", "method"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2946680009", "abstract": "Brenier's theorem is a cornerstone of optimal transport that guarantees the existence of an optimal transport map @math between two probability distributions @math and @math over @math under certain regularity conditions. The main goal of this work is to establish the minimax rates estimation rates for such a transport map from data sampled from @math and @math under additional smoothness assumptions on @math . To achieve this goal, we develop an estimator based on the minimization of an empirical version of the semi-dual optimal transport problem, restricted to truncated wavelet expansions. This estimator is shown to achieve near minimax optimality using new stability arguments for the semi-dual and a complementary minimax lower bound. These are the first minimax estimation rates for transport maps in general dimension.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The approach proposed in this paper is built upon the recent work @cite_11 , where the proposal to solve the semi-dual optimization problem by representing the function @math with an ICNN appeared for the first time.", "The proposed procedure in @cite_11 involves solving a convex optimization problem to compute the convex conjugate @math for each sample in the batch, at each optimization iteration.", "This procedure becomes computationally challenging to scale to large datasets.", "However, in this paper, we propose a minimax formulation to learn the convex conjugate function in a scalable fashion."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Describing used methods"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_11": {"mid": "2917201408", "abstract": "We provide a framework to approximate the 2-Wasserstein distance and the optimal transport map, amenable to efficient training as well as statistical and geometric analysis. With the quadratic cost and considering the Kantorovich dual form of the optimal transportation problem, the Brenier theorem states that the optimal potential function is convex and the optimal transport map is the gradient of the optimal potential function. Using this geometric structure, we restrict the optimization problem to different parametrized classes of convex functions and pay special attention to the class of input-convex neural networks. We analyze the statistical generalization and the discriminative power of the resulting approximate metric, and we prove a restricted moment-matching property for the approximate optimal map. Finally, we discuss a numerical algorithm to solve the restricted optimization problem and provide numerical experiments to illustrate and compare the proposed approach with the established regularization-based approaches. We further discuss practical implications of our proposal in a modular and interpretable design for GANs which connects the generator training with discriminator computations to allow for learning an overall composite generator.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["extends", "background"]}}}
{"sentences": ["There are also other alternative approaches to approximate the optimal transport map that are not based on solving the semi-dual optimization problem .", "@cite_24 , the authors propose to approximate the optimal transport map, through an adversarial computational procedure, by considering the dual optimization problem , and replacing the constraint with a quadratic penalty term.", "However, in contrast to the other regularization-based approaches such as @cite_8 , they consider a GAN architecture, and propose to take the generator, after the training is finished, as the optimal transport map.", "They also provide a theoretical justification for their proposal, however the theoretical justification is valid in an ideal setting where the generator has infinite capacity, the discriminator is optimal at each update step, and the cost is equal to the exact Wasserstein distance.", "These ideal conditions are far from being true in a practical setting."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_24": {"mid": "2950516984", "abstract": "Computing optimal transport maps between high-dimensional and continuous distributions is a challenging problem in optimal transport (OT). Generative adversarial networks (GANs) are powerful generative models which have been successfully applied to learn maps across high-dimensional domains. However, little is known about the nature of the map learned with a GAN objective. To address this problem, we propose a generative adversarial model in which the discriminator's objective is the @math -Wasserstein metric. We show that during training, our generator follows the @math -geodesic between the initial and the target distributions. As a consequence, it reproduces an optimal map at the end of training. We validate our approach empirically in both low-dimensional and high-dimensional continuous settings, and show that it outperforms prior methods on image data.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2767358676", "abstract": "This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT plan and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.", "ref_function": ["objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Another approach, proposed in @cite_22 , is based on a generative learning framework to approximate the optimal coupling, instead of optimal transport map.", "The approach involves a low-dimensional latent random variable, two generators that take the latent variable as input and map it to a high-dimensional space where the real data resides in, and two discriminators that respectively take as inputs the real data and the output of the generator.", "Although, the proposed approach is attractive when an optimal transport map does not exist, it is computationally expensive because it involves learning four deep neural networks, and suffers from unused capacity issues that WGAN architecture suffers from @cite_33 ."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_22": {"mid": "2942758405", "abstract": "Optimal Transport (OT) naturally arises in many machine learning applications, yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.", "ref_function": ["background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2962879692", "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["motivation"]}}}
{"sentences": ["Finally, a procedure is recently proposed to approximate the optimal transport map that is optimal only on a subspace projection instead of the entire space @cite_1 .", "This approach is inspired by the sliced Wasserstein distance method to approximate the Wasserstein distance @cite_16 @cite_27 .", "However, selection of the subspace to project on is a non-trivial task, and optimally selecting the projection is an optimization over the Grassmann manifold which is computationally challenging."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a discontinuous transport mapping.", "reference": {"@cite_27": {"mid": "2963398989", "abstract": "Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our approach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good measure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_16": {"mid": "1639961155", "abstract": "This paper proposes a new definition of the averaging of discrete probability distributions as a barycenter over the Monge-Kantorovich optimal transport space. To overcome the time complexity involved by the numerical solving of such problem, the original Wasserstein metric is replaced by a sliced approximation over 1D distributions. This enables us to introduce a new fast gradient descent algorithm to compute Wasserstein barycenters of point clouds. This new notion of barycenter of probabilities is likely to find applications in computer vision where one wants to average features defined as distributions. We show an application to texture synthesis and mixing, where a texture is characterized by the distribution of the response to a multi-scale oriented filter bank. This leads to a simple way to navigate over a convex domain of color textures.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_1": {"mid": "2970661343", "abstract": "Sliced Wasserstein metrics between probability measures solve the optimal transport (OT) problem on univariate projections, and average such maps across projections. The recent interest for the SW distance shows that much can be gained by looking at optimal maps between measures in smaller subspaces, as opposed to the curse-of-dimensionality price one has to pay in higher dimensions. Any transport estimated in a subspace remains, however, an object that can only be used in that subspace. We propose in this work two methods to extrapolate, from an transport map that is optimal on a subspace, one that is nearly optimal in the entire space. We prove that the best optimal transport plan that takes such \"subspace detours\" is a generalization of the Knothe-Rosenblatt transport. We show that these plans can be explicitly formulated when comparing Gaussians measures (between which the Wasserstein distance is usually referred to as the Bures or Fr 'echet distance). Building from there, we provide an algorithm to select optimal subspaces given pairs of Gaussian measures, and study scenarios in which that mediating subspace can be selected using prior information. We consider applications to NLP and evaluation of image quality (FID scores).", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["black This article contributes to the literature of neural-based chatbots as follows.", "First, our methodology for training value-based DRL agents uses only unlabelled dialogue data.", "Previous work requires manual extensions to the dialogue data @cite_13 or expensive and time consuming ratings for training a reward function @cite_1 .", "Second, our proposed reward function strongly correlates with human judgements.", "Previous work has only shown moderate positive correlations between target dialogue rewards and predicted ones @cite_1 , or rely on high-level annotations requiring external and language-dependent resources typically induced from labelled data @cite_0 .", "Third, while previous work on DRL chatbots train a single agent @cite_1 @cite_13 , our study---confirmed by automatic and human evaluations---shows that an ensemble-based approach performs better than a counterpart single agent.", "The remainder of this article elaborates on these contributions."], "label": ["Describing the objective", "Describing used methods", "General reference to previous research or scholarship: approaches taken", "Describing the results", "Explaining the inadequacies of previous studies", "Describing the results", "Other functional sentences"], "target_paper": "Abstract Trainable chatbots that exhibit fluent and human-like conversations remain a big challenge in artificial intelligence. Deep Reinforcement Learning (DRL) is promising for addressing this challenge, but its successful application remains an open question. This article describes a novel ensemble-based approach applied to value-based DRL chatbots, which use finite action sets as a form of meaning representation. In our approach, while dialogue actions are derived from sentence clustering, the training datasets in our ensemble are derived from dialogue clustering. The latter aim to induce specialised agents that learn to interact in a particular style. In order to facilitate neural chatbot training using our proposed approach, we assume dialogue data in raw text only \u2013 without any manually-labelled data. Experimental results using chitchat data reveal that (1) near human-like dialogue policies can be induced, (2) generalisation to unseen data is a difficult problem, and (3) training an ensemble of chatbot agents is essential for improved performance over using a single agent. In addition to evaluations using held-out data, our results are further supported by a human evaluation that rated dialogues in terms of fluency, engagingness and consistency \u2013 which revealed that our proposed dialogue rewards strongly correlate with human judgements.", "reference": {"@cite_1": {"mid": "2784808670", "abstract": "We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including neural network and template-based models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A B testing with real-world users, where it performed significantly better than other systems. The results highlight the potential of coupling ensemble systems with deep reinforcement learning as a fruitful path for developing real-world, open-domain conversational agents.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["", "motivation", "differences"]}, "@cite_0": {"mid": "2904468521", "abstract": "Abstract End-to-end dialog systems are gaining interest due to the recent advances of deep neural networks and the availability of large human\u2013human dialog corpora. However, in spite of being of fundamental importance to systematically improve the performance of this kind of systems, automatic evaluation of the generated dialog utterances is still an unsolved problem. Indeed, most of the proposed objective metrics shown low correlation with human evaluations. In this paper, we evaluate a two-dimensional evaluation metric that is designed to operate at sentence level, which considers the syntactic and semantic information carried along the answers generated by an end-to-end dialog system with respect to a set of references. The proposed metric, when applied to outputs generated by the systems participating in track 2 of the DSTC-6 challenge, shows a higher correlation with human evaluations (up to 12.8 relative improvement at the system level) than the best of the alternative state-of-the-art automatic metrics currently available.", "ref_function": ["background", "background", "background", "method", "result"], "cite_purpose": ["motivation"]}, "@cite_13": {"mid": "2963167310", "abstract": "", "ref_function": [], "cite_purpose": ["", "differences"]}}}
{"sentences": ["Most of existing face anti-spoofing datasets only contain the RGB modality, including the two widely used PAD datasets Replay-Attack @cite_31 and CASIA-FASD @cite_50 .", "Even the recently released SiW @cite_26 dataset, collected with high resolution image quality, only contains RGB data.", "With the widespread application of face recognition in mobile phones, there are also some RGB datasets recorded by replaying face video with smartphone, such as MSU-MFSD @cite_47 , Replay-Mobile @cite_21 and OULU-NPU @cite_46 ."], "label": ["Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Face anti-spoofing is essential to prevent face recognition systems from a security breach. Much of the progresses have been made by the availability of face anti-spoofing benchmark datasets in recent years. However, existing face anti-spoofing benchmarks have limited number of subjects ( @math ) and modalities ( @math ), which hinder the further development of the academic community. To facilitate face anti-spoofing research, we introduce a large-scale multi-modal dataset, namely CASIA-SURF, which is the largest publicly available dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of @math subjects with @math videos and each sample has @math modalities (i.e., RGB, Depth and IR). We also provide comprehensive evaluation metrics, diverse evaluation protocols, training validation testing subsets and a measurement tool, developing a new benchmark for face anti-spoofing. Moreover, we present a novel multi-modal multi-scale fusion method as a strong baseline, which performs feature re-weighting to select the more informative channel features while suppressing the less useful ones for each modality across different scales. Extensive experiments have been conducted on the proposed dataset to verify its significance and generalization capability. The dataset is available at this https URL", "reference": {"@cite_31": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_26": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": [""]}, "@cite_46": {"mid": "2728977829", "abstract": "The vulnerabilities of face-based biometric systems to presentation attacks have been finally recognized but yet we lack generalized software-based face presentation attack detection (PAD) methods performing robustly in practical mobile authentication scenarios. This is mainly due to the fact that the existing public face PAD datasets are beginning to cover a variety of attack scenarios and acquisition conditions but their standard evaluation protocols do not encourage researchers to assess the generalization capabilities of their methods across these variations. In this present work, we introduce a new public face PAD database, OULU-NPU, aiming at evaluating the generalization of PAD methods in more realistic mobile authentication scenarios across three covariates: unknown environmental conditions (namely illumination and background scene), acquisition devices and presentation attack instruments (PAI). This publicly available database consists of 5940 videos corresponding to 55 subjects recorded in three different environments using high-resolution frontal cameras of six different smartphones. The high-quality print and videoreplay attacks were created using two different printers and two different display devices. Each of the four unambiguously defined evaluation protocols introduces at least one previously unseen condition to the test set, which enables a fair comparison on the generalization capabilities between new and existing approaches. The baseline results using color texture analysis based face PAD method demonstrate the challenging nature of the database.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2552267233", "abstract": "For face authentication to become widespread on mobile devices, robust countermeasures must be developed for face presentation-attack detection (PAD). Existing databases for evaluating face-PAD methods do not capture the specific characteristics of mobile devices. We introduce a new database, REPLAY-MOBILE, for this purpose.1 This publicly available database includes 1,200 videos corresponding to 40 clients. Besides the genuine videos, the database contains a variety of presentation-attacks. The database also provides three non- overlapping sets for training, validating and testing classifiers for the face-PAD problem. This will help researchers in comparing new approaches to existing algorithms in a standardized fashion. For this purpose, we also provide baseline results with state- of-the-art approaches based on image quality analysis and face texture analysis.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_50": {"mid": "1982209341", "abstract": "Face antispoofing has now attracted intensive attention, aiming to assure the reliability of face biometrics. We notice that currently most of face antispoofing databases focus on data with little variations, which may limit the generalization performance of trained models since potential attacks in real world are probably more complex. In this paper we release a face antispoofing database which covers a diverse range of potential attack variations. Specifically, the database contains 50 genuine subjects, and fake faces are made from the high quality records of the genuine faces. Three imaging qualities are considered, namely the low quality, normal quality and high quality. Three fake face attacks are implemented, which include warped photo attack, cut photo attack and video attack. Therefore each subject contains 12 videos (3 genuine and 9 fake), and the final database contains 600 video clips. Test protocol is provided, which consists of 7 scenarios for a thorough evaluation from all possible aspects. A baseline algorithm is also given for comparison, which explores the high frequency information in the facial region to determine the liveness. We hope such a database can serve as an evaluation platform for future researches in the literature.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_47": {"mid": "2003092530", "abstract": "Automatic face recognition is now widely used in applications ranging from deduplication of identity to authentication of mobile payment. This popularity of face recognition has raised concerns about face spoof attacks (also known as biometric sensor presentation attacks), where a photo or video of an authorized person\u2019s face could be used to gain access to facilities or services. While a number of face spoof detection techniques have been proposed, their generalization ability has not been adequately addressed. We propose an efficient and rather robust face spoof detection algorithm based on image distortion analysis (IDA). Four different features (specular reflection, blurriness, chromatic moment, and color diversity) are extracted to form the IDA feature vector. An ensemble classifier, consisting of multiple SVM classifiers trained for different face spoof attacks (e.g., printed photo and replayed video), is used to distinguish between genuine (live) and spoof faces. The proposed approach is extended to multiframe face spoof detection in videos using a voting-based scheme. We also collect a face spoof database, MSU mobile face spoofing database (MSU MFSD), using two mobile devices (Google Nexus 5 and MacBook Air) with three types of spoof attacks (printed photo, replayed video with iPhone 5S, and replayed video with iPad Air). Experimental results on two public-domain face spoof databases (Idiap REPLAY-ATTACK and CASIA FASD), and the MSU MFSD database show that the proposed approach outperforms the state-of-the-art methods in spoof detection. Our results also highlight the difficulty in separating genuine and spoof faces, especially in cross-database and cross-device scenarios.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["As attack techniques are constantly upgraded, some new types of presentation attacks have emerged, , 3D @cite_0 and silicone masks @cite_38 .", "These attacks are more realistic than traditional 2D attacks.", "Therefore, the drawbacks of visible cameras are revealed when facing these realistic face masks.", "Fortunately, some new sensors have been introduced to provide more possibilities for face PAD methods, such as depth cameras, muti-spectral cameras and infrared light cameras.", "Kim al @cite_4 introduce a new dataset to distinguish between the facial skin and mask materials by exploiting their reflectance.", "Kose al @cite_43 propose a 2D+3D face mask attack dataset to study the effects of mask attacks.", "However, associated data has not been made public.", "3DMAD @cite_0 is the first publicly available 3D masks dataset, which is recorded using Microsoft Kinect sensor and consists of Depth and RGB modalities.", "Another multi-modal face PAD dataset is Msspoof @cite_6 , containing visible and near-infrared images of real accesses and printed spoofing attacks with @math objects."], "label": ["General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Face anti-spoofing is essential to prevent face recognition systems from a security breach. Much of the progresses have been made by the availability of face anti-spoofing benchmark datasets in recent years. However, existing face anti-spoofing benchmarks have limited number of subjects ( @math ) and modalities ( @math ), which hinder the further development of the academic community. To facilitate face anti-spoofing research, we introduce a large-scale multi-modal dataset, namely CASIA-SURF, which is the largest publicly available dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of @math subjects with @math videos and each sample has @math modalities (i.e., RGB, Depth and IR). We also provide comprehensive evaluation metrics, diverse evaluation protocols, training validation testing subsets and a measurement tool, developing a new benchmark for face anti-spoofing. Moreover, we present a novel multi-modal multi-scale fusion method as a strong baseline, which performs feature re-weighting to select the more informative channel features while suppressing the less useful ones for each modality across different scales. Extensive experiments have been conducted on the proposed dataset to verify its significance and generalization capability. The dataset is available at this https URL", "reference": {"@cite_38": {"mid": "2887396754", "abstract": "We investigate the vulnerability of convolutional neural network (CNN) based face-recognition (FR) systems to presentation attacks (PA) performed using custom-made silicone masks. Previous works have studied the vulnerability of CNN-FR systems to 2D PAs such as print-attacks, or digital- video replay attacks, and to rigid 3D masks. This is the first study to consider PAs performed using custom-made flexible silicone masks. Before embarking on research on detecting a new variety of PA, it is important to estimate the seriousness of the threat posed by the type of PA. In this work we demonstrate that PAs using custom silicone masks do pose a serious threat to state-of-the-art FR systems. Using a new dataset based on six custom silicone masks, we show that the vulnerability of each FR system in this study is at least 10 times higher than its false match rate. We also propose a simple but effective presentation attack detection method, based on a low-cost thermal camera.", "ref_function": ["background", "background", "background", "method", "method", "result", "method"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "1983008792", "abstract": "This research presents a novel 2D feature space where real faces and masked fake faces can be effectively discriminated. We exploit the reflectance disparity based on albedo between real faces and fake materials. The feature vector used consists of radiance measurements of the forehead region under 850 and 685 nm illuminations. Facial skin and mask material show linearly separable distributions in the feature space proposed. By simply applying Fisher's linear discriminant, we have achieved 97.78 accuracy in fake face detection. Our method can be easily implemented in commercial face verification systems.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2368383431", "abstract": "In this chapter, we give an overview of spoofing attacks and spoofing countermeasures for face recognition systems , with a focus on visual spectrum systems (VIS) in 2D and 3D, as well as near-infrared (NIR) and multispectral systems . We cover the existing types of spoofing attacks and report on their success to bypass several state-of-the-art face recognition systems. The results on two different face spoofing databases in VIS and one newly developed face spoofing database in NIR show that spoofing attacks present a significant security risk for face recognition systems in any part of the spectrum. The risk is partially reduced when using multispectral systems. We also give a systematic overview of the existing anti-spoofing techniques, with an analysis of their advantages and limitations and prospective for future work.", "ref_function": ["background", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2125320497", "abstract": "The problem of detecting face spoofing attacks (presentation attacks) has recently gained a well-deserved popularity. Mainly focusing on 2D attacks forged by displaying printed photos or replaying recorded videos on mobile devices, a significant portion of these studies ground their arguments on the flatness of the spoofing material in front of the sensor. In this paper, we inspect the spoofing potential of subject-specific 3D facial masks for 2D face recognition. Additionally, we analyze Local Binary Patterns based coun-termeasures using both color and depth data, obtained by Kinect. For this purpose, we introduce the 3D Mask Attack Database (3DMAD), the first publicly available 3D spoofing database, recorded with a low-cost depth camera. Extensive experiments on 3DMAD show that easily attainable facial masks can pose a serious threat to 2D face recognition systems and LBP is a powerful weapon to eliminate it.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_43": {"mid": "2011016023", "abstract": "There are several types of spoofing attacks to face recognition systems such as photograph, video or mask attacks. Recent studies show that face recognition systems are vulnerable to these attacks. In this paper, a countermeasure technique is proposed to protect face recognition systems against mask attacks. To the best of our knowledge, this is the first time a countermeasure is proposed to detect mask attacks. The reason for this delay is mainly due to the unavailability of public mask attacks databases. In this study, a 2D+3D face mask attacks database is used which is prepared for a research project in which the authors are all involved. The performance of the countermeasure is evaluated on both the texture images and the depth maps, separately. The results show that the proposed countermeasure gives satisfactory results using both the texture images and the depth maps. The performance of the countermeasure is observed to be slight better when the technique is applied on texture images instead of depth maps, which proves that face texture provides more information than 3D face shape characteristics using the proposed approach.", "ref_function": ["background", "background", "objective", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Face anti-spoofing has been studied for decades.", "Some previous works @cite_34 @cite_51 @cite_1 @cite_53 attempt to detect the evidence of liveness ( , eye-blinking).", "Another works are based on contextual @cite_7 @cite_9 and moving @cite_22 @cite_44 @cite_57 information.", "To improve the robustness to illumination variation, some algorithms adopt HSV and YCbCr color spaces @cite_11 @cite_39 , as well as Fourier spectrum @cite_56 .", "All of these methods use handcrafted features, such as LBP @cite_15 @cite_13 @cite_23 @cite_24 , HoG @cite_23 @cite_24 @cite_19 and GLCM @cite_19 .", "They achieve a relatively satisfactory performance on small public face anti-spoofing datasets."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Summarize the above references"], "target_paper": "Face anti-spoofing is essential to prevent face recognition systems from a security breach. Much of the progresses have been made by the availability of face anti-spoofing benchmark datasets in recent years. However, existing face anti-spoofing benchmarks have limited number of subjects ( @math ) and modalities ( @math ), which hinder the further development of the academic community. To facilitate face anti-spoofing research, we introduce a large-scale multi-modal dataset, namely CASIA-SURF, which is the largest publicly available dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of @math subjects with @math videos and each sample has @math modalities (i.e., RGB, Depth and IR). We also provide comprehensive evaluation metrics, diverse evaluation protocols, training validation testing subsets and a measurement tool, developing a new benchmark for face anti-spoofing. Moreover, we present a novel multi-modal multi-scale fusion method as a strong baseline, which performs feature re-weighting to select the more informative channel features while suppressing the less useful ones for each modality across different scales. Extensive experiments have been conducted on the proposed dataset to verify its significance and generalization capability. The dataset is available at this https URL", "reference": {"@cite_13": {"mid": "2163487272", "abstract": "Spoofing attacks are one of the security traits that biometric recognition systems are proven to be vulnerable to. When spoofed, a biometric recognition system is bypassed by presenting a copy of the biometric evidence of a valid user. Among all biometric modalities, spoofing a face recognition system is particularly easy to perform: all that is needed is a simple photograph of the user. In this paper, we address the problem of detecting face spoofing attacks. In particular, we inspect the potential of texture features based on Local Binary Patterns (LBP) and their variations on three types of attacks: printed photographs, and photos and videos displayed on electronic screens of different sizes. For this purpose, we introduce REPLAY-ATTACK, a novel publicly available face spoofing database which contains all the mentioned types of attacks. We conclude that LBP, with \u223c15 Half Total Error Rate, show moderate discriminability when confronted with a wide set of attack types.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "1971062533", "abstract": "Face recognition, which is security-critical, has been widely deployed in our daily life. However, traditional face recognition technologies in practice can be spoofed easily, for example, by using a simple printed photo. In this paper, we propose a novel face liveness detection approach to counter spoofing attacks by recovering sparse 3D facial structure. Given a face video or several images captured from more than two viewpoints, we detect facial landmarks and select key frames. Then, the sparse 3D facial structure can be recoveredfrom the selected key frames. Finally, an Support Vector Machine (SVM) classifier is trained to distinguish the genuine and fake faces. Compared with the previous works, the proposed method has the following advantages. First, it gives perfect liveness detection results, which meets the security requirement of face biometric systems. Second, it is independent on cameras or systems, which works well on different devices. Experiments with genuine faces versus planar photo faces and warped photo faces demonstrate the superiority of the proposed method over the state-of-the-art liveness detection methods.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2145426126", "abstract": "This paper presents a face liveness detection system against spoofing with photographs, videos, and 3D models of a valid user in a face recognition system. Anti-spoofing clues inside and outside a face are both exploited in our system. The inside-face clues of spontaneous eyeblinks are employed for anti-spoofing of photographs and 3D models. The outside-face clues of scene context are used for anti-spoofing of video replays. The system does not need user collaborations, i.e. it runs in a non-intrusive manner. In our system, the eyeblink detection is formulated as an inference problem of an undirected conditional graphical framework which models contextual dependencies in blink image sequences. The scene context clue is found by comparing the difference of regions of interest between the reference scene image and the input one, which is based on the similarity computed by local binary pattern descriptors on a series of fiducial points extracted in scale space. Extensive experiments are carried out to show the effectiveness of our system.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_53": {"mid": "2145131129", "abstract": "For a robust face biometric system, a reliable anti-spoofing approach must be deployed to circumvent the print and replay attacks. Several techniques have been proposed to counter face spoofing, however a robust solution that is computationally efficient is still unavailable. This paper presents a new approach for spoofing detection in face videos using motion magnification. Eulerian motion magnification approach is used to enhance the facial expressions commonly exhibited by subjects in a captured video. Next, two types of feature extraction algorithms are proposed: (i) a configuration of LBP that provides improved performance compared to other computationally expensive texture based approaches and (ii) motion estimation approach using HOOF descriptor. On the Print Attack and Replay Attack spoofing datasets, the proposed framework improves the state-of-art performance, especially HOOF descriptor yielding a near perfect half total error rate of 0 and 1.25 respectively.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2140593870", "abstract": "Resisting spoofing attempts via photographs and video playbacks is a vital issue for the success of face biometrics. Yet, the ldquolivenessrdquo topic has only been partially studied in the past. In this paper we are suggesting a holistic liveness detection paradigm that collaborates with standard techniques in 2D face biometrics. The experiments show that many attacks are avertible via a combination of anti-spoofing measures. We have investigated the topic using real-time techniques and applied them to real-life spoofing scenarios in an indoor, yet uncontrolled environment.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_39": {"mid": "2551249768", "abstract": "The vulnerabilities of face biometric authentication systems to spoofing attacks have received a significant attention during the recent years. Some of the proposed countermeasures have achieved impressive results when evaluated on intratests, i.e., the system is trained and tested on the same database. Unfortunately, most of these techniques fail to generalize well to unseen attacks, e.g., when the system is trained on one database and then evaluated on another database. This is a major concern in biometric antispoofing research that is mostly overlooked. In this letter, we propose a novel solution based on describing the facial appearance by applying Fisher vector encoding on speeded-up robust features extracted from different color spaces. The evaluation of our countermeasure on three challenging benchmark face-spoofing databases, namely the CASIA face antispoofing database, the replay-attack database, and MSU mobile face spoof database, showed excellent and stable performance across all the three datasets. Most importantly, in interdatabase tests, our proposed approach outperforms the state of the art and yields very promising generalization capabilities, even when only limited training data are used.", "ref_function": ["background", "background", "background", "background", "objective", "result", "result"], "cite_purpose": ["background"]}, "@cite_44": {"mid": "2106938474", "abstract": "Face recognition provides many advantages compared with other available biometrics, but it is particularly subject to spoofing. The most accurate methods in literature addressing this problem, rely on the estimation of the three-dimensionality of faces, which heavily increase the whole cost of the system. This paper proposes an effective and efficient solution to problem of face spoofing. Starting from a set of automatically located facial points, we exploit geometric invariants for detecting replay attacks. The presented results demonstrate the effectiveness and efficiency of the proposed indices.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_57": {"mid": "2012612618", "abstract": "As Face Recognition(FR) technology becomes more mature and commercially available in the market, many different anti-spoofing techniques have been recently developed to enhance the security, reliability, and effectiveness of FR systems. As a part of anti-spoofing techniques, face liveness detection plays an important role to make FR systems be more secured from various attacks. In this paper, we propose a novel method for face liveness detection by using focus, which is one of camera functions. In order to identify fake faces (e.g. 2D pictures), our approach utilizes the variation of pixel values by focusing between two images sequentially taken in different focuses. The experimental result shows that our focus-based approach is a new method that can significantly increase the level of difficulty of spoof attacks, which is a way to improve the security of FR systems. The performance is evaluated and the proposed method achieves 100 fake detection in a given DoF(Depth of Field).", "ref_function": ["background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_56": {"mid": "2591381994", "abstract": "A detent apparatus for maintaining a draw bar in a centered position includes an indentation formed in a cylindrical surface of the pivotable taileye of the draw bar structure. A roller is arranged to ride along the surface and into the indentation, the roller being urged into the indentation by symmetrically arranged levers, pivoted as first class levers, with the inner, shorter arm of each lever urging the roller and the outer arm being urged by compression coil springs which are seated on the anchorage structure attached to a railroad car.", "ref_function": ["background", "method"], "cite_purpose": ["background"]}, "@cite_24": {"mid": "2159270577", "abstract": "Current face biometric systems are vulnerable to spoofing attacks. A spoofing attack occurs when a person tries to masquerade as someone else by falsifying data and thereby gaining illegitimate access. Inspired by image quality assessment, characterisation of printing artefacts and differences in light reflection, the authors propose to approach the problem of spoofing detection from texture analysis point of view. Indeed, face prints usually contain printing quality defects that can be well detected using texture and local shape features. Hence, the authors present a novel approach based on analysing facial image for detecting whether there is a live person in front of the camera or a face print. The proposed approach analyses the texture and gradient structures of the facial images using a set of low-level feature descriptors, fast linear classification scheme and score level fusion. Compared to many previous works, the authors proposed approach is robust and does not require user-cooperation. In addition, the texture features that are used for spoofing detection can also be used for face recognition. This provides a unique feature space for coupling spoofing detection and face recognition. Extensive experimental analysis on three publicly available databases showed excellent results compared to existing works.", "ref_function": ["background", "background", "objective", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "1996664229", "abstract": "Personal identity verification based on biometrics has received increasing attention since it allows reliable authentication through intrinsic characteristics, such as face, voice, iris, fingerprint, and gait. Particularly, face recognition techniques have been used in a number of applications, such as security surveillance, access control, crime solving, law enforcement, among others. To strengthen the results of verification, biometric systems must be robust against spoofing attempts with photographs or videos, which are two common ways of bypassing a face recognition system. In this paper, we describe an anti-spoofing solution based on a set of low-level feature descriptors capable of distinguishing between \u2018live\u2019 and \u2018spoof\u2019 images and videos. The proposed method explores both spatial and temporal information to learn distinctive characteristics between the two classes. Experiments conducted to validate our solution with datasets containing images and videos show results comparable to state-of-the-art approaches.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_23": {"mid": "2042883034", "abstract": "Spoofing attacks mainly include printing artifacts, electronic screens and ultra-realistic face masks or models. In this paper, we propose a component-based face coding approach for liveness detection. The proposed method consists of four steps: (1) locating the components of face; (2) coding the low-level features respectively for all the components; (3) deriving the high-level face representation by pooling the codes with weights derived from Fisher criterion; (4) concatenating the histograms from all components into a classifier for identification. The proposed framework makes good use of micro differences between genuine faces and fake faces. Meanwhile, the inherent appearance differences among different components are retained. Extensive experiments on three published standard databases demonstrate that the method can achieve the best liveness detection performance in three databases.", "ref_function": ["background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2163352848", "abstract": "Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "2151343288", "abstract": "We present a real-time liveness detection approach against photograph spoofing in face recognition, by recognizing spontaneous eyeblinks, which is a non-intrusive manner. The approach requires no extra hardware except for a generic webcamera. Eyeblink sequences often have a complex underlying structure. We formulate blink detection as inference in an undirected conditional graphical framework, and are able to learn a compact and efficient observation and transition potentials from data. For purpose of quick and accurate recognition of the blink behavior, eye closity, an easily-computed discriminative measure derived from the adaptive boosting algorithm, is developed, and then smoothly embedded into the conditional model. An extensive set of experiments are presented to show effectiveness of our approach and how it outperforms the cascaded Adaboost and HMM in task of eyeblink detection.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_51": {"mid": "1994030682", "abstract": "Abstract In recent years, face recognition has often been proposed for personal identification. However, there are many difficulties with face recognition systems. For example, an imposter could login the face recognition system by stealing the facial photograph of a person registered on the facial recognition system. The security of the face recognition system requires a live detection system to prevent system login using photographs of a human face. This paper describes an effective, efficient face live detection method which uses physiological motion detected by estimating the eye blinks from a captured video sequence and an eye contour extraction algorithm. This technique uses the conventional active shape model with a random forest classifier trained to recognize the local appearance around each landmark. This local match provides more robustness for optimizing the fitting procedure. Tests show that this face live detection approach successfully discriminates a live human face from a photograph of the registered person's face to increase the face recognition system reliability.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2341318667", "abstract": "Research on non-intrusive software-based face spoofing detection schemes has been mainly focused on the analysis of the luminance information of the face images, hence discarding the chroma component, which can be very useful for discriminating fake faces from genuine ones. This paper introduces a novel and appealing approach for detecting face spoofing using a colour texture analysis. We exploit the joint colour-texture information from the luminance and the chrominance channels by extracting complementary low-level feature descriptions from different colour spaces. More specifically, the feature histograms are computed over each image band separately. Extensive experiments on the three most challenging benchmark data sets, namely, the CASIA face anti-spoofing database, the replay-attack database, and the MSU mobile face spoof database, showed excellent results compared with the state of the art. More importantly, unlike most of the methods proposed in the literature, our proposed approach is able to achieve stable performance across all the three benchmark data sets. The promising results of our cross-database evaluation suggest that the facial colour texture representation is more stable in unknown conditions compared with its gray-scale counterparts.", "ref_function": ["background", "objective", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["CNN-based methods @cite_36 @cite_40 @cite_20 @cite_5 @cite_52 @cite_3 have been presented recently in the face PAD community.", "They treat face PAD as a binary classification problem and achieve remarkable improvements in the intra-testing.", "Liu al @cite_26 design a network architecture to leverage two auxiliary information (Depth map and rPPG signal) as supervision.", "Amin al @cite_3 introduce a new perspective for solving the face anti-spoofing by inversely decomposing a spoof face into the live face and the spoof noise pattern.", "However, they exhibit a poor generalization ability in the cross-testing due to the over-fitting to training data.", "This problem remains open, although some works @cite_40 @cite_20 adopt transfer learning to train a CNN model from ImageNet @cite_18 .", "These works show the need of a larger PAD dataset."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: about results"], "target_paper": "Face anti-spoofing is essential to prevent face recognition systems from a security breach. Much of the progresses have been made by the availability of face anti-spoofing benchmark datasets in recent years. However, existing face anti-spoofing benchmarks have limited number of subjects ( @math ) and modalities ( @math ), which hinder the further development of the academic community. To facilitate face anti-spoofing research, we introduce a large-scale multi-modal dataset, namely CASIA-SURF, which is the largest publicly available dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of @math subjects with @math videos and each sample has @math modalities (i.e., RGB, Depth and IR). We also provide comprehensive evaluation metrics, diverse evaluation protocols, training validation testing subsets and a measurement tool, developing a new benchmark for face anti-spoofing. Moreover, we present a novel multi-modal multi-scale fusion method as a strong baseline, which performs feature re-weighting to select the more informative channel features while suppressing the less useful ones for each modality across different scales. Extensive experiments have been conducted on the proposed dataset to verify its significance and generalization capability. The dataset is available at this https URL", "reference": {"@cite_18": {"mid": "2108598243", "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.", "ref_function": ["background", "background", "objective", "objective", "result", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_36": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_52": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2950744208", "abstract": "Many prior face anti-spoofing works develop discriminative models for recognizing the subtle differences between live and spoof faces. Those approaches often regard the image as an indivisible unit, and process it holistically, without explicit modeling of the spoofing process. In this work, motivated by the noise modeling and denoising algorithms, we identify a new problem of face de-spoofing, for the purpose of anti-spoofing: inversely decomposing a spoof face into a spoof noise and a live face, and then utilizing the spoof noise for classification. A CNN architecture with proper constraints and supervisions is proposed to overcome the problem of having no ground truth for the decomposition. We evaluate the proposed method on multiple face anti-spoofing databases. The results show promising improvements due to our spoof noise modeling. Moreover, the estimated spoof noise provides a visualization which helps to understand the added spoof noise by each spoof medium.", "ref_function": ["background", "background", "method", "method", "result", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_40": {"mid": "2578178601", "abstract": "Recently deep Convolutional Neural Networks have been successfully applied in many computer vision tasks and achieved promising results. So some works have introduced the deep learning into face anti-spoofing. However, most approaches just use the final fully-connected layer to distinguish the real and fake faces. Inspired by the idea of each convolutional kernel can be regarded as a part filter, we extract the deep partial features from the convolutional neural network (CNN) to distinguish the real and fake faces. In our prosed approach, the CNN is fine-tuned firstly on the face spoofing datasets. Then, the block principle component analysis (PCA) method is utilized to reduce the dimensionality of features that can avoid the over-fitting problem. Lastly, the support vector machine (SVM) is employed to distinguish the real the real and fake faces. The experiments evaluated on two public available databases, Replay-Attack and CASIA, show the proposed method can obtain satisfactory results compared to the state-of-the-art methods.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_5": {"mid": "1704933117", "abstract": "Though having achieved some progresses, the hand-crafted texture features, e.g., LBP [23], LBP-TOP [11] are still unable to capture the most discriminative cues between genuine and fake faces. In this paper, instead of designing feature by ourselves, we rely on the deep convolutional neural network (CNN) to learn features of high discriminative ability in a supervised manner. Combined with some data pre-processing, the face anti-spoofing performance improves drastically. In the experiments, over 70 relative decrease of Half Total Error Rate (HTER) is achieved on two challenging datasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art. Meanwhile, the experimental results from inter-tests between two datasets indicates CNN can obtain features with better generalization ability. Moreover, the nets trained using combined data from two datasets have less biases between two datasets.", "ref_function": ["background", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2418633638", "abstract": "With the wide deployment of the face recognition systems in applications from deduplication to mobile device unlocking, security against the face spoofing attacks requires increased attention; such attacks can be easily launched via printed photos, video replays, and 3D masks of a face. We address the problem of face spoof detection against the print (photo) and replay (photo or video) attacks based on the analysis of image distortion ( e.g. , surface reflection, moire pattern, color distortion, and shape deformation) in spoof face images (or video frames). The application domain of interest is smartphone unlock, given that the growing number of smartphones have the face unlock and mobile payment capabilities. We build an unconstrained smartphone spoof attack database (MSU USSA) containing more than 1000 subjects. Both the print and replay attacks are captured using the front and rear cameras of a Nexus 5 smartphone. We analyze the image distortion of the print and replay attacks using different: 1) intensity channels (R, G, B, and grayscale); 2) image regions (entire image, detected face, and facial component between nose and chin); and 3) feature descriptors. We develop an efficient face spoof detection system on an Android smartphone. Experimental results on the public-domain Idiap Replay-Attack, CASIA FASD, and MSU-MFSD databases, and the MSU USSA database show that the proposed approach is effective in face spoof detection for both the cross-database and intra-database testing scenarios. User studies of our Android face spoof detection system involving 20 participants show that the proposed approach works very well in real application scenarios.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["One way to visualize evidence of a class using deep learning is to perform backpropagation of the outputs of a trained classifier @cite_1 .", "@cite_5 , for example, a model is trained to predict the presence of 14 diseases in chest x-rays, and class activation maps @cite_2 are used to show what regions of the x-rays have a larger influence on the classifier's decision.", "However, as shown in @cite_7 , these methods suffer from low resolution or from highlighting limited regions of the original images."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Knowledge of what spatial elements of medical images deep learning methods use as evidence is important for model interpretability, trustiness, and validation. There is a lack of such techniques for models in regression tasks. We propose a method, called visualization for regression with a generative adversarial network (VR-GAN), for formulating adversarial training specifically for datasets containing regression target values characterizing disease severity. We use a conditional generative adversarial network where the generator attempts to learn to shift the output of a regressor through creating disease effect maps that are added to the original images. Meanwhile, the regressor is trained to predict the original regression value for the modified images. A model trained with this technique learns to provide visualization for how the image would appear at different stages of the disease. We analyze our method in a dataset of chest x-rays associated with pulmonary function tests, used for diagnosing chronic obstructive pulmonary disease (COPD). For validation, we compute the difference of two registered x-rays of the same patient at different time points and correlate it to the generated disease effect map. The proposed method outperforms a technique based on classification and provides realistic-looking images, making modifications to images following what radiologists usually observe for this disease. Implementation code is available at this https URL.", "reference": {"@cite_5": {"mid": "2770241596", "abstract": "We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2785760873", "abstract": "Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2963635991", "abstract": "Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["motivation"]}, "@cite_2": {"mid": "2295107390", "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1 top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["@cite_7 , researchers visualize what brain MRIs of patients with mild cognitive impairment would look like if they developed Alzheimer's disease, generating disease effect maps.", "To solve problems with other visualization methods, they propose an adversarial setup.", "A generator is trained to modify an input image which fools a discriminator.", "The modifications the generator outputs are used as visualization of evidence of one class.", "This setup inspires our method.", "However, instead of classification labels, we use regression values and a novel loss function."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references", "Describing used methods"], "target_paper": "Knowledge of what spatial elements of medical images deep learning methods use as evidence is important for model interpretability, trustiness, and validation. There is a lack of such techniques for models in regression tasks. We propose a method, called visualization for regression with a generative adversarial network (VR-GAN), for formulating adversarial training specifically for datasets containing regression target values characterizing disease severity. We use a conditional generative adversarial network where the generator attempts to learn to shift the output of a regressor through creating disease effect maps that are added to the original images. Meanwhile, the regressor is trained to predict the original regression value for the modified images. A model trained with this technique learns to provide visualization for how the image would appear at different stages of the disease. We analyze our method in a dataset of chest x-rays associated with pulmonary function tests, used for diagnosing chronic obstructive pulmonary disease (COPD). For validation, we compute the difference of two registered x-rays of the same patient at different time points and correlate it to the generated disease effect map. The proposed method outperforms a technique based on classification and provides realistic-looking images, making modifications to images following what radiologists usually observe for this disease. Implementation code is available at this https URL.", "reference": {"@cite_7": {"mid": "2963635991", "abstract": "Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["There have been other works on generating visual attribution for regression.", "@cite_4 , start by training a GAN on a large dataset of frontal x-rays, and then train an encoder that maps from an x-ray to its latent space vector.", "Finally, train a small model for regression that receives the latent vector of the images from a smaller dataset and outputs a value which is used for diagnosing congestive heart failure.", "To interpret their model, they backpropagate through the small regression model, taking steps in the latent space to reach the threshold of diagnosis, and generate the image associated with the new diagnosis."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Knowledge of what spatial elements of medical images deep learning methods use as evidence is important for model interpretability, trustiness, and validation. There is a lack of such techniques for models in regression tasks. We propose a method, called visualization for regression with a generative adversarial network (VR-GAN), for formulating adversarial training specifically for datasets containing regression target values characterizing disease severity. We use a conditional generative adversarial network where the generator attempts to learn to shift the output of a regressor through creating disease effect maps that are added to the original images. Meanwhile, the regressor is trained to predict the original regression value for the modified images. A model trained with this technique learns to provide visualization for how the image would appear at different stages of the disease. We analyze our method in a dataset of chest x-rays associated with pulmonary function tests, used for diagnosing chronic obstructive pulmonary disease (COPD). For validation, we compute the difference of two registered x-rays of the same patient at different time points and correlate it to the generated disease effect map. The proposed method outperforms a technique based on classification and provides realistic-looking images, making modifications to images following what radiologists usually observe for this disease. Implementation code is available at this https URL.", "reference": {"@cite_4": {"mid": "2900003150", "abstract": "Generative Visual Rationales can identify imaging features learned by a model trained to predict congestive heart failure from chest radiographs, allowing radiologists to better identify faults and...", "ref_function": ["background"], "cite_purpose": ["background"]}}}
{"sentences": ["There is a similarly limited amount of previous work on humanoid robots playing games against human opponents.", "Notable exceptions include @cite_18 , where the DB humanoid robot learns to play air hockey using a Nearest Neighbour classifier; @cite_9 , where the Nico humanoid torso robot plays the game of rock-paper-scissors using a Wizzard of Oz' setting; @cite_39 , where the Sky humanoid robot plays catch and juggling using inverse kinematics and induced parameters with least squares linear regression; @cite_28 , where the Nao robot plays a quiz game, an arm imitation game, and a dance game using tabular reinforcement learning; @cite_33 , where the Genie humanoid robot plays the poker game using a Wizard of Oz' setting; and @cite_7 , where the NAO robot plays Checkers using a MinMax search tree.", "Most of these robots only exhibit non-verbal abilities and are either teleoperated or based on heuristic methods, which suggests that verbal abilities in autonomous trainable robots playing games are underdeveloped.", "Apart from @cite_4 @cite_24 , we are not aware of any other previous work in humanoid robots playing social games against human opponents and trained with deep learning methods."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Abstract The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games\u2014and use the game of \u2018Noughts and Crosses\u2019 with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the Pepper robot confirms that highly accurate visual perception is required for successful game play.", "reference": {"@cite_18": {"mid": "1965218672", "abstract": "We present a method for humanoid robots to quickly learn new dynamic tasks from observing others and from practice. Ways in which the robot can adapt to initial and also changing conditions are described. Agents are given domain knowledge in the form of task primitives. A key element of our approach is to break learning problems up into as many simple learning problems as possible. We present a case study of a humanoid robot learning to play air hockey.", "ref_function": ["background", "method", "method", "objective", "method"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2559112319", "abstract": "Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses. Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98 of the games. A pilot test of the proposed multimodal system for the targeted game---integrating speech, vision and gestures---reports that reasonable and fluent interactions can be achieved using the proposed approach.", "ref_function": ["background", "method", "objective", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2161062680", "abstract": "This paper describes the study of human behaviors in a poker game with the game playing humanoid robot. Betting decision and nonverbal behaviors of human players were analyzed between human\u2013human and the human\u2013humanoid poker game. It was found that card hand strength is related to the betting strategy and nonverbal interaction. Moreover, engagement in the poker game with the humanoid was assessed through questionnaire and by measuring the nonverbal behaviors between playtime and breaktime.", "ref_function": ["background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2769375497", "abstract": "In search for better technological solutions for education, we adapted a principle from economic game theory, namely that giving a help will promote collaboration and eventually long-term relations between a robot and a child. This principle has been shown to be effective in games between humans and between humans and computer agents. We compared the social and cognitive engagement of children when playing checkers game combined with a social strategy against a robot or against a computer. We found that by combining the social and game strategy the children (average age of 8.3 years) had more empathy and social engagement with the robot since the children did not want to necessarily win against it. This finding is promising for using social strategies for the creation of long-term relations between robots and children and making educational tasks more engaging. An additional outcome of the study was the significant difference in the perception of the children about the difficulty of the game \u2013 the game with the robot was seen as more challenging and the robot \u2013 as a smarter opponent. This finding might be due to the higher perceived or expected intelligence from the robot, or because of the higher complexity of seeing patterns in three-dimensional world.", "ref_function": ["background", "background", "method", "result", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_28": {"mid": "2092252440", "abstract": "For robots to interact effectively with human users they must be capable of coordinated, timely behavior in response to social context. The Adaptive Strategies for Sustainable Long-Term Social Interaction (ALIZ-E) project focuses on the design of long-term, adaptive social interaction between robots and child users in real-world settings. In this paper, we report on the iterative approach taken to scientific and technical developments toward this goal: advancing individual technical competencies and integrating them to form an autonomous robotic system for evaluation \"in the wild.\" The first evaluation iterations have shown the potential of this methodology in terms of adaptation of the robot to the interactant and the resulting influences on engagement. This sets the foundation for an ongoing research program that seeks to develop technologies for social robot companions.", "ref_function": ["background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2084907907", "abstract": "Using a humanoid robot and a simple children's game, we examine the degree to which variations in behavior result in attributions of mental state and intentionality. Participants play the well-known children's game \"rock-paper-scissors\" against a robot that either plays fairly, or that cheats in one of two ways. In the \"verbal cheat\" condition, the robot announces the wrong outcome on several rounds which it loses, declaring itself the winner. In the \"action cheat\"' condition, the robot changes its gesture after seeing its opponent's play. We find that participants display a greater level of social engagement and make greater attributions of mental state when playing against the robot in the conditions in which it cheats.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_39": {"mid": "1989972452", "abstract": "Entertainment robots in theme park environments typically do not allow for physical interaction and contact with guests. However, catching and throwing back objects is one form of physical engagement that still maintains a safe distance between the robot and participants. Using a theme park type animatronic humanoid robot, we developed a test bed for a throwing and catching game scenario. We use an external camera system (ASUS Xtion PRO LIVE) to locate balls and a Kalman filter to predict ball destination and timing. The robot's hand and joint-space are calibrated to the vision coordinate system using a least-squares technique, such that the hand can be positioned to the predicted location. Successful catches are thrown back two and a half meters forward to the participant, and missed catches are detected to trigger suitable animations that indicate failure. Human to robot partner juggling (three ball cascade pattern, one hand for each partner) is also achieved by speeding up the catching throwing cycle. We tested the throwing catching system on six participants (one child and five adults, including one elderly), and the juggling system on three skilled jugglers.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_24": {"mid": "2771590356", "abstract": "Deep reinforcement learning for interactive multimodal robots is attractive for endowing machines with trainable skill acquisition. But this form of learning still represents several challenges. The challenge that we focus in this paper is effective policy learning. To address that, in this paper we compare the Deep Q-Networks (DQN) method against a variant that aims for stronger decisions than the original method by avoiding decisions with the lowest negative rewards. We evaluated our baseline and proposed algorithms in agents playing the game of Noughts and Crosses with two grid sizes (3\u00d73 and 5\u00d75). Experimental results show evidence that our proposed method can lead to more effective policies than the baseline DQN method, which can be used for training interactive social robots.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In the remainder of the article we describe a deep learning-based approach for efficiently training a robot with the ability of behaving with reasonable performance in a near real world deployment.", "In particular, we measure the effectiveness of neural-based game move interpretation and the effectiveness of Deep Q-Networks (DQN) @cite_16 for interactive social robots.", "Field trial results show that the proposed approach can induce reasonable and competitive behaviours, especially when they are not affected by unseen noisy conditions."], "label": ["Describing used methods", "Describing used methods", "Describing the results"], "target_paper": "Abstract The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games\u2014and use the game of \u2018Noughts and Crosses\u2019 with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the Pepper robot confirms that highly accurate visual perception is required for successful game play.", "reference": {"@cite_16": {"mid": "2145339207", "abstract": "An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.", "ref_function": ["objective"], "cite_purpose": ["uses"]}}}
{"sentences": ["Top-down methods @cite_19 @cite_26 @cite_28 @cite_0 @cite_21 @cite_16 @cite_7 @cite_30 detect a single person keypoints within a person bounding box.", "The person bounding boxes are usually generated by an object detector @cite_3 @cite_18 @cite_6 .", "Mask R-CNN @cite_0 directly adds a keypoint detection branch on Faster R-CNN @cite_3 and reuses features after ROIPooling.", "G-RMI @cite_28 and the following methods further break top-down methods into two steps and use separate models for person detection and pose estimation."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken"], "target_paper": "In this paper, we are interested in bottom-up multi-person human pose estimation. A typical bottom-up pipeline consists of two main steps: heatmap prediction and keypoint grouping. We mainly focus on the first step for improving heatmap prediction accuracy. We propose Higher-Resolution Network (HigherHRNet), which is a simple extension of the High-Resolution Network (HRNet). HigherHRNet generates higher-resolution feature maps by deconvolving the high-resolution feature maps outputted by HRNet, which are spatially more accurate for small and medium persons. Then, we build high-quality multi-level features and perform multi-scale pose prediction. The extra computation overhead is marginal and negligible in comparison to existing bottom-up methods that rely on multi-scale image pyramids or large input image size to generate accurate pose heatmaps. HigherHRNet surpasses all existing bottom-up methods on the COCO dataset without using multi-scale test. The code and models will be released.", "reference": {"@cite_30": {"mid": "2307770531", "abstract": "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a \u201cstacked hourglass\u201d network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.", "ref_function": ["objective", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2565639579", "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2916798096", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2964221239", "abstract": "The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these \"hard\" keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the \"simple\" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the \"hard\" keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19 relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_28": {"mid": "2578797046", "abstract": "We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. In the first stage, we predict the location and scale of boxes which are likely to contain people, for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5 absolute improvement compared to the previous best performing method on the same dataset.", "ref_function": ["method", "method", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_21": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2613718673", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2 mAP) and 2012 (70.4 mAP) using 300 proposals per image. Code is available at https: github.com ShaoqingRen faster_rcnn.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "other"], "cite_purpose": ["background", ""]}, "@cite_6": {"mid": "2962731685", "abstract": "Recent region-based object detectors are usually built with separate classification and localization branches on top of shared feature extraction networks. In this paper, we analyze failure cases of state-of-the-art detectors and observe that most hard false positives result from classification instead of localization. We conjecture that: (1) Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization; (2) multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks; (3) large receptive field for different scales leads to redundant context information for small objects. We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. DCR samples hard false positives from the base classifier in Faster RCNN and trains a RCNN-styled strong classifier. Experiments show new state-of-the-art results on PASCAL VOC and COCO without any bells and whistles.", "ref_function": ["background", "background", "method", "result", "method", "result"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", ""]}, "@cite_19": {"mid": "2963402313", "abstract": "There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https: github.com leoxiaobin pose.pytorch.", "ref_function": ["background", "background", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2963781481", "abstract": "Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76:7 mAP on the MPII (multi person) dataset[3]. Our model and source codes are made publicly available.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Bottom-up methods @cite_11 @cite_27 @cite_8 @cite_17 @cite_12 detect identity-free body joints for all the persons in an image and then group them into individuals.", "OpenPose @cite_17 uses a two-branch multi-stage netork with one branch for heatmap prediction and one branch for grouping.", "OpenPose uses a grouping method named part affinity field which learns a 2D vector field linking two keypoints.", "Grouping is done by calculating line integral between two keypoints and group the pair with the largest integral.", "Newell @cite_12 use stacked hourglass network @cite_30 for both heatmap prediction and grouping.", "Grouping is done by a method named associate embedding, which assigns each keypoint with a tag'' (a vector representation) and groups keypoints based on the @math distance between tag vectors.", "PersonLab @cite_1 uses dilated ResNet @cite_5 and groups keypoints by directly learning a 2D offset field for each pair of keypoints."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "In this paper, we are interested in bottom-up multi-person human pose estimation. A typical bottom-up pipeline consists of two main steps: heatmap prediction and keypoint grouping. We mainly focus on the first step for improving heatmap prediction accuracy. We propose Higher-Resolution Network (HigherHRNet), which is a simple extension of the High-Resolution Network (HRNet). HigherHRNet generates higher-resolution feature maps by deconvolving the high-resolution feature maps outputted by HRNet, which are spatially more accurate for small and medium persons. Then, we build high-quality multi-level features and perform multi-scale pose prediction. The extra computation overhead is marginal and negligible in comparison to existing bottom-up methods that rely on multi-scale image pyramids or large input image size to generate accurate pose heatmaps. HigherHRNet surpasses all existing bottom-up methods on the COCO dataset without using multi-scale test. The code and models will be released.", "reference": {"@cite_30": {"mid": "2307770531", "abstract": "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a \u201cstacked hourglass\u201d network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.", "ref_function": ["objective", "background", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_11": {"mid": "2175012183", "abstract": "This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation1.", "ref_function": ["background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2509865052", "abstract": "Despite of the recent success of neural networks for human pose estimation, current approaches are limited to pose estimation of a single person and cannot handle humans in groups or crowds. In this work, we propose a method that estimates the poses of multiple persons in an image in which a person can be occluded by another person or might be truncated. To this end, we consider multi-person pose estimation as a joint-to-person association problem. We construct a fully connected graph from a set of detected joint candidates in an image and resolve the joint-to-person association and outlier detection using integer linear programming. Since solving joint-to-person association jointly for all persons in an image is an NP-hard problem and even approximations are expensive, we solve the problem locally for each person. On the challenging MPII Human Pose Dataset for multiple persons, our approach achieves the accuracy of a state-of-the-art method, but it is 6,000 to 19,000 times faster.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2962773068", "abstract": "We present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model. The proposed PersonLab model tackles both semantic-level reasoning and object-part associations using part-based modeling. Our model employs a convolutional network which learns to detect individual keypoints and predict their relative displacements, allowing us to group keypoints into person pose instances. Further, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. Our system is based on a fully-convolutional architecture and allows for efficient inference, with runtime essentially independent of the number of people present in the scene. Trained on COCO data alone, our system achieves COCO test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, significantly outperforming all previous bottom-up pose estimation systems. We are also the first bottom-up method to report competitive results for the person class in the COCO instance segmentation task, achieving a person category average precision of 0.417.", "ref_function": ["background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "2382036597", "abstract": "The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at http: pose.mpi-inf.mpg.de).", "ref_function": ["objective", "objective", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2194775991", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57 error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "ref_function": ["background", "background", "method", "result", "result", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2952819818", "abstract": "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to both multi-person pose estimation and instance segmentation and report state-of-the-art performance for multi-person pose on the MPII and MS-COCO datasets.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background", "uses"]}, "@cite_17": {"mid": "2559085405", "abstract": "We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.", "ref_function": ["objective", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["There are mainly 4 methods to generate high resolution feature maps.", "(1) Encoder-decoder @cite_30 @cite_0 @cite_7 @cite_32 @cite_13 @cite_23 @cite_29 captures the context information in the encoder path and recover high resolution features in the decoder path.", "The decoder usually contains a sequence of bilinear upsample operations with skip connections from encoder features with the same resolution.", "(2) Dilated convolution @cite_14 @cite_31 @cite_2 @cite_15 @cite_33 @cite_24 @cite_34 @cite_9 ( atrous'' convolution) is used to remove several stride convolutions max poolings to preserve feature map resolution.", "Dilated convolution prevents losing spatial information but introduces more computational cost.", "(3) Deconvolution (transposed convolution) @cite_19 is used in sequence at the end of a network to efficiently increase feature map resolution.", "SimpleBaseline @cite_19 demonstrates that deconvolution can generate high quality feature maps for heatmap prediction.", "(4) Recently, a High-Resolution Network (HRNet) @cite_26 is proposed as an efficient way to keep a high resolution pass throughout the network.", "HRNet @cite_26 consists of multiple branches with different resolutions.", "Lower resolution branches capture contextual information and higher resolution branches preserve spatial information.", "With multi-scale fusions between branches, HRNet @cite_26 can generate high resolution feature maps with rich semantic."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "In this paper, we are interested in bottom-up multi-person human pose estimation. A typical bottom-up pipeline consists of two main steps: heatmap prediction and keypoint grouping. We mainly focus on the first step for improving heatmap prediction accuracy. We propose Higher-Resolution Network (HigherHRNet), which is a simple extension of the High-Resolution Network (HRNet). HigherHRNet generates higher-resolution feature maps by deconvolving the high-resolution feature maps outputted by HRNet, which are spatially more accurate for small and medium persons. Then, we build high-quality multi-level features and perform multi-scale pose prediction. The extra computation overhead is marginal and negligible in comparison to existing bottom-up methods that rely on multi-scale image pyramids or large input image size to generate accurate pose heatmaps. HigherHRNet surpasses all existing bottom-up methods on the COCO dataset without using multi-scale test. The code and models will be released.", "reference": {"@cite_30": {"mid": "2307770531", "abstract": "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a \u201cstacked hourglass\u201d network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.", "ref_function": ["objective", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_14": {"mid": "2286929393", "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_26": {"mid": "2916798096", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background", "background"]}, "@cite_33": {"mid": "2964309882", "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89 and 82.1 without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: github.com tensorflow models tree master research deeplab.", "ref_function": ["background", "background", "objective", "method", "method", "result", "other"], "cite_purpose": ["uses"]}, "@cite_7": {"mid": "2964221239", "abstract": "The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these \"hard\" keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the \"simple\" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the \"hard\" keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19 relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2630837129", "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_29": {"mid": "2738804062", "abstract": "Many machine vision applications require predictions for every pixel of the input image (for example semantic segmentation, boundary detection). Models for such problems usually consist of encoders which decreases spatial resolution while learning a high-dimensional representation, followed by decoders who recover the original input resolution and result in low-dimensional predictions. While encoders have been studied rigorously, relatively few studies address the decoder side. Therefore this paper presents an extensive comparison of a variety of decoders for a variety of pixel-wise prediction tasks. Our contributions are: (1) Decoders matter: we observe significant variance in results between different types of decoders on various problems. (2) We introduce a novel decoder: bilinear additive upsampling. (3) We introduce new residual-like connections for decoders. (4) We identify two decoder types which give a consistently high performance.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2911831070", "abstract": "We present a single-shot, bottom-up approach for whole image parsing. Whole image parsing, also known as Panoptic Segmentation, generalizes the tasks of semantic segmentation for 'stuff' classes and instance segmentation for 'thing' classes, assigning both semantic and instance labels to every pixel in an image. Recent approaches to whole image parsing typically employ separate standalone modules for the constituent semantic and instance segmentation tasks and require multiple passes of inference. Instead, the proposed DeeperLab image parser performs whole image parsing with a significantly simpler, fully convolutional approach that jointly addresses the semantic and instance segmentation tasks in a single-shot manner, resulting in a streamlined system that better lends itself to fast processing. For quantitative evaluation, we use both the instance-based Panoptic Quality (PQ) metric and the proposed region-based Parsing Covering (PC) metric, which better captures the image parsing quality on 'stuff' classes and larger object instances. We report experimental results on the challenging Mapillary Vistas dataset, in which our single model achieves 31.95 (val) 31.6 PQ (test) and 55.26 PC (val) with 3 frames per second (fps) on GPU or near real-time speed (22.6 fps on GPU) with reduced accuracy.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_32": {"mid": "2952232639", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "other"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_24": {"mid": "2891778567", "abstract": "The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7 on Cityscapes (street scene parsing), 71.3 on PASCAL-Person-Part (person-part segmentation), and 87.9 on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_19": {"mid": "2963402313", "abstract": "There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https: github.com leoxiaobin pose.pytorch.", "ref_function": ["background", "background", "method", "method", "method", "other"], "cite_purpose": ["background", "background"]}, "@cite_23": {"mid": "2563705555", "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2412782625", "abstract": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First , we highlight convolution with upsampled filters, or \u2018atrous convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second , we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third , we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.", "ref_function": ["objective", "method", "method", "method", "method", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["uses"]}, "@cite_31": {"mid": "1923697677", "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6 IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.", "ref_function": ["background", "objective", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_34": {"mid": "2963136578", "abstract": "", "ref_function": [], "cite_purpose": ["uses"]}, "@cite_13": {"mid": "2963881378", "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http: mi.eng.cam.ac.uk projects segnet .", "ref_function": ["background", "background", "method", "objective", "method", "method", "method", "result", "background", "background", "background", "objective", "method", "method", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Before deep learning became popular, most of the traditional CV algorithm variants apply shallow hand-crafted features to solve action recognition.", "Improved Dense Trajectories (IDT) @cite_36 which uses densely sampled trajectory features indicates that the temporal information could be processed differently from that of spatial information.", "Instead of extending the Harris corner detector into 3D, it utilizes the warp optical flow field to obtain some trajectories and eliminate the effects of camera motion in the video sequence.", "For each tracker corner hand-crafted features, like HOF, HOG, and MBH, are extracted along the trajectory.", "Despite their excellent performance, IDT and its improvements @cite_2 , @cite_37 , @cite_6 are still computationally formidable and become intractable on large-scale datasets."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Spatial and temporal stream model has gained great success in video action recognition. Most existing works pay more attention to designing effective features fusion methods, which train the two-stream model in a separate way. However, it's hard to ensure discriminability and explore complementary information between different streams in existing works. In this work, we propose a novel cooperative cross-stream network that investigates the conjoint information in multiple different modalities. The jointly spatial and temporal stream networks feature extraction is accomplished by an end-to-end learning manner. It extracts this complementary information of different modality from a connection block, which aims at exploring correlations of different stream features. Furthermore, different from the conventional ConvNet that learns the deep separable features with only one cross-entropy loss, our proposed model enhances the discriminative power of the deeply learned features and reduces the undesired modality discrepancy by jointly optimizing a modality ranking constraint and a cross-entropy loss for both homogeneous and heterogeneous modalities. The modality ranking constraint constitutes intra-modality discriminative embedding and inter-modality triplet constraint, and it reduces both the intra-modality and cross-modality feature variations. Experiments on three benchmark datasets demonstrate that by cooperating appearance and motion feature extraction, our method can achieve state-of-the-art or competitive performance compared with existing results.", "reference": {"@cite_36": {"mid": "2105101328", "abstract": "Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_37": {"mid": "2500355674", "abstract": "A fast reference frame selection algorithm for the HEVC encoder is proposed.A relationship between the content similarity and reference frame selection is derived.The content similarity is studied without any extra computational complexity.Experimental results show that the proposed algorithm efficiently removes the encoding complexity of the best reference frame decision process. The high efficiency video coding (HEVC) is the state-of-the-art video coding standard, which achieves about 50 bit rate saving while maintaining the same visual quality as compared to the H.264 AVC. This achieved coding efficiency benefits from a set of advanced coding tools, such as the multiple reference frames (MRF) based interframe prediction, which efficiently improves the coding efficiency of the HEVC encoder, while it also increases heavy computation into the HEVC encoder. The high encoding complexity becomes a bottleneck for the high definition videos and HEVC encoder to be widely used in real-time and low power multimedia applications. In this paper, we propose a content similarity based fast reference frame selection algorithm for reducing the computational complexity of the multiple reference frames based interframe prediction. Based the large content similarity between the parent prediction unit (Inter_2N2N) and the children prediction units (Inter_2NN, Inter_N2N, Inter_NN, Inter_2NnU, Inter_2NnD, Inter_nL2N, and Inter_nR2N), the reference frame selection information of the children prediction units are obtained by learning the results of their parent prediction unit. Experimental results show that the proposed algorithm can reduce about 54.29 and 43.46 MRF encoding time saving for the low-delay-main and random-access-main coding structures, respectively, while the rate distortion performance degradation is negligible.", "ref_function": ["background", "background", "background", "background", "objective", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_6": {"mid": "2766724094", "abstract": "Recently, deep neural networks based hashing methods have greatly improved the multimedia retrieval performance by simultaneously learning feature representations and binary hash functions. Inspired by the latest advance in the asymmetric hashing scheme, in this work, we propose a novel Deep Asymmetric Pairwise Hashing approach (DAPH) for supervised hashing. The core idea is that two deep convolutional models are jointly trained such that their output codes for a pair of images can well reveal the similarity indicated by their semantic labels. A pairwise loss is elaborately designed to preserve the pairwise similarities between images as well as incorporating the independence and balance hash code learning criteria. By taking advantage of the flexibility of asymmetric hash functions, we devise an efficient alternating algorithm to optimize the asymmetric deep hash functions and high-quality binary code jointly. Experiments on three image benchmarks show that DAPH achieves the state-of-the-art performance on large-scale image retrieval.", "ref_function": ["background", "objective", "method", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_2": {"mid": "1871385855", "abstract": "A Comprehensive study on the BoVW pipeline for action recognition task.An evaluation and a generic analysis on 13 encoding methods.Intra-normalization for supervector based encoding methods.An evaluation on three fusion methods.Several good practices of the BoVW pipeline for action recognition task. Video based action recognition is one of the important and challenging problems in computer vision research. Bag of visual words model (BoVW) with local features has been very popular for a long time and obtained the state-of-the-art performance on several realistic datasets, such as the HMDB51, UCF50, and UCF101. BoVW is a general pipeline to construct a global representation from local features, which is mainly composed of five steps; (i) feature extraction, (ii) feature pre-processing, (iii) codebook generation, (iv) feature encoding, and (v) pooling and normalization. Although many efforts have been made in each step independently in different scenarios, their effects on action recognition are still unknown. Meanwhile, video data exhibits different views of visual patterns , such as static appearance and motion dynamics. Multiple descriptors are usually extracted to represent these different views. Fusing these descriptors is crucial for boosting the final performance of an action recognition system. This paper aims to provide a comprehensive study of all steps in BoVW and different fusion methods, and uncover some good practices to produce a state-of-the-art action recognition system. Specifically, we explore two kinds of local features, ten kinds of encoding methods, eight kinds of pooling and normalization strategies, and three kinds of fusion methods. We conclude that every step is crucial for contributing to the final recognition rate and improper choice in one of the steps may counteract the performance improvement of other steps. Furthermore, based on our comprehensive study, we propose a simple yet effective representation, called hybrid supervector, by exploring the complementarity of different BoVW frameworks with improved dense trajectories. Using this representation, we obtain impressive results on the three challenging datasets; HMDB51 (61.9 ), UCF50 (92.3 ), and UCF101 (87.9 ).", "ref_function": ["background", "background", "background", "method", "result", "result", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}}}
{"sentences": ["An activate research which devotes to the design of deep networks for video representation learning has been trying to devise effective ConvNet architectures @cite_40 @cite_3 @cite_19 @cite_3 @cite_23 .", "@cite_40 attempt to design a deep network which stacks CNN-based frame-level features in a fixed size and then conduct spatiotemporal convolutions for video-level features learning.", "However, the results which implied the difficulty of CNNs in capturing motion information of the video is not satisfied.", "Later, many works in this genre leverage ConvNets trained on frames to extract low-level features an then perform high-level temporal integration of those features using pooling @cite_28 @cite_30 , high-dimensional feature encoding @cite_26 @cite_21 , or recurrent neural networks @cite_23 @cite_22 @cite_3 @cite_32 .", "Recently, the CNN-LSTM frameworks @cite_23 @cite_22 , using stacked LSTM network to connect frame-level representation and exploring long-term temporal relationships of video for learning a more robust representation, have yielded an improvement for modeling temporal dynamics of convolution features in videos.", "However, this genre using CNN as an encoder and RNN as a decoder of the video will lose low-level temporal context which is essential for action recognition."], "label": ["General reference to previous research or scholarship: research objective", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies"], "target_paper": "Spatial and temporal stream model has gained great success in video action recognition. Most existing works pay more attention to designing effective features fusion methods, which train the two-stream model in a separate way. However, it's hard to ensure discriminability and explore complementary information between different streams in existing works. In this work, we propose a novel cooperative cross-stream network that investigates the conjoint information in multiple different modalities. The jointly spatial and temporal stream networks feature extraction is accomplished by an end-to-end learning manner. It extracts this complementary information of different modality from a connection block, which aims at exploring correlations of different stream features. Furthermore, different from the conventional ConvNet that learns the deep separable features with only one cross-entropy loss, our proposed model enhances the discriminative power of the deeply learned features and reduces the undesired modality discrepancy by jointly optimizing a modality ranking constraint and a cross-entropy loss for both homogeneous and heterogeneous modalities. The modality ranking constraint constitutes intra-modality discriminative embedding and inter-modality triplet constraint, and it reduces both the intra-modality and cross-modality feature variations. Experiments on three benchmark datasets demonstrate that by cooperating appearance and motion feature extraction, our method can achieve state-of-the-art or competitive performance compared with existing results.", "reference": {"@cite_30": {"mid": "2884883933", "abstract": "Adversarial perturbations are noise-like patterns that can subtly change the data, while failing an otherwise accurate classifier. In this paper, we propose to use such perturbations for improving the robustness of video representations. To this end, given a well-trained deep model for per-frame video recognition, we first generate adversarial noise adapted to this model. Using the original data features from the full video sequence and their perturbed counterparts, as two separate bags, we develop a binary classification problem that learns a set of discriminative hyperplanes \u2013 as a subspace \u2013 that will separate the two bags from each other. This subspace is then used as a descriptor for the video, dubbed discriminative subspace pooling. As the perturbed features belong to data classes that are likely to be confused with the original features, the discriminative subspace will characterize parts of the feature space that are more representative of the original data, and thus may provide robust video representations. To learn such descriptors, we formulate a subspace learning objective on the Stiefel manifold and resort to Riemannian optimization methods for solving it efficiently. We provide experiments on several video datasets and demonstrate state-of-the-art results.", "ref_function": ["background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2608988379", "abstract": "In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks [42] with learnable spatio-temporal feature aggregation [6]. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that: (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13 relative) as well as outperforms other baselines with comparable base architectures on HMDB51, UCF101, and Charades video classification benchmarks.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}, "@cite_28": {"mid": "2962710043", "abstract": "Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action-indeed, many are common across multiple actions-pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2556024076", "abstract": "The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space, (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification, and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "1923404803", "abstract": "Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1 vs. 60.9 ) and the UCF-101 datasets with (88.6 vs. 88.0 ) and without additional optical flow information (82.6 vs. 73.0 ).", "ref_function": ["background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2235034809", "abstract": "Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7 ) and HMDB51 (67.2 ).", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_19": {"mid": "2745519816", "abstract": "Learning image representations with ConvNets by pre-training on ImageNet has proven useful across many visual understanding tasks including object detection, semantic segmentation, and image captioning. Although any image representation can be applied to video frames, a dedicated spatiotemporal representation is still vital in order to incorporate motion patterns that cannot be captured by appearance based models alone. This paper presents an empirical ConvNet architecture search for spatiotemporal feature learning, culminating in a deep 3-dimensional (3D) Residual ConvNet. Our proposed architecture outperforms C3D by a good margin on Sports-1M, UCF101, HMDB51, THUMOS14, and ASLAN while being 2 times faster at inference time, 2 times smaller in model size, and having a more compact representation.", "ref_function": ["background", "background", "objective", "result"], "cite_purpose": ["background"]}, "@cite_40": {"mid": "2016053056", "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3 to 63.9 ), but only a surprisingly modest improvement compared to single-frame models (59.3 to 60.9 ). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3 up from 43.9 ).", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_23": {"mid": "2951183276", "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and or optimized.", "ref_function": ["background", "background", "method", "background", "method", "method", "method", "result"], "cite_purpose": ["background", "background", "background"]}}}
{"sentences": ["These works implied the importance of temporal information for action recognition and the incapability of CNNs to capture such information.", "To exploiting the temporal information, some studies resort to the use of the 3D convolution kernel.", "@cite_12 @cite_19 apply 3D CNN, both appearance and motion features learned with 3D convolution, simultaneously encode spatial and temporal cues.", "Several works explored the effect of performing 3D convolutions over the long-range temporal structure with ConvNets @cite_1 @cite_24 .", "Unfortunately, the network accepts a predefined number of frames as the input, and it's unclear of the right choice of the temporal span.", "What's more, the 3D convolution kernel inevitably has more network parameters.", "Therefore, recent interests have proposed a variant of factorizing a 3D filter into a combination of a 2D and 1D filter, including R(2+1)D'' @cite_34 , Pseudo3D network'' @cite_4 , factorized spatiotemporal convolutional networks'' @cite_46 ."], "label": ["Explain the significance of references", "General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken"], "target_paper": "Spatial and temporal stream model has gained great success in video action recognition. Most existing works pay more attention to designing effective features fusion methods, which train the two-stream model in a separate way. However, it's hard to ensure discriminability and explore complementary information between different streams in existing works. In this work, we propose a novel cooperative cross-stream network that investigates the conjoint information in multiple different modalities. The jointly spatial and temporal stream networks feature extraction is accomplished by an end-to-end learning manner. It extracts this complementary information of different modality from a connection block, which aims at exploring correlations of different stream features. Furthermore, different from the conventional ConvNet that learns the deep separable features with only one cross-entropy loss, our proposed model enhances the discriminative power of the deeply learned features and reduces the undesired modality discrepancy by jointly optimizing a modality ranking constraint and a cross-entropy loss for both homogeneous and heterogeneous modalities. The modality ranking constraint constitutes intra-modality discriminative embedding and inter-modality triplet constraint, and it reduces both the intra-modality and cross-modality feature variations. Experiments on three benchmark datasets demonstrate that by cooperating appearance and motion feature extraction, our method can achieve state-of-the-art or competitive performance compared with existing results.", "reference": {"@cite_4": {"mid": "2963820951", "abstract": "Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing 3D convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating 3 x 3 x 3 convolutions with 1 \u00d7 3 \u00d7 3 convolutional filters on spatial domain (equivalent to 2D CNN) plus 3 \u00d7 1 \u00d7 1 convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net (P3D ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3 and 1.8 , respectively. We further examine the generalization performance of video representation produced by our pre-trained P3D ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques.", "ref_function": ["background", "background", "background", "background", "objective", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2751445731", "abstract": "3-D convolutional neural networks (3-D-convNets) have been very recently proposed for action recognition in videos, and promising results are achieved. However, existing 3-D-convNets has two \u201cartificial\u201d requirements that may reduce the quality of video analysis: 1) It requires a fixed-sized (e.g., 112 @math 112) input video; and 2) most of the 3-D-convNets require a fixed-length input (i.e., video shots with fixed number of frames). To tackle these issues, we propose an end-to-end pipeline named Two-stream 3-D-convNet Fusion , which can recognize human actions in videos of arbitrary size and length using multiple features. Specifically, we decompose a video into spatial and temporal shots. By taking a sequence of shots as input, each stream is implemented using a spatial temporal pyramid pooling (STPP) convNet with a long short-term memory (LSTM) or CNN-E model, softmax scores of which are combined by a late fusion. We devise the STPP convNet to extract equal-dimensional descriptions for each variable-size shot, and we adopt the LSTM CNN-E model to learn a global description for the input video using these time-varying descriptions. With these advantages, our method should improve all 3-D CNN-based video analysis methods. We empirically evaluate our method for action recognition in videos and the experimental results show that our method outperforms the state-of-the-art methods (both 2-D and 3-D based) on three standard benchmark datasets (UCF101, HMDB51 and ACT datasets).", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_24": {"mid": "1586939924", "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2745519816", "abstract": "Learning image representations with ConvNets by pre-training on ImageNet has proven useful across many visual understanding tasks including object detection, semantic segmentation, and image captioning. Although any image representation can be applied to video frames, a dedicated spatiotemporal representation is still vital in order to incorporate motion patterns that cannot be captured by appearance based models alone. This paper presents an empirical ConvNet architecture search for spatiotemporal feature learning, culminating in a deep 3-dimensional (3D) Residual ConvNet. Our proposed architecture outperforms C3D by a good margin on Sports-1M, UCF101, HMDB51, THUMOS14, and ASLAN while being 2 times faster at inference time, 2 times smaller in model size, and having a more compact representation.", "ref_function": ["background", "background", "objective", "result"], "cite_purpose": ["background"]}, "@cite_46": {"mid": "2964191259", "abstract": "Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "2963155035", "abstract": "In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block \"R(2+1)D\" which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "1522734439", "abstract": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8 accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.", "ref_function": ["background", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Another efficient way to extract temporal features is to precomputing the optical flow @cite_11 using traditional optical flow estimation methods and training a separate CNN to encode the precomputed optical flow, which is kind of escape from temporal modeling but effective in motion features extraction.", "The famous two-stream architecture @cite_44 proposed to apply two CNN architectures separately on visual frames and staked optical flows to extract spatiotemporal features and then fuse classification score.", "Further improvements base on this architecture including multi-granular structure @cite_14 @cite_13 , convolutional fusion @cite_25 @cite_1 , key-volume mining @cite_31 , temporal segment networks @cite_8 and ActionVLAD @cite_26 for video representation learning.", "Remarkably, a recent work (I3D) @cite_35 which combines two-stream processing and 3D convolutions holds the state-of-art action recognition results.", "The work reflects the power of ultra-deep architectures and pre-trained models."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: about results"], "target_paper": "Spatial and temporal stream model has gained great success in video action recognition. Most existing works pay more attention to designing effective features fusion methods, which train the two-stream model in a separate way. However, it's hard to ensure discriminability and explore complementary information between different streams in existing works. In this work, we propose a novel cooperative cross-stream network that investigates the conjoint information in multiple different modalities. The jointly spatial and temporal stream networks feature extraction is accomplished by an end-to-end learning manner. It extracts this complementary information of different modality from a connection block, which aims at exploring correlations of different stream features. Furthermore, different from the conventional ConvNet that learns the deep separable features with only one cross-entropy loss, our proposed model enhances the discriminative power of the deeply learned features and reduces the undesired modality discrepancy by jointly optimizing a modality ranking constraint and a cross-entropy loss for both homogeneous and heterogeneous modalities. The modality ranking constraint constitutes intra-modality discriminative embedding and inter-modality triplet constraint, and it reduces both the intra-modality and cross-modality feature variations. Experiments on three benchmark datasets demonstrate that by cooperating appearance and motion feature extraction, our method can achieve state-of-the-art or competitive performance compared with existing results.", "reference": {"@cite_35": {"mid": "2963524571", "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2 on HMDB-51 and 97.9 on UCF-101.", "ref_function": ["background", "objective", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_14": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2608988379", "abstract": "In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks [42] with learnable spatio-temporal feature aggregation [6]. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that: (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13 relative) as well as outperforms other baselines with comparable base architectures on HMDB51, UCF101, and Charades video classification benchmarks.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2507009361", "abstract": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( ( 69.4 , )) and UCF101 ( ( 94.2 , )). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https: github.com yjxiong temporal-segment-networks).", "ref_function": ["background", "background", "objective", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2751445731", "abstract": "3-D convolutional neural networks (3-D-convNets) have been very recently proposed for action recognition in videos, and promising results are achieved. However, existing 3-D-convNets has two \u201cartificial\u201d requirements that may reduce the quality of video analysis: 1) It requires a fixed-sized (e.g., 112 @math 112) input video; and 2) most of the 3-D-convNets require a fixed-length input (i.e., video shots with fixed number of frames). To tackle these issues, we propose an end-to-end pipeline named Two-stream 3-D-convNet Fusion , which can recognize human actions in videos of arbitrary size and length using multiple features. Specifically, we decompose a video into spatial and temporal shots. By taking a sequence of shots as input, each stream is implemented using a spatial temporal pyramid pooling (STPP) convNet with a long short-term memory (LSTM) or CNN-E model, softmax scores of which are combined by a late fusion. We devise the STPP convNet to extract equal-dimensional descriptions for each variable-size shot, and we adopt the LSTM CNN-E model to learn a global description for the input video using these time-varying descriptions. With these advantages, our method should improve all 3-D CNN-based video analysis methods. We empirically evaluate our method for action recognition in videos and the experimental results show that our method outperforms the state-of-the-art methods (both 2-D and 3-D based) on three standard benchmark datasets (UCF101, HMDB51 and ACT datasets).", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_44": {"mid": "2156303437", "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.", "ref_function": ["background", "objective", "objective", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2472293097", "abstract": "Recently, deep learning approaches have demonstrated remarkable progresses for action recognition in videos. Most existing deep frameworks equally treat every volume i.e. spatial-temporal video clip, and directly assign a video label to all volumes sampled from it. However, within a video, discriminative actions may occur sparsely in a few key volumes, and most other volumes are irrelevant to the labeled action category. Training with a large proportion of irrelevant volumes will hurt performance. To address this issue, we propose a key volume mining deep framework to identify key volumes and conduct classification simultaneously. Specifically, our framework is trained is optimized in an alternative way integrated to the forward and backward stages of Stochastic Gradient Descent (SGD). In the forward pass, our network mines key volumes for each action class. In the backward pass, it updates network parameters with the help of these mined key volumes. In addition, we propose \"Stochastic out\" to model key volumes from multi-modalities, and an effective yet simple \"unsupervised key volume proposal\" method for high quality volume sampling. Our experiments show that action recognition performance can be significantly improved by mining key volumes, and we achieve state-of-the-art performance on HMDB51 and UCF101 (93.1 ).", "ref_function": ["background", "background", "background", "background", "result", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2962852931", "abstract": "From the frame clip-level feature learning to the video-level representation building, deep learning methods in action recognition have developed rapidly in recent years. However, current methods suffer from the confusion caused by partial observation training, or without end-to-end learning, or restricted to single temporal scale modeling and so on. In this paper, we build upon two-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning approach, to address these problems. Specifically, at first, RGB images and optical flow stacks are sparsely sampled across the whole video. Then a temporal pyramid pooling layer is used to aggregate the frame-level features which consist of spatial and temporal cues. Lastly, the trained model has compact video-level representation with multiple temporal scales, which is both global and sequence-aware. Experimental results show that DTPP achieves the state-of-the-art performance on two challenging video action datasets: UCF101 and HMDB51, either by ImageNet pre-training or Kinetics pre-training.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2964094092", "abstract": "Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3 on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96.0 and 74.2 accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at: https: github.com kevin-ssy Optical-Flow-Guided-Feature", "ref_function": ["background", "objective", "method", "method", "result", "background", "background", "method", "result", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Reinforcement Learning (RL) methods are typically based on value functions or policy search @cite_19 , which also applies to deep RL methods.", "While value functions have been particularly applied to task-oriented dialogue systems @cite_12 @cite_31 @cite_6 @cite_22 @cite_3 @cite_29 , policy search has been particularly applied to open-ended dialogue systems such as (chitchat) chatbots @cite_7 @cite_33 @cite_0 @cite_23 @cite_11 .", "This is not surprising given the fact that task-oriented dialogue systems use finite action sets, while chatbot systems use infinite action sets.", "So far there is a preference for policy search methods for chatbots, but it is not clear whether they should be preferred because they face problems such as local optima rather than global optima, inefficiency and high variance.", "It is thus that this paper explores the feasibility of value function-based methods for chatbots, which has not been explored before---at least not from the perspective of deriving the action sets automatically as attempted in this paper."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Other comments", "Explaining the inadequacies of previous studies", "Describing the objective"], "target_paper": "Training chatbots using the reinforcement learning paradigm is challenging due to high-dimensional states, infinite action spaces and the difficulty in specifying the reward function. We address such problems using clustered actions instead of infinite actions, and a simple but promising reward function based on human-likeness scores derived from human-human dialogue data. We train Deep Reinforcement Learning (DRL) agents using chitchat data in raw text\u2014without any manual annotations. Experimental results using different splits of training data report the following. First, that our agents learn reasonable policies in the environments they get familiarised with, but their performance drops substantially when they are exposed to a test set of unseen dialogues. Second, that the choice of sentence embedding size between 100 and 300 dimensions is not significantly different on test data. Third, that our proposed human-likeness rewards are reasonable for training chatbots as long as they use lengthy dialogue histories of \u226510 sentences.", "reference": {"@cite_22": {"mid": "2732273801", "abstract": "Deep reinforcement learning dialogue systems are attractive because they can jointly learn their feature representations and policies without manual feature engineering. But its application is challenging due to slow learning. We propose a two-stage method for accelerating the induction of single or multi-domain dialogue policies. While the first stage reduces the amount of weight updates over time, the second stage uses very limited minibatches (of as much as two learning experiences) sampled from experience replay memories. The former frequently updates the weights of the neural nets at early stages of training, and decreases the amount of updates as training progresses by performing updates during exploration and by skipping updates during exploitation. The learning process is thus accelerated through less weight updates in both stages. An empirical evaluation in three domains (restaurants, hotels and tv guide) confirms that the proposed method trains policies 5 times faster than a baseline without the proposed method. Our findings are useful for training larger-scale neural-based spoken dialogue systems.", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2963167310", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2581637843", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_29": {"mid": "2737041661", "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2603550564", "abstract": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN) for multiple tasks (domains) face scalability problems due to large search spaces. This paper proposes a three-stage method for multi-domain dialogue policy learning-termed NDQN, and applies it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. In this method, the first stage does multi-policy learning via a network of DQN agents; the second makes use of compact state representations by compressing raw inputs; and the third stage applies a pre-training phase for bootstraping the behaviour of agents in the network. Experimental results comparing DQN (baseline) versus NDQN (proposed) using simulations report that the proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems. An additional evaluation reports that the NDQN agents outperformed a K-Nearest Neighbour baseline in task success and dialogue length, yielding more efficient and successful dialogues.", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2594726847", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2784808670", "abstract": "We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including neural network and template-based models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A B testing with real-world users, where it performed significantly better than other systems. The results highlight the potential of coupling ensemble systems with deep reinforcement learning as a fruitful path for developing real-world, open-domain conversational agents.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_23": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2294065713", "abstract": "This article presents SimpleDS, a simple and publicly available dialogue system trained with deep reinforcement learning. In contrast to previous reinforcement learning dialogue systems, this system avoids manual feature engineering by performing action selection directly from raw text of the last system and (noisy) user responses. Our initial results, in the restaurant domain, report that it is indeed possible to induce reasonable behaviours with such an approach that aims for higher levels of automation in dialogue control for intelligent interactive systems and robots.", "ref_function": ["background", "background", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "2963306198", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["The outliers we are interested in this work are due to outliers caused by extreme events.", "Another related problem considers methods to detect outliers caused by data measurement errors, such as sensor malfunction, malicious tampering, or measurement error @cite_13 @cite_2 @cite_38 .", "The latter methods can be seen as a part of a standard data cleaning or data pre-processing step.", "On the other hand, outliers caused by extreme traffic have valuable information for congestion management, and can provide agencies with insights into the performance of urban network.", "The works @cite_1 @cite_35 @cite_21 explore the problem of outlier detection caused by events, while the works @cite_6 @cite_3 @cite_50 @cite_4 focus on determining the root causes of the outlier."], "label": ["Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "General descriptions of the topic", "Reference to current state of knowledge", "General reference to previous research or scholarship: research objective"], "target_paper": "Event detection is gaining increasing attention in smart cities research. Large-scale mobility data serves as an important tool to uncover the dynamics of urban transportation systems, and more often than not the dataset is incomplete. In this article, we develop a method to detect extreme events in large traffic datasets, and to impute missing data during regular conditions. Specifically, we propose a robust tensor recovery problem to recover low rank tensors under fiber-sparse corruptions with partial observations, and use it to identify events, and impute missing data under typical conditions. Our approach is scalable to large urban areas, taking full advantage of the spatio-temporal correlations in traffic patterns. We develop an efficient algorithm to solve the tensor recovery problem based on the alternating direction method of multipliers (ADMM) framework. Compared with existing @math norm regularized tensor decomposition methods, our algorithm can exactly recover the values of uncorrupted fibers of a low rank tensor and find the positions of corrupted fibers under mild conditions. Numerical experiments illustrate that our algorithm can exactly detect outliers even with missing data rates as high as 40 , conditioned on the outlier corruption rate and the Tucker rank of the low rank tensor. Finally, we apply our method on a real traffic dataset corresponding to downtown Nashville, TN, USA and successfully detect the events like severe car crashes, construction lane closures, and other large events that cause significant traffic disruptions.", "reference": {"@cite_38": {"mid": "1974476933", "abstract": "Novel methods for implementation of detector-level multivariate screening methods are presented. The methods use present data and classify data as outliers on the basis of comparisons with empirical cutoff points derived from extensive archived data rather than from standard statistical tables. In addition, while many of the ideas of the classical Hotelling's T2-statistic are used, modern statistical trend removal and blocking are incorporated. The methods are applied to intelligent transportation system data from San Antonio and Austin, Texas. These examples show how the suggested new methods perform with high-quality traffic data and apparently lower-quality traffic data. All algorithms were implemented by using the SAS programming language.", "ref_function": ["method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_35": {"mid": "2093855404", "abstract": "The increasing availability of large-scale trajectory data provides us great opportunity to explore them for knowledge discovery in transportation systems using advanced data mining techniques. Nowadays, large number of taxicabs in major metropolitan cities are equipped with a GPS device. Since taxis are on the road nearly 24h a day (with drivers changing shifts), they can now act as reliable sensors to monitor the behavior of traffic. In this article, we use GPS data from taxis to monitor the emergence of unexpected behavior in the Beijing metropolitan area, which has the potential to estimate and improve traffic conditions in advance. We adapt likelihood ratio test statistic (LRT) which have previously been mostly used in epidemiological studies to describe traffic patterns. To the best of our knowledge the use of LRT in traffic domain is not only novel but results in accurate and rapid detection of anomalous behavior.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2042850276", "abstract": "The recent availability of datasets on transportation networks with higher spatial and temporal resolution is enabling new research activities in the fields of Territorial Intelligence and Smart Cities. Among these, many research efforts are aimed at predicting traffic congestions to alleviate their negative effects on society, mainly by learning recurring mobility patterns. Within this field, in this paper we propose an integrated solution to predict and visualize non-recurring traffic congestion in urban environments caused by Planned Special Events (PSE), such as a soccer game or a concert. Predictions are done by means of two Machine Learning-based techniques. These have been proven to successfully outperform current state of the art predictions by 35 in an empirical assessment we conducted over a time frame of 7 months within the inner city of Cologne, Germany. The predicted congestions are fed into a specifically conceived visualization tool we designed to allow Decision Makers to evaluate the situation and take actions to improve mobility. HighlightsWe analyze in detail the impact of Planned Special Events (PSEs) on traffic.We propose two novel methods to predict upcoming congestions caused by PSEs.Results show that our prediction methods outperform the state of the art by 35 .We introduce a specifically designed tool for visualizing the prediction results.Visualization tool allows experts to evaluate upcoming situations ahead of time.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2117618130", "abstract": "The detection of outliers in spatio-temporal traffic data is an important research problem in the data mining and knowledge discovery community. However to the best of our knowledge, the discovery of relationships, especially causal interactions, among detected traffic outliers has not been investigated before. In this paper we propose algorithms which construct outlier causality trees based on temporal and spatial properties of detected outliers. Frequent substructures of these causality trees reveal not only recurring interactions among spatio-temporal outliers, but potential flaws in the design of existing traffic networks. The effectiveness and strength of our algorithms are validated by experiments on a very large volume of real taxi trajectories in an urban road network.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2743812350", "abstract": "As the development of crowdsourcing technique, acquiring amounts of data in urban cities becomes possible and reliable, which makes it possible to mine useful and significant information from data. Traffic anomaly detection is to find the traffic patterns which are not expected and it can be used to explore traffic problems accurately and efficiently. In this paper, we propose LoTAD to explore anomalous regions with long-term poor traffic situations. Specifically, we process crowdsourced bus data into TS-segments (Temporal and Spatial segments) to model the traffic condition. Later, we explore anomalous TS-segments in each bus line by calculating their AI (Anomaly Index). Then, we combine anomalous TS-segments detected in different lines to mine anomalous regions. The information of anomalous regions provides suggestions for future traffic planning. We conduct experiments with real crowdsourced bus trajectory datasets of October in 2014 and March in 2015 in Hangzhou. We analyze the varieties of the results and explain how they are consistent with the real urban traffic planning or social events happened between the time interval of the two datasets. At last we do a contrast experiment with the most ten congested roads in Hangzhou, which verifies the effectiveness of LoTAD.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2014211872", "abstract": "Anomaly detection (a.k.a., outlier or burst detection) is a well-motivated problem and a major data mining and knowledge discovery task. In this article, we study the problem of population anomaly detection, one of the key issues related to event monitoring and population management within a city. Through studying detected population anomalies, we can trace and analyze these anomalies, which could help to model city traffic design and event impact analysis and prediction. Although a significant and interesting issue, it is very hard to detect population anomalies and retrieve anomaly trajectories, especially given that it is difficult to get actual and sufficient population data. To address the difficulties of a lack of real population data, we take advantage of mobile phone networks, which offer enormous spatial and temporal communication data on persons. More importantly, we claim that we can utilize these mobile phone data to infer and approximate population data. Thus, we can study the population anomaly detection problem by taking advantages of unique features hidden in mobile phone data. In this article, we present a system to conduct Population Anomaly Detection (PAD). First, we propose an effective clustering method, correlation-based clustering, to cluster the incomplete location information from mobile phone data (i.e., from mobile call volume distribution to population density distribution). Then, we design an adaptive parameter-free detection method, R-scan, to capture the distributed dynamic anomalies. Finally, we devise an efficient algorithm, BT-miner, to retrieve anomaly trajectories. The experimental results from real-life mobile phone data confirm the effectiveness and efficiency of the proposed algorithms. Finally, the proposed methods are realized as a pilot system in a city in China.", "ref_function": ["background", "objective", "method", "method", "method", "result", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2963024417", "abstract": "Non-recurring traffic congestion is caused by temporary disruptions, such as accidents, sports games, adverse weather, etc. We use data related to real-time traffic speed, jam factors (a traffic congestion indicator), and events collected over a year from Nashville, TN to train a multi-layered deep neural network. The traffic dataset contains over 900 million data records. The network is thereafter used to classify the real-time data and identify anomalous operations. Compared with traditional approaches of using statistical or machine learning techniques, our model reaches an accuracy of 98.73 percent when identifying traffic congestion caused by football games. Our approach first encodes the traffic across a region as a scaled image. After that the image data from different timestamps is fused with event- and time-related data. Then a crossover operator is used as a data augmentation method to generate training datasets with more balanced classes. Finally, we use the receiver operating characteristic (ROC) analysis to tune the sensitivity of the classifier. We present the analysis of the training time and the inference time separately.", "ref_function": ["background", "method", "background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_50": {"mid": "2021002141", "abstract": "With the increasing amount of traffic information collected through floating car data, it is highly desirable to find meaningful traffic patterns such as congestion patterns from the accumulated massive historical dataset. It is however challenging due to the huge size of the dataset and the complexity and dynamics of traffic phenomena. A novel floating car data analysis method based on data cube for congestion pattern exploration is proposed in this paper. This method is different from traditional methods that depend only on numerical statistics of traffic data. The view of the event or spatial-temporal progress is adapted to model and measure traffic congestions. According to a multi-dimensional analysis framework, the traffic congestion event is first identified based on spatial-temporal related relationship of slow-speed road segment. Then, it is aggregated by a cluster style to get the traffic pattern on a different level of detail of spatial-temporal dimension. Aggregated location, time period and duration time for recurrent and important congestions are used to represent the congestion pattern. The authors evaluate methods using a historical traffic dataset collected from about 12000 taxi-based floating cars for one week in a large urban area. Results show that the method can effectively identify and summarize the congestion pattern with efficient computation and reduced storage cost.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2088400370", "abstract": "In order to improve the veracity and reliability of a traffic model built, or to extract important and valuable information from collected traffic data, the technique of outlier mining has been introduced into the traffic engineering domain for detecting and analyzing the outliers in traffic data sets. Three typical outlier algorithms, respectively the statistics-based approach, the distance-based approach, and the density-based local outlier approach, are described with respect to the principle, the characteristics and the time complexity of the algorithms. A comparison among the three algorithms is made through application to intelligent transportation systems (ITS). Two traffic data sets with different dimensions have been used in our experiments carried out, one is travel time data, and the other is traffic flow data. We conducted a number of experiments to recognize outliers hidden in the data sets before building the travel time prediction model and the traffic flow foundation diagram. In addition, some artificial generated outliers are introduced into the traffic flow data to see how well the different algorithms detect them. Three strategies-based on ensemble learning, partition and average LOF have been proposed to develop a better outlier recognizer. The experimental results reveal that these methods of outlier mining are feasible and valid to detect outliers in traffic data sets, and have a good potential for use in the domain of traffic engineering. The comparison and analysis presented in this paper are expected to provide some insights to practitioners who plan to use outlier mining for ITS data.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "1544613517", "abstract": "In their goal to effectively manage the use of existing infrastructures, intelligent transportation systems require precise forecasting of near-term traffic volumes to feed real-time analytical models and traffic surveillance tools that alert of network links reaching their capacity. This article proposes a new methodological approach for short-term predictions of time series of volume data at isolated cross sections. The originality in the computational modeling stems from the fit of threshold values used in the stationary wavelet-based denoising process applied on the time series, and from the determination of patterns that characterize the evolution of its samples over a fixed prediction horizon. A self-organizing fuzzy neural network is optimized in its configuration parameters for learning and recognition of these patterns. Four real-world data sets from 3 interstate roads are considered for evaluating the performance of the proposed model. A quantitative comparison made with the results obtained by 4 other relevant prediction models shows a favorable outcome.", "ref_function": ["background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Low rank matrix and tensor learning has been widely used to utilize the inner structure of the data.", "Various application have benefited from matrix and tensor based methods, including data completion @cite_43 @cite_47 , link prediction @cite_33 , network structure clustering @cite_8 , etc."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: research objective"], "target_paper": "Event detection is gaining increasing attention in smart cities research. Large-scale mobility data serves as an important tool to uncover the dynamics of urban transportation systems, and more often than not the dataset is incomplete. In this article, we develop a method to detect extreme events in large traffic datasets, and to impute missing data during regular conditions. Specifically, we propose a robust tensor recovery problem to recover low rank tensors under fiber-sparse corruptions with partial observations, and use it to identify events, and impute missing data under typical conditions. Our approach is scalable to large urban areas, taking full advantage of the spatio-temporal correlations in traffic patterns. We develop an efficient algorithm to solve the tensor recovery problem based on the alternating direction method of multipliers (ADMM) framework. Compared with existing @math norm regularized tensor decomposition methods, our algorithm can exactly recover the values of uncorrupted fibers of a low rank tensor and find the positions of corrupted fibers under mild conditions. Numerical experiments illustrate that our algorithm can exactly detect outliers even with missing data rates as high as 40 , conditioned on the outlier corruption rate and the Tucker rank of the low rank tensor. Finally, we apply our method on a real traffic dataset corresponding to downtown Nashville, TN, USA and successfully detect the events like severe car crashes, construction lane closures, and other large events that cause significant traffic disruptions.", "reference": {"@cite_43": {"mid": "2963472624", "abstract": "Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.", "ref_function": ["background", "background", "objective", "method", "method", "objective", "result"], "cite_purpose": ["background"]}, "@cite_47": {"mid": "2343462218", "abstract": "Intelligent transportation systems (ITSs) gather information about traffic conditions by collecting data from a wide range of on-ground sensors. The collected data usually suffer from irregular spatial and temporal resolution. Consequently, missing data is a common problem faced by ITSs. In this paper, we consider the problem of missing data in large and diverse road networks. We propose various matrix and tensor based methods to estimate these missing values by extracting common traffic patterns in large road networks. To obtain these traffic patterns in the presence of missing data, we apply fixed-point continuation with approximate singular value decomposition, canonical polyadic decomposition, least squares, and variational Bayesian principal component analysis. For analysis, we consider different road networks, each of which is composed of around 1500 road segments. We evaluate the performance of these methods in terms of estimation accuracy, variance of the data set, and the bias imparted by these methods.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "1864134408", "abstract": "The data in many disciplines such as social networks, Web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this article, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T + 1? If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T + 2, T + 3, etc.? In this article, we consider bipartite graphs that evolve over time and consider matrix- and tensor-based methods for predicting future links. We present a weight-based method for collapsing multiyear data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix- and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem. Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "40609341", "abstract": "The traffic networks reflect the pulse and structure of a city and shows some dynamic characteristic. Previous research in mining structure from networks mostly focus on static networks and fail to exploit the temporal patterns. In this paper, we aim to solve the problem of discovering the urban spatio-temporal structure from time-evolving traffic networks. We model the time-evolving traffic networks into a 3-order tensor, each element of which indicates the volume of traffic from i-th origin area to j-th destination area in k-th time domain. Considering traffic data and urban contextual knowledge together, we propose a regularized Non-negative Tucker Decomposition (rNTD) method, which discovers the spatial clusters, temporal patterns and relations among them simultaneously. Abundant experiments are conducted in a large dataset collected from Beijing. Results show that our method outperforms the baseline method.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The most relevant works with ours are robust matrix and tensor PCA for outlier detection.", "@math norm regularized robust tensor recovery, as proposed by Goldfarb and Qin @cite_46 , is useful when data is polluted with unstructured random noises.", "@cite_5 also used @math norm regularized tensor decomposition for traffic data recovery, in face of random noise corruption.", "But if outliers are structured, for example grouped in columns, @math norm regularization does not yield good results.", "In addition, although traffic is also modeled in tensor format in @cite_5 , only a single road segment is considered, not taking into account network spacial structures."], "label": ["Explaining the method relationship between own work and references", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "Event detection is gaining increasing attention in smart cities research. Large-scale mobility data serves as an important tool to uncover the dynamics of urban transportation systems, and more often than not the dataset is incomplete. In this article, we develop a method to detect extreme events in large traffic datasets, and to impute missing data during regular conditions. Specifically, we propose a robust tensor recovery problem to recover low rank tensors under fiber-sparse corruptions with partial observations, and use it to identify events, and impute missing data under typical conditions. Our approach is scalable to large urban areas, taking full advantage of the spatio-temporal correlations in traffic patterns. We develop an efficient algorithm to solve the tensor recovery problem based on the alternating direction method of multipliers (ADMM) framework. Compared with existing @math norm regularized tensor decomposition methods, our algorithm can exactly recover the values of uncorrupted fibers of a low rank tensor and find the positions of corrupted fibers under mild conditions. Numerical experiments illustrate that our algorithm can exactly detect outliers even with missing data rates as high as 40 , conditioned on the outlier corruption rate and the Tucker rank of the low rank tensor. Finally, we apply our method on a real traffic dataset corresponding to downtown Nashville, TN, USA and successfully detect the events like severe car crashes, construction lane closures, and other large events that cause significant traffic disruptions.", "reference": {"@cite_5": {"mid": "2083797062", "abstract": "Traffic volume data is already collected and used for a variety of purposes in intelligent transportation system (ITS). However, the collected data might be abnormal due to the problem of outlier data caused by malfunctions in data collection and record systems. To fully analyze and operate the collected data, it is necessary to develop a validate method for addressing the outlier data. Many existing algorithms have studied the problem of outlier recovery based on the time series methods. In this paper, a multiway tensor model is proposed for constructing the traffic volume data based on the intrinsic multilinear correlations, such as day to day and hour to hour. Then, a novel tensor recovery method, called ADMM-TR, is proposed for recovering outlier data of traffic volume data. The proposed method is evaluated on synthetic data and real world traffic volume data. Experimental results demonstrate the practicability, effectiveness, and advantage of the proposed method, especially for the real world traffic volume data.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "differences"]}, "@cite_46": {"mid": "1999136078", "abstract": "Robust tensor recovery plays an instrumental role in robustifying tensor decompositions for multilinear data analysis against outliers, gross corruptions, and missing values and has a diverse array of applications. In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon recent advances in robust principal component analysis and tensor completion. We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem. These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods. We also propose a nonconvex model that can often improve the recovery results from the convex models. We investigate the empirical recoverability properties of the convex and nonconvex formulations and compare the computational performance of the algorithms on simulated data. We demonstrate through a number o...", "ref_function": ["background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["uses"]}}}
{"sentences": ["In face of large events, outliers tend to group in columns or fibers in the dataset, as illustrated in section .", "@math norm regularized decomposition is suitable for group outlier detection, as shown in @cite_25 @cite_51 for matrices, and @cite_27 @cite_41 for tensors.", "In addition, @cite_9 introduced a multi-view low-rank analysis framework for outlier detection, and @cite_15 used discriminant tensor factorization for event analytics.", "Our methods differ from the existing tensor outliers pursuit @cite_27 @cite_41 in that they are dealing with slab outliers, i.e., outliers form an entire slice instead of fibers of the tensor.", "Moreover, compared with existing works, we take one step further and deal with partial observations.", "As stated in Section , without an overall understanding of the underlying pattern, we can easily impute the missing entries incorrectly and influence our decision about outliers.", "We will show in Section simulation that our new algorithm can exactly detect the outliers even with 40"], "label": ["Reference to current state of knowledge", "General reference to previous research or scholarship: about results", "General reference to previous research or scholarship: approaches taken", "Explaining the method relationship between own work and references", "Describing used methods", "Describing used methods", "Signalling Transition"], "target_paper": "Event detection is gaining increasing attention in smart cities research. Large-scale mobility data serves as an important tool to uncover the dynamics of urban transportation systems, and more often than not the dataset is incomplete. In this article, we develop a method to detect extreme events in large traffic datasets, and to impute missing data during regular conditions. Specifically, we propose a robust tensor recovery problem to recover low rank tensors under fiber-sparse corruptions with partial observations, and use it to identify events, and impute missing data under typical conditions. Our approach is scalable to large urban areas, taking full advantage of the spatio-temporal correlations in traffic patterns. We develop an efficient algorithm to solve the tensor recovery problem based on the alternating direction method of multipliers (ADMM) framework. Compared with existing @math norm regularized tensor decomposition methods, our algorithm can exactly recover the values of uncorrupted fibers of a low rank tensor and find the positions of corrupted fibers under mild conditions. Numerical experiments illustrate that our algorithm can exactly detect outliers even with missing data rates as high as 40 , conditioned on the outlier corruption rate and the Tucker rank of the low rank tensor. Finally, we apply our method on a real traffic dataset corresponding to downtown Nashville, TN, USA and successfully detect the events like severe car crashes, construction lane closures, and other large events that cause significant traffic disruptions.", "reference": {"@cite_41": {"mid": "2594059414", "abstract": "In this paper, we study robust principal component analysis on tensors, in the setting where frame-wise outliers exist. We propose a convex formulation to decompose a tensor into a low rank component and a frame-wise sparse component. Theoretically, we guarantee that exact subspace recovery and outlier identification can be achieved under mild model assumptions. Compared with entry-wise outlier pursuit and naive matricization of tensors with frame-wise outliers, our approach can handle higher ranks and proportion of outliers. Extensive numerical evaluations are provided on both synthetic and real data to support our theory.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background", "differences"]}, "@cite_9": {"mid": "2791255512", "abstract": "Detecting outliers or anomalies is a fundamental problem in various machine learning and data mining applications. Conventional outlier detection algorithms are mainly designed for single-view data. Nowadays, data can be easily collected from multiple views, and many learning tasks such as clustering and classification have benefited from multi-view data. However, outlier detection from multi-view data is still a very challenging problem, as the data in multiple views usually have more complicated distributions and exhibit inconsistent behaviors. To address this problem, we propose a multi-view low-rank analysis (MLRA) framework for outlier detection in this article. MLRA pursuits outliers from a new perspective, robust data representation. It contains two major components. First, the cross-view low-rank coding is performed to reveal the intrinsic structures of data. In particular, we formulate a regularized rank-minimization problem, which is solved by an efficient optimization algorithm. Second, the outliers are identified through an outlier score estimation procedure. Different from the existing multi-view outlier detection methods, MLRA is able to detect two different types of outliers from multiple views simultaneously. To this end, we design a criterion to estimate the outlier scores by analyzing the obtained representation coefficients. Moreover, we extend MLRA to tackle the multi-view group outlier detection problem. Extensive evaluations on seven UCI datasets, the MovieLens, the USPS-MNIST, and the WebKB datasets demon strate that our approach outperforms several state-of-the-art outlier detection methods.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background", "differences"]}, "@cite_15": {"mid": "2897753390", "abstract": "Analyzing the impact of disastrous events has been central to understanding and responding to crises. Traditionally, the assessment of disaster impact has primarily relied on the manual collection and analysis of surveys and questionnaires as well as the review of authority reports. This can be costly and time-consuming, whereas a timely assessment of an event\u2019s impact is critical for crisis management and humanitarian operations. In this work, we formulate the impact discovery as the problem to identify the shared and discriminative subspace via tensor factorization due to the multi-dimensional nature of mobility data. Existing work in mining the shared and discriminative subspaces typically requires the predefined number of either type of them. In the context of event impact discovery, this could be impractical, especially for those unprecedented events. To overcome this, we propose a new framework, called \u201cPairFac,\u201d that jointly factorizes the multi-dimensional data to discover the latent mobility pattern along with its associated discriminative weight. This framework does not require splitting the shared and discriminative subspaces in advance and at the same time automatically captures the persistent and changing patterns from multi-dimensional behavioral data. Our work has important applications in crisis management and urban planning, which provides a timely assessment of impacts of major events in the urban environment.", "ref_function": ["background", "background", "background", "objective", "method", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_51": {"mid": "2120580172", "abstract": "Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative filtering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efficient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identifies the corrupted points. Such identification of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and financial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2160813243", "abstract": "In this paper, we propose a convex program for low-rank and block-sparse matrix decomposition. Potential applications include outlier detection when certain columns of the data matrix are outliers. We design an algorithm based on the augmented Lagrange multiplier method to solve the convex program. We solve the subproblems involved in the augmented Lagrange multiplier method using the Douglas Peaceman-Rachford (DR) monotone operator splitting method. Numerical simulations demonstrate the accuracy of our method compared with the robust principal component analysis based on low-rank and sparse matrix decomposition.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Query expansion has a long history of literature in the field of information retrieval.", "It was first coined by @cite_29 in the 1960s for literature indexing and searching in a mechanized library system.", "In 1971, Rocchio @cite_21 brought QE to spotlight through the relevance feedback method and its characterization in a vector space model.", "While this was the first use of relevance feedback method, Rocchio's method is still used for QE in its original and modified forms.", "The availability of several standard text collections (e.g., Text Retrieval Conference (TREC) http: trec.nist.gov , and Forum for Information Retrieval Evaluation (FIRE) http: fire.irsi.res.in ) and IR platforms (e.g., Terrier http: terrier.org and Apache Lucene http: lucene.apache.org ) have been very instrumental in evaluating the progress in this area in a systematic way.", "Carpineto and Romano @cite_22 and Azad and Deepak @cite_2 present state-of-the-art comprehensive surveys on QE.", "This article focuses on web based QE techniques."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Other comments", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Describing the objective"], "target_paper": "In the field of information retrieval, query expansion (QE) has long been used as a technique to deal with the fundamental issue of word mismatch between a user's query and the target information. In the context of the relationship between the query and expanded terms, existing weighting techniques often fail to appropriately capture the term-term relationship and term to the whole query relationship, resulting in low retrieval effectiveness. Our proposed QE approach addresses this by proposing three weighting models based on (1) tf-itf, (2) k-nearest neighbor (kNN) based cosine similarity, and (3) correlation score. Further, to extract the initial set of expanded terms, we use pseudo-relevant web knowledge consisting of the top N web pages returned by the three popular search engines namely, Google, Bing, and DuckDuckGo, in response to the original query. Among the three weighting models, tf-itf scores each of the individual terms obtained from the web content, kNN-based cosine similarity scores the expansion terms to obtain the term-term relationship, and correlation score weighs the selected expansion terms with respect to the whole query. The proposed model, called web knowledge based query expansion (WKQE), achieves an improvement of 25.89 on the MAP score and 30.83 on the GMAP score over the unexpanded queries on the FIRE dataset. A comparative analysis of the WKQE techniques with other related approaches clearly shows significant improvement in the retrieval performance. We have also analyzed the effect of varying the number of pseudo-relevant documents and expansion terms on the retrieval effectiveness of the proposed model.", "reference": {"@cite_29": {"mid": "2082729696", "abstract": "This paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called \u201cProbabilistic Indexing,\u201d allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the \u201crelevance number\u201d) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance. The paper goes on to show that whereas in a conventional library system the cross-referencing (\u201csee\u201d and \u201csee also\u201d) is based solely on the \u201csemantical closeness\u201d between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected. Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2164547069", "abstract": "1332840 Primer compositions DOW CORNINGCORP 6 Oct 1971 [30 Dec 1970] 46462 71 Heading C3T [Also in Divisions B2 and C4] A primer composition comprises 1 pbw of tetra ethoxy or propoxy silane or poly ethyl or propyl silicate or any mixture thereof, 0A75-2A5 pbw of bis(acetylacetonyl) diisopropyl titanate, 0A75- 5 pbw of a compound CF 3 CH 2 CH 2 Si[OSi(CH 3 ) 2 - X] 3 wherein each X is H or -CH 2 CH 2 Si- (OOCCH 3 ) 3 , at least one being the latter, and 1-20 pbw of a ketone, hydrocarbon or halohydrocarbon solvent boiling not above 150\u2039 C. In the examples 1 pbw each of bis(acetylacetonyl)diisopropyl titanate, polyethyl silicate and are dissolved in 10 pbw of acetone or in 9 pbw of light naphtha and 1 of methylisobutylketone. The solutions are used to prime Ti panels, to which a Pt-catalysed room-temperature vulcanizable poly-trifluoropropylmethyl siloxanebased rubber is then applied.", "ref_function": ["method", "method"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "1993692165", "abstract": "The relative ineffectiveness of information retrieval systems is largely caused by the inaccuracy with which a query formed by a few keywords models the actual user information need. One well known method to overcome this limitation is automatic query expansion (AQE), whereby the user\u2019s original query is augmented by new features with a similar meaning. AQE has a long history in the information retrieval community but it is only in the last years that it has reached a level of scientific and experimental maturity, especially in laboratory settings such as TREC. This survey presents a unified view of a large number of recent approaches to AQE that leverage various data sources and employ very different principles and techniques. The following questions are addressed. Why is query expansion so important to improve search effectiveness? What are the main steps involved in the design and implementation of an AQE component? What approaches to AQE are available and how do they compare? Which issues must still be resolved before AQE becomes a standard component of large operational information retrieval systems (e.g., search engines)?", "ref_function": ["background", "background", "background", "objective", "objective", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2963764152", "abstract": "Abstract With the ever increasing size of the web, relevant information extraction on the Internet with a query formed by a few keywords has become a big challenge. Query Expansion (QE) plays a crucial role in improving searches on the Internet. Here, the user\u2019s initial query is reformulated by adding additional meaningful terms with similar significance. QE \u2013 as part of information retrieval (IR) \u2013 has long attracted researchers\u2019 attention. It has become very influential in the field of personalized social document, question answering, cross-language IR, information filtering and multimedia IR. Research in QE has gained further prominence because of IR dedicated conferences such as TREC (Text Information Retrieval Conference) and CLEF (Conference and Labs of the Evaluation Forum). This paper surveys QE techniques in IR from 1960 to 2017 with respect to core techniques, data sources used, weighting and ranking methodologies, user participation and applications \u2013 bringing out similarities and differences.", "ref_function": ["background", "background", "background", "background", "background", "background", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Based on web search query logs, two types of QE approaches are usually used.", "The first type extract features from the queries, stored in logs, that are related to the user's original query, with or without making use of their respective retrieval results @cite_43 @cite_20 .", "In techniques based on the first approach, some use their combined retrieval results @cite_41 , while some do not (e.g., @cite_43 @cite_20 ).", "In the second type of approach, the features are extracted on relational behavior of queries and retrieval results.", "For example, @cite_26 represent queries in a graph based vector space model (query-click bipartite graph) and analyze the graph constructed using the query logs.", "Under the second approach, the expansion terms are extracted form several approaches: through user clicks @cite_13 @cite_20 @cite_36 , directly from the clicked results @cite_9 @cite_44 @cite_40 , the top results from the past query terms entered by the user @cite_10 @cite_19 , and queries related with the same documents @cite_4 @cite_6 .", "However, the second type of approach is more widely used and has been shown to provide better results."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Explain the significance of references"], "target_paper": "In the field of information retrieval, query expansion (QE) has long been used as a technique to deal with the fundamental issue of word mismatch between a user's query and the target information. In the context of the relationship between the query and expanded terms, existing weighting techniques often fail to appropriately capture the term-term relationship and term to the whole query relationship, resulting in low retrieval effectiveness. Our proposed QE approach addresses this by proposing three weighting models based on (1) tf-itf, (2) k-nearest neighbor (kNN) based cosine similarity, and (3) correlation score. Further, to extract the initial set of expanded terms, we use pseudo-relevant web knowledge consisting of the top N web pages returned by the three popular search engines namely, Google, Bing, and DuckDuckGo, in response to the original query. Among the three weighting models, tf-itf scores each of the individual terms obtained from the web content, kNN-based cosine similarity scores the expansion terms to obtain the term-term relationship, and correlation score weighs the selected expansion terms with respect to the whole query. The proposed model, called web knowledge based query expansion (WKQE), achieves an improvement of 25.89 on the MAP score and 30.83 on the GMAP score over the unexpanded queries on the FIRE dataset. A comparative analysis of the WKQE techniques with other related approaches clearly shows significant improvement in the retrieval performance. We have also analyzed the effect of varying the number of pseudo-relevant documents and expansion terms on the retrieval effectiveness of the proposed model.", "reference": {"@cite_26": {"mid": "2086378526", "abstract": "In this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic relations that are implicitly captured in the actions of users submitting queries and clicking answers. Previous query log analyses were mostly done with just the queries and not the actions that followed after them. We first propose a novel way to represent queries in a vector space based on a graph derived from the query-click bipartite graph. We then analyze the graph produced by our query log, showing that it is less sparse than previous results suggested, and that almost all the measures of these graphs follow power laws, shedding some light on the searching user behavior as well as on the distribution of topics that people want in the Web. The representation we introduce allows to infer interesting semantic relationships between queries. Second, we provide an experimental analysis on the quality of these relations, showing that most of them are relevant. Finally we sketch an application that detects multitopical URLs.", "ref_function": ["objective", "background", "method", "result", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2039499764", "abstract": "Hundreds of millions of users each day use web search engines to meet their information needs. Advances in web search effectiveness are therefore perhaps the most significant public outcomes of IR research. Query expansion is one such method for improving the effectiveness of ranked retrieval by adding additional terms to a query. In previous approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial retrieval run. We propose a new method of obtaining expansion terms, based on selecting terms from past user queries that are associated with documents in the collection. Our scheme is effective for query expansion for web retrieval: our results show relative improvements over unexpanded full text retrieval of 26 --29 , and 18 --20 over an optimised, conventional expansion approach.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_41": {"mid": "2163987313", "abstract": "Users frequently modify a previous search query in hope of retrieving better results. These modifications are called query reformulations or query refinements. Existing research has studied how web search engines can propose reformulations, but has given less attention to how people perform query reformulations. In this paper, we aim to better understand how web searchers refine queries and form a theoretical foundation for query reformulation. We study users' reformulation strategies in the context of the AOL query logs. We create a taxonomy of query refinement strategies and build a high precision rule-based classifier to detect each type of reformulation. Effectiveness of reformulations is measured using user click behavior. Most reformulation strategies result in some benefit to the user. Certain strategies like add remove words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. In contrast, users often click the same result as their previous query or select no results when forming acronyms and reordering words. Perhaps the most surprising finding is that some reformulations are better suited to helping users when the current results are already fruitful, while other reformulations are more effective when the results are lacking. Our findings inform the design of applications that can assist searchers; examples are described in this paper.", "ref_function": ["background", "background", "background", "objective", "method", "method", "background", "background", "background", "background", "result", "result"], "cite_purpose": ["background"]}, "@cite_36": {"mid": "2077528174", "abstract": "The semantic gap between low-level visual features and high-level semantics has been investigated for decades but still remains a big challenge in multimedia. When \"search\" became one of the most frequently used applications, \"intent gap\", the gap between query expressions and users' search intents, emerged. Researchers have been focusing on three approaches to bridge the semantic and intent gaps: 1) developing more representative features, 2) exploiting better learning approaches or statistical models to represent the semantics, and 3) collecting more training data with better quality. However, it remains a challenge to close the gaps. In this paper, we argue that the massive amount of click data from commercial search engines provides a data set that is unique in the bridging of the semantic and intent gap. Search engines generate millions of click data (a.k.a. image-query pairs), which provide almost \"unlimited\" yet strong connections between semantics and images, as well as connections between users' intents and queries. To study the intrinsic properties of click data and to investigate how to effectively leverage this huge amount of data to bridge semantic and intent gap is a promising direction to advance multimedia research. In the past, the primary obstacle is that there is no such dataset available to the public research community. This changes as Microsoft has released a new large-scale real-world image click data to public. This paper presents preliminary studies on the power of large-scale click data with a variety of experiments, such as building large-scale concept detectors, tag processing, search, definitive tag detection, intent analysis, etc., with the goal to inspire deeper researches based on this dataset.", "ref_function": ["background", "background", "background", "background", "result", "background", "background", "objective", "background", "method", "objective"], "cite_purpose": ["uses"]}, "@cite_9": {"mid": "2099548400", "abstract": "Queries to search engines on the Web are usually short. They do not provide sufficient information for an effective selection of relevant documents. Previous research has proposed the utilization of query expansion to deal with this problem. However, expansion terms are usually determined on term co-occurrences within documents. In this study, we propose a new method for query expansion based on user interactions recorded in user logs. The central idea is to extract correlations between query terms and document terms by analyzing user logs. These correlations are then used to select high-quality expansion terms for new queries. Compared to previous query expansion methods, ours takes advantage of the user judgments implied in user logs. The experimental results show that the log-based query expansion method can produce much better results than both the classical search method and the other query expansion methods.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_10": {"mid": "2000145992", "abstract": "", "ref_function": [], "cite_purpose": ["uses"]}, "@cite_6": {"mid": "2001306626", "abstract": "Search engine logs are an emerging new type of data that offers interesting opportunities for data mining. Existing work on mining such data has mostly attempted to discover knowledge at the level of queries (e.g., query clusters). In this paper, we propose to mine search engine logs for patterns at the level of terms through analyzing the relations of terms inside a query. We define two novel term association patterns (i.e., context-sensitive term substitutions and term additions) and propose new methods for mining such patterns from search engine logs. These two patterns can be used to address the mis-specification and under-specification problems of ineffective queries. Experiment results on real search engine logs show that the mined context-sensitive term substitutions can be used to effectively reword queries and improve their accuracy, while the mined context-sensitive term addition patterns can be used to support query refinement in a more effective way.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_44": {"mid": "2105051853", "abstract": "We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion techniques on tfidf retrieval from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMTbased expansion improves retrieval performance over local expansion and over retrieval without expansion.", "ref_function": ["objective", "method", "result", "result"], "cite_purpose": ["uses"]}, "@cite_43": {"mid": "1990387666", "abstract": "This paper proposes an effective term suggestion approach to interactive Web search. Conventional approaches to making term suggestions involve extracting co-occurring keyterms from highly ranked retrieved documents. Such approaches must deal with term extraction difficulties and interference from irrelevant documents, and, more importantly, have difficulty extracting terms that are conceptually related but do not frequently co-occur in documents. In this paper, we present a new, effective log-based approach to relevant term extraction and term suggestion. Using this approach, the relevant terms suggested for a user query are those that co-occur in similar query sessions from search engine logs, rather than in the retrieved documents. In addition, the suggested terms in each interactive search step can be organized according to its relevance to the entire query session, rather than to the most recent single query as in conventional approaches. The proposed approach was tested using a proxy server log containing about two million query transactions submitted to search engines in Taiwan. The obtained experimental results show that the proposed approach can provide organized and highly relevant terms, and can exploit the contextual information in a user's query session to make more effective suggestions.", "ref_function": ["objective", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_40": {"mid": "2171743956", "abstract": "Query suggestion plays an important role in improving the usability of search engines. Although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context-aware - they do not take into account the immediately preceding queries as context in query suggestion. In this paper, we propose a novel context-aware query suggestion approach which is in two steps. In the offine model-learning step, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. Then, from session data a concept sequence suffix tree is constructed as the query suggestion model. In the online query suggestion step, a user's search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence sufix tree, our approach suggests queries to the user in a context-aware manner. We test our approach on a large-scale search log of a commercial search engine containing 1:8 billion search queries, 2:6 billion clicks, and 840 million query sessions. The experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["uses"]}, "@cite_19": {"mid": "1976719972", "abstract": "Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user's perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by (1) learning \"interesting aspects\" of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.", "ref_function": ["background", "background", "background", "objective", "result", "result"], "cite_purpose": ["uses"]}, "@cite_13": {"mid": "2129235726", "abstract": "The performance of web search engines may often deteriorate due to the diversity and noisy information contained within web pages. User click-through data can be used to introduce more accurate description (metadata) for web pages, and to improve the search performance. However, noise and incompleteness, sparseness, and the volatility of web pages and queries are three major challenges for research work on user click-through log mining. In this paper, we propose a novel iterative reinforced algorithm to utilize the user click-through data to improve search performance. The algorithm fully explores the interrelations between queries and web pages, and effectively finds \"virtual queries\" for web pages and overcomes the challenges discussed above. Experiment results on a large set of MSN click-through log data show a significant improvement on search performance over the naive query log mining algorithm as well as the baseline search engine.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["uses"]}, "@cite_20": {"mid": "1898200041", "abstract": "Automatic query expansion may be used in document retrieval to improve search effectiveness. Traditional query expansion methods are based on the document collection itself. For example, pseudo-relevance feedback (PRF) assumes that the top retrieved documents are relevant, and uses the terms extracted from those documents for query expansion. However, there are other sources of evidence that can be used for expansion, some of which may give better search results with greater efficiency at query time. In this paper, we use the external evidence, especially the hints obtained from external web search engines to expand the original query. We explore 6 different methods using search engine query log, snippets and search result documents. We conduct extensive experiments, with state of the art PRF baselines and careful parameter tuning, on three TREC collections: AP, WT10g, GOV2. Log-based methods do not show consistent significant gains, despite being very efficient at query-time. Snippet-based expansion, using the summaries provided by an external search engine, provides significant effectiveness gains with good efficiency at query-time.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background", "uses"]}}}
{"sentences": ["In the context of web-based knowledge, anchor texts can play a role similar to the user's search queries because an anchor text to a page can serve as a brief summary of its content.", "Anchor texts were first used by McBryan @cite_28 for associating hyperlinks with linked pages as well as with the pages in which the anchor texts are found.", "Kraft and Zien @cite_56 also used anchor texts for QE; their experimental results suggest that anchor texts can be used to improve traditional QE based on query logs.", "Similarly, Dang and Croft @cite_3 suggested that anchor text could be an effective alternative to query logs.", "They demonstrated the effectiveness of QE techniques using log-based stemming through experiments on standard TREC collection dataset."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result"], "target_paper": "In the field of information retrieval, query expansion (QE) has long been used as a technique to deal with the fundamental issue of word mismatch between a user's query and the target information. In the context of the relationship between the query and expanded terms, existing weighting techniques often fail to appropriately capture the term-term relationship and term to the whole query relationship, resulting in low retrieval effectiveness. Our proposed QE approach addresses this by proposing three weighting models based on (1) tf-itf, (2) k-nearest neighbor (kNN) based cosine similarity, and (3) correlation score. Further, to extract the initial set of expanded terms, we use pseudo-relevant web knowledge consisting of the top N web pages returned by the three popular search engines namely, Google, Bing, and DuckDuckGo, in response to the original query. Among the three weighting models, tf-itf scores each of the individual terms obtained from the web content, kNN-based cosine similarity scores the expansion terms to obtain the term-term relationship, and correlation score weighs the selected expansion terms with respect to the whole query. The proposed model, called web knowledge based query expansion (WKQE), achieves an improvement of 25.89 on the MAP score and 30.83 on the GMAP score over the unexpanded queries on the FIRE dataset. A comparative analysis of the WKQE techniques with other related approaches clearly shows significant improvement in the retrieval performance. We have also analyzed the effect of varying the number of pseudo-relevant documents and expansion terms on the retrieval effectiveness of the proposed model.", "reference": {"@cite_28": {"mid": "2976375847", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2080825533", "abstract": "Query reformulation techniques based on query logs have been studied as a method of capturing user intent and improving retrieval effectiveness. The evaluation of these techniques has primarily, however, focused on proprietary query logs and selected samples of queries. In this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a query log and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard TREC collections. Our results show that log-based query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. We also show that using anchor text as a simulated query log is as least as effective as a real log for these techniques.", "ref_function": ["background", "background", "objective", "result", "result"], "cite_purpose": ["background"]}, "@cite_56": {"mid": "2171161922", "abstract": "When searching large hypertext document collections, it is often possible that there are too many results available for ambiguous queries. Query refinement is an interactive process of query modification that can be used to narrow down the scope of search results. We propose a new method for automatically generating refinements or related terms to queries by mining anchor text for a large hypertext document collection. We show that the usage of anchor text as a basis for query refinement produces high quality refinement suggestions that are significantly better in terms of perceived usefulness compared to refinements that are derived using the document content. Furthermore, our study suggests that anchor text refinements can also be used to augment traditional query refinement algorithms based on query logs, since they typically differ in coverage and produce different refinements. Our results are based on experiments on an anchor text collection of a large corporate intranet.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The wavelet transform is a powerful tool for processing data and developing time-frequency representations.", "A thorough theoretical background on wavelets is explained in @cite_41 @cite_25 .", "Applying wavelet transform in the context of neural networks is not novel.", "Earlier work @cite_40 @cite_27 has presented a theoretical approach for wavelet-based feed-forward neural networks.", "The ability to use wavelet based interpolation for real time unknown function approximation has been researched by Bernard @cite_24 .", "In this case, the results have been achieved with a relatively less number of coefficients due to the high compression ability of the wavelets.", "The work by Alexandridis @cite_33 has proposed a statistical model identification framework in applying wavelet networks and it is investigated under many subjects including architecture, initialization, variable and model selection.", "Literature indicates applications of wavelet based neural networks in many different fields such as signal classification and compression @cite_11 @cite_28 @cite_8 , in time series predicting @cite_35 @cite_19 @cite_44 , electrical load forecasting @cite_45 @cite_5 and power distribution recognition @cite_10 ."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: research objective", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: research objective"], "target_paper": "Despite the remarkable success of deep learning in pattern recognition, deep network models face the problem of training a large number of parameters. In this paper, we propose and evaluate a novel multi-path wavelet neural network architecture for image classification with far less number of trainable parameters. The model architecture consists of a multi-path layout with several levels of wavelet decompositions performed in parallel followed by fully connected layers. These decomposition operations comprise wavelet neurons with learnable parameters, which are updated during the training phase using the back-propagation algorithm. We evaluate the performance of the introduced network using common image datasets without data augmentation except for SVHN and compare the results with influential deep learning models. Our findings support the possibility of reducing the number of parameters significantly in deep neural networks without compromising its accuracy.", "reference": {"@cite_35": {"mid": "2734777338", "abstract": "The application of deep learning approaches to finance has received a great deal of attention from both investors and researchers. This study presents a novel deep learning framework where wavelet transforms (WT), stacked autoencoders (SAEs) and long-short term memory (LSTM) are combined for stock price forecasting. The SAEs for hierarchically extracted deep features is introduced into stock price forecasting for the first time. The deep learning framework comprises three stages. First, the stock price time series is decomposed by WT to eliminate noise. Second, SAEs is applied to generate deep high-level features for predicting the stock price. Third, high-level denoising features are fed into LSTM to forecast the next day\u2019s closing price. Six market indices and their corresponding index futures are chosen to examine the performance of the proposed model. Results show that the proposed model outperforms other similar models in both predictive accuracy and profitability performance.", "ref_function": ["background", "objective", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2006447203", "abstract": "Wavelet networks (WNs) are a new class of networks which have been used with great success in a wide range of applications. However a general accepted framework for applying WNs is missing from the literature. In this study, we present a complete statistical model identification framework in order to apply WNs in various applications. The following subjects were thoroughly examined: the structure of a WN, training methods, initialization algorithms, variable significance and variable selection algorithms, model selection methods and finally methods to construct confidence and prediction intervals. In addition the complexity of each algorithm is discussed. Our proposed framework was tested in two simulated cases, in one chaotic time series described by the Mackey-Glass equation and in three real datasets described by daily temperatures in Berlin, daily wind speeds in New York and breast cancer classification. Our results have shown that the proposed algorithms produce stable and robust results indicating that our proposed framework can be applied in various applications.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "1970876195", "abstract": "Since EEG is one of the most important sources of information in therapy of epilepsy, several researchers tried to address the issue of decision support for such a data. In this paper, we introduce two fundamentally different approaches for designing classification models (classifiers); the traditional statistical method based on logistic regression and the emerging computationally powerful techniques based on artificial neural networks (ANNs). Logistic regression as well as feedforward error backpropagation artificial neural networks (FEBANN) and wavelet neural networks (WNN) based classifiers were developed and compared in relation to their accuracy in classification of EEG signals. In these methods we used FFT and autoregressive (AR) model by using maximum likelihood estimation (MLE) of EEG signals as an input to classification system with two discrete outputs: epileptic seizure or nonepileptic seizure. By identifying features in the signal we want to provide an automatic system that will support a physician in the diagnosing process. By applying AR with MLE in connection with WNN, we obtained novel and reliable classifier architecture. The network is constructed by the error backpropagation neural network using Morlet mother wavelet basic function as node activation function. The comparisons between the developed classifiers were primarily based on analysis of the receiver operating characteristic (ROC) curves as well as a number of scalar performance measures pertaining to the classification. The WNN-based classifier outperformed the FEBANN and logistic regression based counterpart. Within the same group, the WNN-based classifier was more accurate than the FEBANN-based classifier, and the logistic regression-based classifier.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_41": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_28": {"mid": "2290765693", "abstract": "It is known that the force and vibration sensor signals in a turning process are sensitive to the gradually increasing flank wear. Based on this fact, this paper investigates a flank wear assessment technique in turning through force and vibration signals. Mainly to reduce the computational burden associated with the existing sensor-based methods for flank wear assessment, a so-called wavelet network is investigated. The basic idea in this new method is to optimize simultaneously the wavelet parameters (that represent signal features) and the signal-interpretation parameters (that are equivalent to neural network weights) to eliminate the feature extraction phase without increasing the computational complexity of the neural network. A neural network architecture similar to a standard one-hidden-layer feedforward neural network is used to relate sensor signal measurements to flank wear classes. A novel training algorithm for such a network is developed. The performance of this n ew method is compared with a previously developed flank wear assessment method which uses a separate feature extraction step. The proposed wavelet network can also be useful for developing signal interpretation schemes for manufacturing process monitoring, critical component monitoring, and product quality monitoring.", "ref_function": ["background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_24": {"mid": "346668388", "abstract": "We describe a new approach to real time learning of unknown functions based on an interpolating wavelet estimation. We choose a subfamily of a wavelet basis relying on nested hierarchical allocation and update in real time our estimate of the unknown function. Such an interpolation process can be used for real time applications like neural network adaptive control, where learning an unknown function very fast is critical.", "ref_function": ["objective", "method", "method"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2030032088", "abstract": "A new technique, wavelet network, is introduced to predict chaotic time series. By using this technique, firstly, we make accurate short-term predictions of the time series from chaotic attractors. Secondly, we make accurate predictions of the values and bifurcation structures of the time series from dynamical systems whose parameter values are changing with time. Finally we predict chaotic attractors by making long-term predictions based on remarkably few data points, where the correlation dimensions of predicted attractors are calculated and are found to be almost identical to those of actual attractors.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_27": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_40": {"mid": "2171506994", "abstract": "A representation of a class of feedforward neural networks in terms of discrete affine wavelet transforms is developed. It is shown that by appropriate grouping of terms, feedforward neural networks with sigmoidal activation functions can be viewed as architectures which implement affine wavelet decompositions of mappings. It is shown that the wavelet transform formalism provides a mathematical framework within which it is possible to perform both analysis and synthesis of feedforward networks. For the purpose of analysis, the wavelet formulation characterizes a class of mappings which can be implemented by feedforward networks as well as reveals an exact implementation of a given mapping in this class. Spatio-spectral localization properties of wavelets can be exploited in synthesizing a feedforward network to perform a given approximation task. Two synthesis procedures based on spatio-spectral localization that reduce the training problem to one of convex optimization are outlined. >", "ref_function": ["background", "background", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_44": {"mid": "1980418485", "abstract": "A local linear wavelet neural network (LLWNN) is presented in this paper. The difference of the network with conventional wavelet neural network (WNN) is that the connection weights between the hidden layer and output layer of conventional WNN are replaced by a local linear model. A hybrid training algorithm of particle swarm optimization (PSO) with diversity learning and gradient descent method is introduced for training the LLWNN. Simulation results for the prediction of time-series show the feasibility and effectiveness of the proposed method.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_45": {"mid": "2094887027", "abstract": "We propose a wavelet multiscale decomposition-based autoregressive approach for the prediction of 1-h ahead load based on historical electricity load data. This approach is based on a multiple resolution decomposition of the signal using the non-decimated or redundant Haar a trous wavelet transform whose advantage is taking into account the asymmetric nature of the time-varying data. There is an additional computational advantage in that there is no need to recompute the wavelet transform (wavelet coefficients) of the full signal if the electricity data (time series) is regularly updated. We assess results produced by this multiscale autoregressive (MAR) method, in both linear and non-linear variants, with single resolution autoregression (AR), multilayer perceptron (MLP), Elman recurrent neural network (ERN) and the general regression neural network (GRNN) models. Results are based on the New South Wales (Australia) electricity load data that is provided by the National Electricity Market Management Company (NEMMCO).", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2288990565", "abstract": "We consider a wavelet neural network approach for electricity load prediction. The wavelet transform is used to decompose the load into different frequency components that are predicted separately using neural networks. We firstly propose a new approach for signal extension which minimizes the border distortion when decomposing the data, outperforming three standard methods. We also compare the performance of the standard wavelet transform, which is shift variant, with a non-decimated transform, which is shift invariant. Our results show that the use of shift invariant transform considerably improves the prediction accuracy. In addition to wavelet neural network, we also present the results of wavelet linear regression, wavelet model trees and a number of baselines. Our evaluation uses two years of Australian electricity data.", "ref_function": ["background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2105693855", "abstract": "In this paper, a prototype wavelet-based neural-network classifier for recognizing power-quality disturbances is implemented and tested under various transient events. The discrete wavelet transform (DWT) technique is integrated with the probabilistic neural-network (PNN) model to construct the classifier. First, the multiresolution-analysis technique of DWT and the Parseval's theorem are employed to extract the energy distribution features of the distorted signal at different resolution levels. Then, the PNN classifies these extracted features to identify the disturbance type according to the transient duration and the energy features. Since the proposed methodology can reduce a great quantity of the distorted signal features without losing its original property, less memory space and computing time are required. Various transient events tested, such as momentary interruption, capacitor switching, voltage sag swell, harmonic distortion, and flicker show that the classifier can detect and classify different power disturbance types efficiently.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2115755118", "abstract": "Introduction to a Transient World. Fourier Kingdom. Discrete Revolution. Time Meets Frequency. Frames. Wavelet Zoom. Wavelet Bases. Wavelet Packet and Local Cosine Bases. An Approximation Tour. Estimations are Approximations. Transform Coding. Appendix A: Mathematical Complements. Appendix B: Software Toolboxes.", "ref_function": ["background", "background", "background", "background", "background", "other", "background", "background", "background", "background", "other", "other", "other"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2075621527", "abstract": "Abstract This paper discusses the concept of adaptive wavelets and their application to signal compression and classification. When wavelet non-linear functions are used in the neurons of a neural network we can estimate the wavelet parameters\u2014scale and translation and the relative importance (weight) of each basis function optimally with respect to approximating a given function (signal) in the minimum mean square error sense or with respect to classifying a function (signal) with minimum mean classification error. This estimation can be considered as adaptive (signal or function dependent) sampling. In this paper, we give theoretical proof to show that such an adaptive sampling scheme constitutes a frame which implies that the neural network-based estimation technique allows us to reconstruct the input signal from the adaptive wavelets such that the reconstruction is numerically stable. We apply the proposed representation and classification architectures for coding and classifying biological signals such as electrocardiogram (ECG). Experimental details of ECG coder, and classification of ECG waves such as P, QRS and T are provided. The experimental results reported in this paper are obtained by applying the proposed methodologies to standard ECG (American Heart Association) data base. The results indicate that the compression ratio of about twice better than the current techniques can be obtained and average classification accuracy of 94 can be obtained for abnormal P, QRS and T classification.", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The connectivity (respectively, @math -connectivity) of wireless sensor networks secured by the classical scheme under a uniform on off channel model was investigated in @cite_33 (respectively, @cite_11 ).", "The network was modeled by a composite random graph formed by the intersection of random key graphs @math (induced by scheme) with graphs @math (induced by the uniform on-off channel model).", "Our paper generalizes this model to heterogeneous setting where different nodes could be given different number of keys depending on their respective classes and the availability of a wireless channel between two nodes depends on their respective classes.", "Hence, our model highly resembles emerging wireless sensor networks which are essentially complex and heterogeneous."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "Describing used methods", "Describing the results"], "target_paper": "In this paper, we investigate the secure connectivity of wireless sensor networks utilizing the heterogeneous random key predistribution scheme, where each sensor node is classified as class- @math with probability @math for @math with @math and @math . A class- @math sensor is given @math cryptographic keys selected uniformly at random from a key pool of size @math . After deployment, two nodes can communicate securely if they share at least one cryptographic key. We consider the wireless connectivity of the network using a heterogeneous on-off channel model, where the channel between a class- @math node and a class- @math node is on (respectively, off) with probability @math (respectively, @math ) for @math . Collectively, two sensor nodes are adjacent if they i) share a cryptographic key and ii) have a wireless channel in between that is on. We model the overall network using a composite random graph obtained by the intersection of inhomogeneous random key graphs (IRKG) @math with inhomogeneous Erd o s-R 'enyi graphs (IERG) @math . The former graph is naturally induced by the heterogeneous random key predistribution scheme, while the latter is induced by the heterogeneous on-off channel model. More specifically, two nodes are adjacent in the composite graph if they are i) adjacent in the IRKG i.e., share a cryptographic key and ii) adjacent in the IERG, i.e., have an available wireless channel. We investigate the connectivity of the composite random graph and present conditions (in the form of zero-one laws) on how to scale its parameters so that it i) has no secure node which is isolated and ii) is securely connected, both with high probability when the number of nodes gets large. We also present numerical results to support these zero-one laws in the finite-node regime.", "reference": {"@cite_33": {"mid": "2165958223", "abstract": "We investigate the secure connectivity of wireless sensor networks under the random key distribution scheme of Eschenauer and Gligor. Unlike recent work which was carried out under the assumption of full visibility, here we assume a (simplified) communication model where unreliable wireless links are represented as on off channels. We present conditions on how to scale the model parameters so that the network: 1) has no secure node which is isolated and 2) is securely connected, both with high probability when the number of sensor nodes becomes large. The results are given in the form of full zero-one laws, and constitute the first complete analysis of the EG scheme under non-full visibility. Through simulations, these zero-one laws are shown to be valid also under a more realistic communication model (i.e., the disk model). The relations to the Gupta and Kumar's conjecture on the connectivity of geometric random graphs with randomly deleted edges are also discussed.", "ref_function": ["background", "background", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2107866581", "abstract": "Random key graphs form a class of random intersection graphs that are naturally induced by the random key predistribution scheme of Eschenauer and Gligor for securing wireless sensor network (WSN) communications. Random key graphs have received much attention recently, owing in part to their wide applicability in various domains, including recommender systems, social networks, secure sensor networks, clustering and classification analysis, and cryptanalysis to name a few. In this paper, we study connectivity properties of random key graphs in the presence of unreliable links. Unreliability of graph links is captured by independent Bernoulli random variables, rendering them to be on or off independently from each other. The resulting model is an intersection of a random key graph and an Erd\u0151s\u2013Renyi graph, and is expected to be useful in capturing various real-world networks; e.g., with secure WSN applications in mind, link unreliability can be attributed to harsh environmental conditions severely impairing transmissions. We present conditions on how to scale this model\u2019s parameters so that: 1) the minimum node degree in the graph is at least @math and 2) the graph is @math -connected, both with high probability as the number of nodes becomes large. The results are given in the form of zero-one laws with critical thresholds identified and shown to coincide for both graph properties. These findings improve the previous results by Rybarczyk on @math -connectivity of random key graphs (with reliable links), as well as the zero-one laws by Yagan on one-connectivity of random key graphs with unreliable links.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In @cite_7 , Ya g an considered the connectivity of wireless sensor networks secured by the heterogeneous random key predistribution scheme under the full visibility assumption, i.e., all wireless channels are available and reliable, hence the only condition for two nodes to be adjacent is to share a key.", "It is clear that the full visibility assumption is not likely to hold in most practical deployments of wireless sensor networks as the wireless medium is typically unreliable.", "Our paper extends the results given in @cite_7 to more practical scenarios where the wireless connectivity is taken into account through the heterogeneous on-off channel model.", "In fact, by setting @math for @math and each @math (i.e., by assuming that all wireless channels are on ), our results reduce to those given in @cite_7 ."], "label": ["Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Describing used methods", "Explaining the result relationship between own work and references"], "target_paper": "In this paper, we investigate the secure connectivity of wireless sensor networks utilizing the heterogeneous random key predistribution scheme, where each sensor node is classified as class- @math with probability @math for @math with @math and @math . A class- @math sensor is given @math cryptographic keys selected uniformly at random from a key pool of size @math . After deployment, two nodes can communicate securely if they share at least one cryptographic key. We consider the wireless connectivity of the network using a heterogeneous on-off channel model, where the channel between a class- @math node and a class- @math node is on (respectively, off) with probability @math (respectively, @math ) for @math . Collectively, two sensor nodes are adjacent if they i) share a cryptographic key and ii) have a wireless channel in between that is on. We model the overall network using a composite random graph obtained by the intersection of inhomogeneous random key graphs (IRKG) @math with inhomogeneous Erd o s-R 'enyi graphs (IERG) @math . The former graph is naturally induced by the heterogeneous random key predistribution scheme, while the latter is induced by the heterogeneous on-off channel model. More specifically, two nodes are adjacent in the composite graph if they are i) adjacent in the IRKG i.e., share a cryptographic key and ii) adjacent in the IERG, i.e., have an available wireless channel. We investigate the connectivity of the composite random graph and present conditions (in the form of zero-one laws) on how to scale its parameters so that it i) has no secure node which is isolated and ii) is securely connected, both with high probability when the number of nodes gets large. We also present numerical results to support these zero-one laws in the finite-node regime.", "reference": {"@cite_7": {"mid": "2196704028", "abstract": "We introduce a new random key predistribution scheme for securing heterogeneous wireless sensor networks. Each of the @math sensors in the network is classified into @math classes according to some probability distribution @math . Before deployment, a class- @math sensor is assigned @math cryptographic keys that are selected uniformly at random from a common pool of @math keys. Once deployed, a pair of sensors can communicate securely if and only if they have a key in common. We model the communication topology of this network by a newly defined inhomogeneous random key graph. We establish scaling conditions on the parameters @math and @math so that this graph: 1) has no isolated nodes and 2) is connected, both with high probability. The results are given in the form of zero-one laws with the number of sensors @math growing unboundedly large; critical scalings are identified and shown to coincide for both graph properties. Our results are shown to complement and improve those given by and for the same model, therein referred to as the general random intersection graph.", "ref_function": ["background", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background", "extends", "similarities"]}}}
{"sentences": ["In comparison with the existing literature on similar models, our result can be seen to extend the work by Eletreby and Ya g an in @cite_8 (respectively, @cite_18 ).", "Therein, the authors established a zero-one law for the @math -connectivity (respectively, @math -connectivity) of @math , i.e., for a wireless sensor network under the heterogeneous key predistribution scheme and a uniform on-off channel model.", "Although these results form a crucial starting point towards the analysis of the heterogeneous key predistribution scheme under a wireless connectivity model, they are limited to uniform on-off channel model where all channels are on (respectively, off) with the same probability @math (respectively, @math ).", "The heterogeneous on-off channel model accounts for the fact that different nodes could have different radio capabilities, or could be deployed in locations with different channel characteristics.", "In addition, it offers the flexibility of modeling several interesting scenarios, such as when nodes of the same type are more (or less) likely to be adjacent with one another than with nodes belonging to other classes.", "Indeed, by setting @math for @math and each @math , our results reduce to those given in @cite_8 ."], "label": ["Explaining the method relationship between own work and references", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Reference to current state of knowledge", "Reference to current state of knowledge", "Explaining the result relationship between own work and references"], "target_paper": "In this paper, we investigate the secure connectivity of wireless sensor networks utilizing the heterogeneous random key predistribution scheme, where each sensor node is classified as class- @math with probability @math for @math with @math and @math . A class- @math sensor is given @math cryptographic keys selected uniformly at random from a key pool of size @math . After deployment, two nodes can communicate securely if they share at least one cryptographic key. We consider the wireless connectivity of the network using a heterogeneous on-off channel model, where the channel between a class- @math node and a class- @math node is on (respectively, off) with probability @math (respectively, @math ) for @math . Collectively, two sensor nodes are adjacent if they i) share a cryptographic key and ii) have a wireless channel in between that is on. We model the overall network using a composite random graph obtained by the intersection of inhomogeneous random key graphs (IRKG) @math with inhomogeneous Erd o s-R 'enyi graphs (IERG) @math . The former graph is naturally induced by the heterogeneous random key predistribution scheme, while the latter is induced by the heterogeneous on-off channel model. More specifically, two nodes are adjacent in the composite graph if they are i) adjacent in the IRKG i.e., share a cryptographic key and ii) adjacent in the IERG, i.e., have an available wireless channel. We investigate the connectivity of the composite random graph and present conditions (in the form of zero-one laws) on how to scale its parameters so that it i) has no secure node which is isolated and ii) is securely connected, both with high probability when the number of nodes gets large. We also present numerical results to support these zero-one laws in the finite-node regime.", "reference": {"@cite_18": {"mid": "2555492793", "abstract": "We consider secure and reliable connectivity in wireless sensor networks that utilize the heterogeneous random key predistribution scheme. We model the unreliability of wireless links by an on off channel model that induces an Erd\u0151s-Renyi graph, while the heterogeneous scheme induces an inhomogeneous random key graph. The overall network can thus be modeled by the intersection of both graphs. We present conditions (in the form of zero-one laws) on how to scale the parameters of the intersection model, so that with high probability: i) all of its nodes are connected to at least @math other nodes, i.e., the minimum node degree of the graph is no less than @math , and ii) the graph is @math -connected, i.e., the graph remains connected even if any @math nodes leave the network. These results are shown to complement and generalize several previous results in the literature. We also present numerical results to support our findings in the finite-node regime. Finally, we demonstrate via simulations that our results are also useful when the on off channel model is replaced with the more realistic disk communication model .", "ref_function": ["background", "background", "background", "method", "result", "result", "result"], "cite_purpose": ["differences"]}, "@cite_8": {"mid": "2963495066", "abstract": "We investigate the connectivity of a wireless sensor network (WSN) secured by the heterogeneous key predistribution scheme under an independent on off channel model. The heterogeneous scheme induces an inhomogeneous random key graph, denoted by @math and the on off channel model induces an Erd\u0151s-Renyi graph, denoted by @math . Hence, the overall random graph modeling the WSN is obtained by the intersection of @math and @math . We present conditions on how to scale the parameters of the intersecting graph with respect to the network size @math such that the graph i) has no isolated nodes and ii) is connected, both with high probability (whp) as the number of nodes gets large. Our results are supported by a simulation study demonstrating that i) despite their asymptotic nature, our results can in fact be useful in designing finite -node WSNs so that they achieve secure connectivity whp; and ii) despite the simplicity of the on off communication model, the probability of connectivity in the resulting WSN approximates very well the case where the disk model is used.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["differences", "similarities"]}}}
{"sentences": ["Heuristic weight pruning methods @cite_4 are widely used in neuromorphic computing designs to reduce the weight storage and computing delay @cite_20 .", "@cite_20 implemented weight pruning techniques on a neuromorphic computing system using irregular pruning caused unbalanced workload, greater circuits overheads and extra memory requirement on indices.", "To overcome the limitations, @cite_21 proposed group connection deletion, which structually prunes connections to reduce routing congestion between memristor crossbar arrays."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "The state-of-art DNN structures involve intensive computation and high memory storage. To mitigate the challenges, the memristor crossbar array has emerged as an intrinsically suitable matrix computation and low-power acceleration framework for DNN applications. However, the high accuracy solution for extreme model compression on memristor crossbar array architecture is still waiting for unraveling. In this paper, we propose a memristor-based DNN framework which combines both structured weight pruning and quantization by incorporating alternating direction method of multipliers (ADMM) algorithm for better pruning and quantization performance. We also discover the non-optimality of the ADMM solution in weight pruning and the unused data path in a structured pruned model. Motivated by these discoveries, we design a software-hardware co-optimization framework which contains the first proposed Network Purification and Unused Path Removal algorithms targeting on post-processing a structured pruned model after ADMM steps. By taking memristor hardware constraints into our whole framework, we achieve extreme high compression ratio on the state-of-art neural network structures with minimum accuracy loss. For quantizing structured pruned model, our framework achieves nearly no accuracy loss after quantizing weights to 8-bit memristor weight representation. We share our models at anonymous link this https URL.", "reference": {"@cite_21": {"mid": "2593769476", "abstract": "Synapse crossbar is an elementary structure in neuromorphic computing systems (NCS). However, the limited size of crossbars and heavy routing congestion impede the NCS implementation of large neural networks. In this paper, we propose a two-step framework (namely, group scissor) to scale NCS designs to large neural networks. The first step rank clipping integrates low-rank approximation into the training to reduce total crossbar area. The second step is group connection deletion, which structurally prunes connections to reduce routing congestion between crossbars. Tested on convolutional neural networks of LeNet on MNIST database and ConvNet on CIFAR-10 database, our experiments show significant reduction of crossbar and routing area in NCS designs. Without accuracy loss, rank clipping reduces the total crossbar area to 13.62 or 51.81 in the NCS design of LeNet or ConvNet, respectively. The following group connection deletion further decreases the routing area of LeNet or ConvNet to 8.1 or 52.06 , respectively.", "ref_function": ["background", "background", "objective", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2963674932", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2752640714", "abstract": "Implementation of Neuromorphic Systems using post Complementary Met al-Oxide-Semiconductor (CMOS) technology based Memristive Crossbar Array (MCA) has emerged as a promising solution to enable low-power acceleration of neural networks. However, the recent trend to design Deep Neural Networks (DNNs) for achieving human-like cognitive abilities poses significant challenges towards the scalable design of neuromorphic systems (due to the increase in computation storage demands). Network pruning [7] is a powerful technique to remove redundant connections for designing optimally connected (maximally sparse) DNNs. However, such pruning techniques induce irregular connections that are incoherent to the crossbar structure. Eventually they produce DNNs with highly inefficient hardware realizations (in terms of area and energy). In this work, we propose TraNNsformer - an integrated training framework that transforms DNNs to enable their efficient realization on MCA-based systems. TraNNsformer first prunes the connectivity matrix while forming clusters with the remaining connections. Subsequently, it retrains the network to fine tune the connections and reinforce the clusters. This is done iteratively to transform the original connectivity into an optimally pruned and maximally clustered mapping. We evaluated the proposed framework by transforming different Multi-Layer Perceptron (MLP) based Spiking Neural Networks (SNNs) on a wide range of datasets (MNIST, SVHN and CIFAR10) and executing them on MCA-based systems to analyze the area and energy benefits. Without accuracy loss, TraNNsformer reduces the area (energy) consumption by 28 - 55 (49 - 67 ) with respect to the original network. Compared to network pruning, TraNNsformer achieves 28 - 49 (15 - 29 ) area (energy) savings. Furthermore, TraNNsformer is a technology-aware framework that allows mapping a given DNN to any MCA size permissible by the memristive technology for reliable operations.", "ref_function": ["background", "background", "method", "method", "method", "objective", "background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["Weight quantization can mitigate hardware imperfection of memristor including state drift and process variations, caused by the imperfect fabrication process or by the device feature itself @cite_3 @cite_10 .", "@cite_1 presented a technique to reduce the overhead of Digital-to-Analog Converters (DACs) Analog-to-Digital Converters (ADCs) in resistive random-access memory (ReRAM) neuromorphic computing systems.", "They first normalized the data, and then quantized intermediary data to 1-bit value.", "This can be directly used as the analog input for ReRAM crossbar and, hence, avoids the need of DACs."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "The state-of-art DNN structures involve intensive computation and high memory storage. To mitigate the challenges, the memristor crossbar array has emerged as an intrinsically suitable matrix computation and low-power acceleration framework for DNN applications. However, the high accuracy solution for extreme model compression on memristor crossbar array architecture is still waiting for unraveling. In this paper, we propose a memristor-based DNN framework which combines both structured weight pruning and quantization by incorporating alternating direction method of multipliers (ADMM) algorithm for better pruning and quantization performance. We also discover the non-optimality of the ADMM solution in weight pruning and the unused data path in a structured pruned model. Motivated by these discoveries, we design a software-hardware co-optimization framework which contains the first proposed Network Purification and Unused Path Removal algorithms targeting on post-processing a structured pruned model after ADMM steps. By taking memristor hardware constraints into our whole framework, we achieve extreme high compression ratio on the state-of-art neural network structures with minimum accuracy loss. For quantizing structured pruned model, our framework achieves nearly no accuracy loss after quantizing weights to 8-bit memristor weight representation. We share our models at anonymous link this https URL.", "reference": {"@cite_1": {"mid": "2408724663", "abstract": "Convolutional Neural Network (CNN) is a powerful technique widely used in computer vision area, which also demands much more computations and memory resources than traditional solutions. The emerging met al-oxide resistive random-access memory (RRAM) and RRAM crossbar have shown great potential on neuromorphic applications with high energy efficiency. However, the interfaces between analog RRAM crossbars and digital peripheral functions, namely Analog-to-Digital Converters (AD-Cs) and Digital-to-Analog Converters (DACs), consume most of the area and energy of RRAM-based CNN design due to the large amount of intermediate data in CNN. In this paper, we propose an energy efficient structure for RRAM-based CNN. Based on the analysis of data distribution, a quantization method is proposed to transfer the intermediate data into 1 bit and eliminate DACs. An energy efficient structure using input data as selection signals is proposed to reduce the ADC cost for merging results of multiple crossbars. The experimental results show that the proposed method and structure can save 80 area and more than 95 energy while maintaining the same or comparable classification accuracy of CNN on MNIST.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2233116163", "abstract": "Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4 6\u00d7 speed-up and 15 20\u00d7 compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.", "ref_function": ["background", "background", "objective", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2748818695", "abstract": "Quantization is considered as one of the most effective methods to optimize the inference cost of neural network models for their deployment to mobile and embedded systems, which have tight resource constraints. In such approaches, it is critical to provide low-cost quantization under a tight accuracy loss constraint (e.g., 1 ). In this paper, we propose a novel method for quantizing weights and activations based on the concept of weighted entropy. Unlike recent work on binary-weight neural networks, our approach is multi-bit quantization, in which weights and activations can be quantized by any number of bits depending on the target accuracy. This facilitates much more flexible exploitation of accuracy-performance trade-off provided by different levels of quantization. Moreover, our scheme provides an automated quantization flow based on conventional training algorithms, which greatly reduces the design-time effort to quantize the network. According to our extensive evaluations based on practical neural network models for image classification (AlexNet, GoogLeNet and ResNet-50 101), object detection (R-FCN with 50-layer ResNet), and language modeling (an LSTM network), our method achieves significant reductions in both the model size and the amount of computation with minimal accuracy loss. Also, compared to existing quantization schemes, ours provides higher accuracy with a similar resource constraint and requires much lower design effort.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": [": Open set recognition was first introduced in @cite_19 , which considers the problem of detecting unseen classes that are never seen in the training phase @cite_2 @cite_27 .", "Many open-set recognition methods based on SVM @cite_31 @cite_24 and NCM @cite_9 have since been proposed, but all built on shallow models for classification.", "@cite_19 formulated the problem of open set recognition for static one-vs-all learning scenario by balancing open space risk while minimizing empirical error,going on to extend the work to multi-class settings by introducing a compact abating probability model @cite_34 .", "For the scalability problem, @cite_7 proposed the use of a scalable Weibull based calibration for hypothesis generation to model matching scores, but did not address its use for the general recognition problem.", "@cite_9 proposed a novel detection method dealing with deep model architecture by introducing an openmax layer, while @cite_16 proposed a one class classification based on the DCNN which can be used as a novel detector and outlier detector for a single known class.", "However, none have not addressed the problem of how to incrementally update their model after a new class has been recognized."], "label": ["General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "At present, object recognition studies are mostly conducted in a closed lab setting with classes in test phase typically in training phase. However, real-world problem is far more challenging because: i) new classes unseen in the training phase can appear when predicting; ii) discriminative features need to evolve when new classes emerge in real time; and iii) instances in new classes may not follow the \"independent and identically distributed\" (iid) assumption. Most existing work only aims to detect the unknown classes and is incapable of continuing to learn newer classes. Although a few methods consider both detecting and including new classes, all are based on the predefined handcrafted features that cannot evolve and are out-of-date for characterizing emerging classes. Thus, to address the above challenges, we propose a novel generic end-to-end framework consisting of a dynamic cascade of classifiers that incrementally learn their dynamic and inherent features. The proposed method injects dynamic elements into the system by detecting instances from unknown classes, while at the same time incrementally updating the model to include the new classes. The resulting cascade tree grows by adding a new leaf node classifier once a new class is detected, and the discriminative features are updated via an end-to-end learning strategy. Experiments on two real-world datasets demonstrate that our proposed method outperforms existing state-of-the-art methods.", "reference": {"@cite_7": {"mid": "2114759747", "abstract": "Algorithms based on RANSAC that estimate models using feature correspondences between images can slow down tremendously when the percentage of correct correspondences (inliers) is small. In this paper, we present a probabilistic parametric model that allows us to assign confidence values for each matching correspondence and therefore accelerates the generation of hypothesis models for RANSAC under these conditions. Our framework leverages Extreme Value Theory to accurately model the statistics of matching scores produced by a nearest-neighbor feature matcher. Using a new algorithm based on this model, we are able to estimate accurate hypotheses with RANSAC at low inlier ratios significantly faster than previous state-of-the-art approaches, while still performing comparably when the number of inliers is large. We present results of homography and fundamental matrix estimation experiments for both SIFT and SURF matches that demonstrate that our method leads to accurate and fast model estimations.", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_9": {"mid": "2963149653", "abstract": "Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class \u2013 deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. Open-Max allows rejection of \"fooling\" and unrelated open set images presented to the system, OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.", "ref_function": ["background", "background", "background", "other", "result", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background", "background"]}, "@cite_16": {"mid": "2783748519", "abstract": "We propose a deep learning-based solution for the problem of feature learning in one-class classification. The proposed method operates on top of a Convolutional Neural Network (CNN) of choice and produces descriptive features while maintaining a low intra-class variance in the feature space for the given class. For this purpose two loss functions, compactness loss and descriptiveness loss are proposed along with a parallel CNN architecture. A template matching-based framework is introduced to facilitate the testing process. Extensive experiments on publicly available anomaly detection, novelty detection and mobile active authentication datasets show that the proposed Deep One-Class (DOC) classification method achieves significant improvements over the state-of-the-art.", "ref_function": ["objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_24": {"mid": "2039229101", "abstract": "In this paper, we propose using local learning for multiclass novelty detection, a framework that we call local novelty detection. Estimating the novelty of a new sample is an extremely challenging task due to the large variability of known object categories. The features used to judge on the novelty are often very specific for the object in the image and therefore we argue that individual novelty models for each test sample are important. Similar to human experts, it seems intuitive to first look for the most related images thus filtering out unrelated data. Afterwards, the system focuses on discovering similarities and differences to those images only. Therefore, we claim that it is beneficial to solely consider training images most similar to a test sample when deciding about its novelty. Following the principle of local learning, for each test sample a local novelty detection model is learned and evaluated. Our local novelty score turns out to be a valuable indicator for deciding whether the sample belongs to a known category from the training set or to a new, unseen one. With our local novelty detection approach, we achieve state-of-the-art performance in multi-class novelty detection on two popular visual object recognition datasets, Caltech-256 and Image Net. We further show that our framework: (i) can be successfully applied to unknown face detection using the Labeled-Faces-in-the-Wild dataset and (ii) outperforms recent work on attribute-based unfamiliar class detection in fine-grained recognition of bird species on the challenging CUB-200-2011 dataset.", "ref_function": ["background", "background", "background", "method", "method", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2119880843", "abstract": "To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of \u201cclosed set\u201d recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is \u201copen set\u201d recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and formalizes its definition as a constrained minimization problem. The open set recognition problem is not well addressed by existing algorithms because it requires strong generalization. As a step toward a solution, we introduce a novel \u201c1-vs-set machine,\u201d which sculpts a decision space from the marginal distances of a 1-class or binary SVM with a linear kernel. This methodology applies to several different applications in computer vision where open set recognition is a challenging problem, including object recognition and face verification. We consider both in this work, with large scale cross-dataset experiments performed over the Caltech 256 and ImageNet sets, as well as face matching experiments performed over the Labeled Faces in the Wild set. The experiments highlight the effectiveness of machines adapted for open set evaluation compared to existing 1-class and binary SVMs for the same tasks.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_27": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2099064293", "abstract": "In this paper we propose a probabilistic model for online document clustering. We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. Furthermore, cluster uncertainty is modeled with a Bayesian Dirichlet-multinomial distribution. We use empirical Bayes method to estimate hyperparameters based on a historical dataset. Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature.", "ref_function": ["objective", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2015563892", "abstract": "We study large-scale image classification methods that can incorporate new classes and training images continuously over time at negligible cost. To this end, we consider two distance-based classifiers, the k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers, and introduce a new metric learning approach for the latter. We also introduce an extension of the NCM classifier to allow for richer class representations. Experiments on the ImageNet 2010 challenge dataset, which contains over 106 training images of 1,000 classes, show that, surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier. Moreover, the NCM performance is comparable to that of linear SVMs which obtain current state-of-the-art performance. Experimentally, we study the generalization performance to classes that were not used to learn the metrics. Using a metric learned on 1,000 classes, we show results for the ImageNet-10K dataset which contains 10,000 classes, and obtain performance that is competitive with the current state-of-the-art while being orders of magnitude faster. Furthermore, we show how a zero-shot class prior based on the ImageNet hierarchy can improve performance when few training images are available.", "ref_function": ["background", "method", "method", "method", "result", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "2018459374", "abstract": "Real-world tasks in computer vision often touch upon open set recognition: multi-class recognition with incomplete knowledge of the world and many unknown inputs. Recent work on this problem has proposed a model incorporating an open space risk term to account for the space beyond the reasonable support of known classes. This paper extends the general idea of open space risk limiting classification to accommodate non-linear classifiers in a multiclass setting. We introduce a new open set recognition model called compact abating probability (CAP), where the probability of class membership decreases in value (abates) as points move from known data toward open space. We show that CAP models improve open set recognition for multiple algorithms. Leveraging the CAP formulation, we go on to describe the novel Weibull-calibrated SVM (W-SVM) algorithm, which combines the useful properties of statistical extreme value theory for score calibration with one-class and binary support vector machines. Our experiments show that the W-SVM is significantly better for open set object detection and OCR problems when compared to the state-of-the-art for the same tasks.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": [": different from incremental learning problem, other researchers have proposed tree based classification methods to address the scalability of object categories in large scale visual recognition challenges @cite_6 @cite_10 @cite_18 @cite_4 .", "Recent advances in the deep learning domain @cite_1 @cite_13 of scalable learning have resulted in state of the art performances, which are extremely useful when the goal is to maximize classification recognition performance.", "These systems assume a priori availability of comprehensive training data containing both images and categories.", "However, adapting such methods to a dynamic learning scenario becomes extremely challenging.", "Adding object categories requires retraining the entire system, which could be unfeasible for many applications.", "As a result, these methods are scalable but not incremental."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: about results", "General reference to previous research or scholarship: approaches taken", "Explaining the inadequacies of previous studies", "Reference to current state of knowledge", "Explaining the inadequacies of previous studies"], "target_paper": "At present, object recognition studies are mostly conducted in a closed lab setting with classes in test phase typically in training phase. However, real-world problem is far more challenging because: i) new classes unseen in the training phase can appear when predicting; ii) discriminative features need to evolve when new classes emerge in real time; and iii) instances in new classes may not follow the \"independent and identically distributed\" (iid) assumption. Most existing work only aims to detect the unknown classes and is incapable of continuing to learn newer classes. Although a few methods consider both detecting and including new classes, all are based on the predefined handcrafted features that cannot evolve and are out-of-date for characterizing emerging classes. Thus, to address the above challenges, we propose a novel generic end-to-end framework consisting of a dynamic cascade of classifiers that incrementally learn their dynamic and inherent features. The proposed method injects dynamic elements into the system by detecting instances from unknown classes, while at the same time incrementally updating the model to include the new classes. The resulting cascade tree grows by adding a new leaf node classifier once a new class is detected, and the discriminative features are updated via an end-to-end learning strategy. Experiments on two real-world datasets demonstrate that our proposed method outperforms existing state-of-the-art methods.", "reference": {"@cite_13": {"mid": "1686810756", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", "ref_function": ["background", "background", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_18": {"mid": "2157065343", "abstract": "We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by", "ref_function": ["background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2031489346", "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2618530766", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5 and 17.0 , respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3 , compared to 26.2 achieved by the second-best entry.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "1851597118", "abstract": "Class hierarchies are commonly used to reduce the complexity of the classification problem. This is crucial when dealing with a large number of categories. In this work, we evaluate class hierarchies currently constructed for visual recognition. We show that top-down as well as bottom-up approaches, which are commonly used to automatically construct hierarchies, incorporate assumptions about the separability of classes. Those assumptions do not hold for visual recognition of a large number of object categories. We therefore propose a modification which is appropriate for most top-down approaches. It allows to construct class hierarchies that postpone decisions in the presence of uncertainty and thus provide higher recognition accuracy. We also compare our method to a one-against-all approach and show how to control the speed-for-accuracy trade-off with our method. For the experimental evaluation, we use the Caltech-256 visual object classes dataset and compare to state-of-the-art methods.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "1967732418", "abstract": "Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Open world recognition considers both detection and learning to distinguish the new classes.", "proposed a NCM learning algorithm that relies on the estimation of a determined threshold in conjunction with the threshold counts on some known new classes @cite_38 .", "For a more practical situation, proposed an online-learning approach that involves the NBC classifier instead of NCM @cite_28 , while @cite_8 proposed an online learning for streaming data where new classes come continuously.", "It is worth noting that Bayesian non-parametric models @cite_32 @cite_12 are not related to our problem.", "Though they were originally proposed to identify mixed components or clusters in the test data that may cover unseen classes, their clusters are not themselves classes and multiple clusters must be mapped to one class manually."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Other comments", "Explaining the method relationship between own work and references"], "target_paper": "At present, object recognition studies are mostly conducted in a closed lab setting with classes in test phase typically in training phase. However, real-world problem is far more challenging because: i) new classes unseen in the training phase can appear when predicting; ii) discriminative features need to evolve when new classes emerge in real time; and iii) instances in new classes may not follow the \"independent and identically distributed\" (iid) assumption. Most existing work only aims to detect the unknown classes and is incapable of continuing to learn newer classes. Although a few methods consider both detecting and including new classes, all are based on the predefined handcrafted features that cannot evolve and are out-of-date for characterizing emerging classes. Thus, to address the above challenges, we propose a novel generic end-to-end framework consisting of a dynamic cascade of classifiers that incrementally learn their dynamic and inherent features. The proposed method injects dynamic elements into the system by detecting instances from unknown classes, while at the same time incrementally updating the model to include the new classes. The resulting cascade tree grows by adding a new leaf node classifier once a new class is detected, and the discriminative features are updated via an end-to-end learning strategy. Experiments on two real-world datasets demonstrate that our proposed method outperforms existing state-of-the-art methods.", "reference": {"@cite_38": {"mid": "1917989004", "abstract": "With the of advent rich classification models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimal downtime, even to learn. To handle these operational issues, we present the problem of Open World Recognition and formally define it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance \u201copen space risk\u201d and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm that evolves model efficiently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition.", "ref_function": ["background", "background", "background", "background", "method", "objective", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2963183879", "abstract": "This paper investigates an important problem in stream mining, i.e., classification under streaming emerging new classes or SENC . The SENC problem can be decomposed into three subproblems: detecting emerging new classes, classifying known classes, and updating models to integrate each new class as part of known classes. The common approach is to treat it as a classification problem and solve it using either a supervised learner or a semi-supervised learner. We propose an alternative approach by using unsupervised learning as the basis to solve this problem. The proposed method employs completely-random trees which have been shown to work well in unsupervised learning and supervised learning independently in the literature. The completely-random trees are used as a single common core to solve all three subproblems: unsupervised learning, supervised learning, and model update on data streams. We show that the proposed unsupervised-learning-focused method often achieves significantly better outcomes than existing classification-focused methods.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_28": {"mid": "2340646384", "abstract": "As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. Experimentally we validate our approach on two large-scale datasets in different learning scenarios. For all these scenarios our proposed methods outperform their non-online counterparts. We conclude that local and online learning is important to capture the full dynamics of open world recognition.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_32": {"mid": "2030962165", "abstract": "We present a new direction for semi-supervised learning where self-adjusting generative models replace fixed ones and unlabeled data can potentially improve learning even when labeled data is only partially-observed. We model each class data by a mixture model and use a hierarchical Dirichlet process (HDP) to model observed as well as unobserved classes. We extend the standard HDP model to accommodate unlabeled samples and introduce a new sharing strategy, within the context of Gaussian mixture models, that restricts sharing with covariance matrices while leaving the mean vectors free. Our research is mainly driven by real-world applications with evolving data-generating mechanisms where obtaining a fully-observed labeled data set is impractical. We demonstrate the feasibility of the proposed approach for semi-supervised learning in two such applications.", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["differences"]}, "@cite_12": {"mid": "2951532892", "abstract": "We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["differences"]}}}
{"sentences": ["Selection or clustering points in PF have been studied with applications to MOO algorithms.", "Firstly, a motivation is to store representative elements of a large PF (exponential sizes of PF are possible @cite_45 ) for exact methods or population meta-heuristics.", "Maximizing the quality of discrete representations of Pareto sets was studied with the hypervolume measure in the Hypervolume Subset Selection (HSS) problem @cite_31 @cite_43 .", "Secondly, a crucial issue in the design of population meta-heuristics for MOO problems is to select relevant solutions for operators like cross-over or mutation phases in evolutionary algorithms @cite_2 @cite_38 .", "Selecting knee-points is another known approach for such goals @cite_42 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method"], "target_paper": "This paper is motivated by real-life applications of bi-objective optimization. Having many non dominated solutions, one wishes to cluster the Pareto front using Euclidian distances. The p-center problems, both in the discrete and continuous versions, are proven solvable in polynomial time with a common dynamic programming algorithm. Having @math points to partition in @math clusters, the complexity is proven in @math (resp @math ) time and @math memory space for the continuous (resp discrete) @math -center problem. @math -center problems have complexities in @math . To speed-up the algorithm, parallelization issues are discussed. A posteriori, these results allow an application inside multi-objective heuristics to archive partial Pareto Fronts.", "reference": {"@cite_38": {"mid": "1990421110", "abstract": "In many multiobjective optimization problems, the Pareto Fronts and Sets contain a large number of solutions and this makes it difficult for the decision maker to identify the preferred ones. A possible way to alleviate this difficulty is to present to the decision maker a subset of a small number of solutions representatives of the Pareto Front characteristics. In this paper, a two-steps procedure is presented, aimed at identifying a limited number of representative solutions to be presented to the decision maker. Pareto Front solutions are first clustered into \"families\", which are then synthetically represented by a \"head-of-the-family\" solution. Level Diagrams are then used to represent, analyse and interpret the Pareto Front reduced to its head-of-the-family solutions. The procedure is applied to a reliability allocation case study of literature, in decision-making contexts both without or with explicit preferences by the decision maker on the objectives to be optimized.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_42": {"mid": "2729527533", "abstract": "The current boom of Unmanned Aerial Vehicles (UAVs) is increasing the number of potential industrial and research applications. One of the most demanded topics in this area is related to the automated planning of a UAVs swarm, controlled by one or several Ground Control Stations (GCSs). In this context, there are several variables that influence the selection of the most appropriate plan, such as the makespan, the cost or the risk of the mission. This problem can be seen as a Multi-Objective Optimization Problem (MOP). On previous approaches, the problem was modelled as a Constraint Satisfaction Problem (CSP) and solved using a Multi-Objective Genetic Algorithm (MOGA), so a Pareto Optimal Frontier (POF) was obtained. The main problem with this approach is based on the large number of obtained solutions, which hinders the selection of the best solution. This paper presents a new algorithm that has been designed to obtain the most significant solutions in the POF. This approach is based on Knee Points applied to MOGA. The new algorithm has been proved in a real scenario with different number of optimization variables, the experimental results show a significant improvement of the algorithm performance.", "ref_function": ["background", "background", "background", "method", "method", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_43": {"mid": "2039191483", "abstract": "One way of solving multiple objective mathematical programming problems is finding discrete representations of the efficient set. A modified goal of finding good discrete representations of the efficient set would contribute to the practicality of vector maximization algorithms. We define coverage, uniformity and cardinality as the three attributes of quality of discrete representations and introduce a framework that includes these attributes in which discrete representations can be evaluated, compared to each other, and judged satisfactory or unsatisfactory by a Decision Maker. We provide simple mathematical programming formulations that can be used to compute the coverage error of a given discrete representation. Our formulations are practically implementable when the problem under study is a multiobjective linear programming problem. We believe that the interactive algorithms along with the vector maximization methods can make use of our framework and its tools.", "ref_function": ["background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_45": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_2": {"mid": "1613273921", "abstract": "A unified view of metaheuristics This book provides a complete background on metaheuristics and shows readers how to design and implement efficient algorithms to solve complex optimization problems across a diverse range of applications, from networking and bioinformatics to engineering design, routing, and scheduling. It presents the main design questions for all families of metaheuristics and clearly illustrates how to implement the algorithms under a software framework to reuse both the design and code. Throughout the book, the key search components of metaheuristics are considered as a toolbox for: Designing efficient metaheuristics (e.g. local search, tabu search, simulated annealing, evolutionary algorithms, particle swarm optimization, scatter search, ant colonies, bee colonies, artificial immune systems) for optimization problems Designing efficient metaheuristics for multi-objective optimization problems Designing hybrid, parallel, and distributed metaheuristics Implementing metaheuristics on sequential and parallel machines Using many case studies and treating design and implementation independently, this book gives readers the skills necessary to solve large-scale optimization problems quickly and efficiently. It is a valuable reference for practicing engineers and researchers from diverse areas dealing with optimization or machine learning; and graduate students in computer science, operations research, control, engineering, business and management, and applied mathematics.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_31": {"mid": "2002682954", "abstract": "Optimizing the hypervolume indicator within evolutionary multiobjective optimizers has become popular in the last years. Recently, the indicator has been generalized to the weighted case to incorporate various user preferences into hypervolume-based search algorithms. There are two main open questions in this context: (i) how does the specified weight influence the distribution of a fixed number of points that maximize the weighted hypervolume indicator? (ii) how can the user articulate her preferences easily without specifying a certain weight distribution function? In this paper, we tackle both questions. First, we theoretically investigate optimal distributions of \u03bc points that maximize the weighted hypervolume indicator. Second, based on the obtained theoretical results, we propose a new approach to articulate user preferences within biobjective hypervolume-based optimization in terms of specifying a desired density of points on a predefined (imaginary) Pareto front. Within this approach, a new exact algorithm based on dynamic programming is proposed which selects the set of \u03bc points that maximizes the (weighted) hypervolume indicator. Experiments on various test functions show the usefulness of this new preference articulation approach and the agreement between theory and practice.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The HSS problem, maximizing the representativity of @math solutions among a PF of size @math , is known to be NP-hard in dimension 3 (and greater) since @cite_10 .", "An exact algorithm in @math and a polynomial-time approximation scheme for any constant dimension @math are also provided in @cite_10 .", "The 2d case is solvable in polynomial time thanks to a DP algorithm with a complexity in @math time and @math space provided in @cite_31 .", "The time complexity of the DP algorithm was improved in @math by @cite_26 and in @math by @cite_14 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "This paper is motivated by real-life applications of bi-objective optimization. Having many non dominated solutions, one wishes to cluster the Pareto front using Euclidian distances. The p-center problems, both in the discrete and continuous versions, are proven solvable in polynomial time with a common dynamic programming algorithm. Having @math points to partition in @math clusters, the complexity is proven in @math (resp @math ) time and @math memory space for the continuous (resp discrete) @math -center problem. @math -center problems have complexities in @math . To speed-up the algorithm, parallelization issues are discussed. A posteriori, these results allow an application inside multi-objective heuristics to archive partial Pareto Fronts.", "reference": {"@cite_14": {"mid": "2117068724", "abstract": "The hypervolume subset selection problem consists of finding a subset, with a given cardinality k, of a set of nondominated points that maximizes the hypervolume indicator. This problem arises in selection procedures of evolutionary algorithms for multiobjective optimization, for which practically efficient algorithms are required. In this article, two new formulations are provided for the two-dimensional variant of this problem. The first is a linear integer programming formulation that can be solved by solving its linear programming relaxation. The second formulation is a k-link shortest path formulation on a special digraph with the Monge property that can be solved by dynamic programming in time. This improves upon the result of in Bader 2009, and slightly improves upon the result of in Bringmann eti\u00be al. 2014b, which was developed independently from this work using different techniques. Numerical results are shown for several values of n and k.", "ref_function": ["background", "background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2002682954", "abstract": "Optimizing the hypervolume indicator within evolutionary multiobjective optimizers has become popular in the last years. Recently, the indicator has been generalized to the weighted case to incorporate various user preferences into hypervolume-based search algorithms. There are two main open questions in this context: (i) how does the specified weight influence the distribution of a fixed number of points that maximize the weighted hypervolume indicator? (ii) how can the user articulate her preferences easily without specifying a certain weight distribution function? In this paper, we tackle both questions. First, we theoretically investigate optimal distributions of \u03bc points that maximize the weighted hypervolume indicator. Second, based on the obtained theoretical results, we propose a new approach to articulate user preferences within biobjective hypervolume-based optimization in terms of specifying a desired density of points on a predefined (imaginary) Pareto front. Within this approach, a new exact algorithm based on dynamic programming is proposed which selects the set of \u03bc points that maximizes the (weighted) hypervolume indicator. Experiments on various test functions show the usefulness of this new preference articulation approach and the agreement between theory and practice.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2008049315", "abstract": "The goal of bi-objective optimization is to find a small set of good compromise solutions. A common problem for bi-objective evolutionary algorithms is the following subset selection problem (SSP): Given n solutions P \u2282 R2 in the objective space, select k solutions P* from P that optimize an indicator function. In the hypervolume SSP we want to select k points P* that maximize the hypervolume indicator IHYP(P*, r) for some reference point r \u2208 R2. Similarly, the e-indicator SSP aims at selecting k points P* that minimize the e-indicator Ie(P*,R) for some reference set R \u2282 R2 of size m (which can be R=P). We first present a new algorithm for the hypervolume SSP with runtime O(n (k + log n)). Our second main result is a new algorithm for the e-indicator SSP with runtime O(n log n + m log m). Both results improve the current state of the art runtimes by a factor of (nearly) @math and make the problems tractable for new applications. Preliminary experiments confirm that the theoretical results translate into substantial empirical runtime improvements.", "ref_function": ["objective", "background", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2624826130", "abstract": "Let B be a set of n axis-parallel boxes in d-dimensions such that each box has a corner at the origin and the other corner in the positive quadrant, and let k be a positive integer. We study the problem of selecting k boxes in B that maximize the volume of the union of the selected boxes. The research is motivated by applications in skyline queries for databases and in multicriteria optimization, where the problem is known as the hypervolume subset selection problem. It is known that the problem can be solved in polynomial time in the plane, while the best known algorithms in any dimension d>2 enumerate all size-k subsets. We show that: * The problem is NP-hard already in 3 dimensions. * In 3 dimensions, we break the enumeration of all size-k subsets, by providing an n^O(sqrt(k)) algorithm. * For any constant dimension d, we give an efficient polynomial-time approximation scheme.", "ref_function": ["background", "objective", "objective", "method", "result", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["We note that an affine 2d PF is a line in @math , clustering is equivalent to 1 dimensional cases.", "1-dimension K-means was proven to be solvable in polynomial time with a DP algorithm in @math time and @math space.", "This complexity was improved for a DP algorithm in @math time and @math space in @cite_5 .", "This is thus the complexity of K-means in an affine 2d PF.", "The specific case, already mentioned in the previous section, of the continuous p-center problem with centers on a straight line is more general that the case of an affine 2d PF, with a complexity proven in @math time and @math space by @cite_34 .", "2d cases of clustering problems can also be seen as specific cases of three-dimensional (3d) PF, affine 3d PF.", "Having NP-hard complexities proven for planar cases of clustering, which is the case for k-means, p-median, k-medoids, p-center problems since @cite_44 @cite_1 , it implies that the considered clustering problems are also NP-hard for 3d PF."], "label": ["Reference to current state of knowledge", "Reference to current state of knowledge", "Reference to single investigations in the past: about result", "Reference to current state of knowledge", "Reference to single investigations in the past: about result", "Reference to current state of knowledge", "Reference to current state of knowledge"], "target_paper": "This paper is motivated by real-life applications of bi-objective optimization. Having many non dominated solutions, one wishes to cluster the Pareto front using Euclidian distances. The p-center problems, both in the discrete and continuous versions, are proven solvable in polynomial time with a common dynamic programming algorithm. Having @math points to partition in @math clusters, the complexity is proven in @math (resp @math ) time and @math memory space for the continuous (resp discrete) @math -center problem. @math -center problems have complexities in @math . To speed-up the algorithm, parallelization issues are discussed. A posteriori, these results allow an application inside multi-objective heuristics to archive partial Pareto Fronts.", "reference": {"@cite_44": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2582351091", "abstract": "The @math -Means clustering problem on @math points is NP-Hard for any dimension @math , however, for the 1D case there exists exact polynomial time algorithms. Previous literature reported an @math time dynamic programming algorithm that uses @math space. It turns out that the problem has been considered under a different name more than twenty years ago. We present all the existing work that had been overlooked and compare the various solutions theoretically. Moreover, we show how to reduce the space usage for some of them, as well as generalize them to data structures that can quickly report an optimal @math -Means clustering for any @math . Finally we also generalize all the algorithms to work for the absolute distance and to work for any Bregman Divergence. We complement our theoretical contributions by experiments that compare the practical performance of the various algorithms.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_34": {"mid": "2755780781", "abstract": "Given a set P of n points and a straight line L, we study three important variations of minimum enclosing circle problem as follows:", "ref_function": ["background"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2032624334", "abstract": "An @math algorithm for the continuous p-center problem on a tree is presented. Following a sequence of previous algorithms, ours is the first one whose time bound in uniform in p and less than quadratic in n. We also present an @math algorithm for a weighted discrete p-center problem.", "ref_function": ["background", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["The problem of successive POI recommendation has received much attention recently @cite_22 @cite_8 @cite_23 @cite_11 .", "To predict where a user will visit next, we need to consider the relationship between POIs.", "However, existing private recommendation methods @cite_1 @cite_7 @cite_13 only focus on learning the relationship between users and items.", "Our research direction is to incorporate the relationship between POIs by adapting the transfer learning approach @cite_18 @cite_12 @cite_16 .", "Most transfer learning methods in collaborative filtering utilize auxiliary domain data by sharing the latent matrix between two different domain.", "In our work, we use two domain data from users' check-in history: visiting counts and POI transition patterns.", "We assume that the POI latent factors can bridge the user-POI and POI-POI relationships.", "To figure out the POI-POI relationship, we build a POI-POI matrix, which represents global preference transitions between two POIs.", "After that, in the learning process, users update their profile vector based on the visiting counts which describe user-POI relationship."], "label": ["General reference to previous research or scholarship: research objective", "Describing the objective", "General reference to previous research or scholarship: approaches taken", "Describing used methods", "Reference to current state of knowledge", "Describing used methods", "Describing used methods", "Describing used methods", "Describing used methods"], "target_paper": "A point-of-interest (POI) recommendation system plays an important role in location-based services (LBS) because it can help people to explore new locations and promote advertisers to launch ads to target users. Exiting POI recommendation methods need users' raw check-in data, which can raise location privacy breaches. Even worse, several privacy-preserving recommendation systems could not utilize the transition pattern in the human movement. To address these problems, we propose Successive Point-of-Interest REcommendation with Local differential privacy (SPIREL) framework. SPIREL employs two types of sources from users' check-in history: a transition pattern between two POIs and visiting counts of POIs. We propose a novel objective function for learning the user-POI and POI-POI relationships simultaneously. We further propose two privacy-preserving mechanisms to train our recommendation system. Experiments using two public datasets demonstrate that SPIREL achieves better POI recommendation quality while preserving stronger privacy for check-in history.", "reference": {"@cite_18": {"mid": "1502375784", "abstract": "Data sparsity is a major problem for collaborative filtering (CF) techniques in recommender systems, especially for new users and items. We observe that, while our target data are sparse for CF systems, related and relatively dense auxiliary data may already exist in some other more mature application domains. In this paper, we address the data sparsity problem in a target domain by transferring knowledge about both users and items from auxiliary data sources. We observe that in different domains the user feedbacks are often heterogeneous such as ratings vs. clicks. Our solution is to integrate both user and item knowledge in auxiliary data sources through a principled matrix-based transfer learning framework that takes into account the data heterogeneity. In particular, we discover the principle coordinates of both users and items in the auxiliary data matrices, and transfer them to the target domain in order to reduce the effect of data sparsity. We describe our method, which is known as coordinate system transfer or CST, and demonstrate its effectiveness in alleviating the data sparsity problem in collaborative filtering. We show that our proposed method can significantly outperform several state-of-the-art solutions for this problem.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["extends"]}, "@cite_22": {"mid": "1546409232", "abstract": "Personalized point-of-interest (POI) recommendation is a significant task in location-based social networks (LBSNs) as it can help provide better user experience as well as enable third-party services, e.g., launching advertisements. To provide a good recommendation, various research has been conducted in the literature. However, pervious efforts mainly consider the \"check-ins\" in a whole and omit their temporal relation. They can only recommend POI globally and cannot know where a user would like to go tomorrow or in the next few days. In this paper, we consider the task of successive personalized POI recommendation in LBSNs, which is a much harder task than standard personalized POI recommendation or prediction. To solve this task, we observe two prominent properties in the check-in sequence: personalized Markov chain and region localization. Hence, we propose a novel matrix factorization method, namely FPMC-LR, to embed the personalized Markov chains and the localized regions. Our proposed FPMC-LR not only exploits the personalized Markov chain in the check-in sequence, but also takes into account users' movement constraint, i.e., moving around a localized region. More importantly, utilizing the information of localized regions, we not only reduce the computation cost largely, but also discard the noisy information to boost recommendation. Results on two real-world LBSNs datasets demonstrate the merits of our proposed FPMC-LR.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2896723315", "abstract": "Probabilistic matrix factorization (PMF) plays a crucial role in recommendation systems. It requires a large amount of user data (such as user shopping records and movie ratings) to predict personal preferences, and thereby provides users high-quality recommendation services, which expose the risk of leakage of user privacy. Differential privacy, as a provable privacy protection framework, has been applied widely to recommendation systems. It is common that different individuals have different levels of privacy requirements on items. However, traditional differential privacy can only provide a uniform level of privacy protection for all users. In this paper, we mainly propose a probabilistic matrix factorization recommendation scheme with personalized differential privacy (PDP-PMF). It aims to meet users' privacy requirements specified at the item-level instead of giving the same level of privacy guarantees for all. We then develop a modified sampling mechanism (with bounded differential privacy) for achieving PDP. We also perform a theoretical analysis of the PDP-PMF scheme and demonstrate the privacy of the PDP-PMF scheme. In addition, we implement the probabilistic matrix factorization schemes both with traditional and with personalized differential privacy (DP-PMF, PDP-PMF) and compare them through a series of experiments. The results show that the PDP-PMF scheme performs well on protecting the privacy of each user and its recommendation quality is much better than the DP-PMF scheme.", "ref_function": ["background", "background", "background", "background", "result", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_8": {"mid": "2072609015", "abstract": "Location-based social networks (LBSNs) offer researchers rich data to study people's online activities and mobility patterns. One important application of such studies is to provide personalized point-of-interest (POI) recommendations to enhance user experience in LBSNs. Previous solutions directly predict users' preference on locations but fail to provide insights about users' preference transitions among locations. In this work, we propose a novel category-aware POI recommendation model, which exploits the transition patterns of users' preference over location categories to improve location recommendation accuracy. Our approach consists of two stages: (1) preference transition (over location categories) prediction, and (2) category-aware POI recommendation. Matrix factorization is employed to predict a user's preference transitions over categories and then her preference on locations in the corresponding categories. Real data based experiments demonstrate that our approach outperforms the state-of-the-art POI recommendation models by at least 39.75 in terms of recall.", "ref_function": ["background", "objective", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2296635479", "abstract": "Matrix factorization (MF) is a prevailing collaborative filtering method for building recommender systems. It requires users to upload their personal preferences to the recommender for performing MF, which raises serious privacy concerns. This paper proposes a differentially private MF mechanism that can prevent an untrusted recommender from learning any users' ratings or profiles. Our design decouples computations upon users' private data from the recommender to users, and makes the recommender aggregate local results in a privacy-preserving way. It uses the objective perturbation to make sure that the final item profiles satisfy differential privacy and solves the challenge to decompose the noise component for objective perturbation into small pieces that can be determined locally and independently by users. We also propose a third-party based mechanism to reduce noises added in each iteration and adapt our online algorithm to the dynamic setting that allows users to leave and join. The experiments show that our proposal is efficient and introduces acceptable side effects on the precision of results.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_23": {"mid": "2017921654", "abstract": "Point-of-Interest (POI) recommendation has become an important means to help people discover attractive locations. However, extreme sparsity of user-POI matrices creates a severe challenge. To cope with this challenge, viewing mobility records on location-based social networks (LBSNs) as implicit feedback for POI recommendation, we first propose to exploit weighted matrix factorization for this task since it usually serves collaborative filtering with implicit feedback better. Besides, researchers have recently discovered a spatial clustering phenomenon in human mobility behavior on the LBSNs, i.e., individual visiting locations tend to cluster together, and also demonstrated its effectiveness in POI recommendation, thus we incorporate it into the factorization model. Particularly, we augment users' and POIs' latent factors in the factorization model with activity area vectors of users and influence area vectors of POIs, respectively. Based on such an augmented model, we not only capture the spatial clustering phenomenon in terms of two-dimensional kernel density estimation, but we also explain why the introduction of such a phenomenon into matrix factorization helps to deal with the challenge from matrix sparsity. We then evaluate the proposed algorithm on a large-scale LBSN dataset. The results indicate that weighted matrix factorization is superior to other forms of factorization models and that incorporating the spatial clustering phenomenon into matrix factorization improves recommendation performance.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "1994576156", "abstract": "A major challenge for collaborative filtering (CF) techniques in recommender systems is the data sparsity that is caused by missing and noisy ratings. This problem is even more serious for CF domains where the ratings are expressed numerically, e.g. as 5-star grades. We assume the 5-star ratings are unordered bins instead of ordinal relative preferences. We observe that, while we may lack the information in numerical ratings, we sometimes have additional auxiliary data in the form of binary ratings. This is especially true given that users can easily express themselves with their preferences expressed as likes or dislikes for items. In this paper, we explore how to use these binary auxiliary preference data to help reduce the impact of data sparsity for CF domains expressed in numerical ratings. We solve this problem by transferring the rating knowledge from some auxiliary data source in binary form (that is, likes or dislikes), to a target numerical rating matrix. In particular, our solution is to model both the numerical ratings and ratings expressed as like or dislike in a principled way. We present a novel framework of Transfer by Collective Factorization (TCF), in which we construct a shared latent space collectively and learn the data-dependent effect separately. A major advantage of the TCF approach over the previous bilinear method of collective matrix factorization is that we are able to capture the data-dependent effect when sharing the data-independent knowledge. This allows us to increase the overall quality of knowledge transfer. We present extensive experimental results to demonstrate the effectiveness of TCF at various sparsity levels, and show improvements of our approach as compared to several state-of-the-art methods.", "ref_function": ["background", "background", "background", "method", "method", "result", "background", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["extends"]}, "@cite_13": {"mid": "2789607830", "abstract": "Recommender systems are collecting and analyzing user data to provide better user experience. However, several privacy concerns have been raised when a recommender knows user's set of items or their ratings. A number of solutions have been suggested to improve privacy of legacy recommender systems, but the existing solutions in the literature can protect either items or ratings only. In this paper, we propose a recommender system that protects both user's items and ratings. For this, we develop novel matrix factorization algorithms under local differential privacy (LDP). In a recommender system with LDP, individual users randomize their data themselves to satisfy differential privacy and send the perturbed data to the recommender. Then, the recommender computes aggregates of the perturbed data. This framework ensures that both user's items and ratings remain private from the recommender. However, applying LDP to matrix factorization typically raises utility issues with i) high dimensionality due to a large number of items and ii) iterative estimation algorithms. To tackle these technical challenges, we adopt dimensionality reduction technique and a novel binary mechanism based on sampling. We additionally introduce a factor that stabilizes the perturbed gradients. With MovieLens and LibimSeTi datasets, we evaluate recommendation accuracy of our recommender system and demonstrate that our algorithm performs better than the existing differentially private gradient descent algorithm for matrix factorization under stronger privacy requirements.", "ref_function": ["background", "background", "background", "objective", "method", "method", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}, "@cite_12": {"mid": "193545200", "abstract": "Data sparsity due to missing ratings is a major challenge for collaborative filtering (CF) techniques in recommender systems. This is especially true for CF domains where the ratings are expressed numerically. We observe that, while we may lack the information in numerical ratings, we may have more data in the form of binary ratings. This is especially true when users can easily express themselves with their likes and dislikes for certain items. In this paper, we explore how to use the binary preference data expressed in the form of like dislike to help reduce the impact of data sparsity of more expressive numerical ratings. We do this by transferring the rating knowledge from some auxiliary data source in binary form (that is, likes or dislikes), to a target numerical rating matrix. Our solution is to model both numerical ratings and like dislike in a principled way, using a novel framework of Transfer by Collective Factorization (TCF). In particular, we construct the shared latent space collectively and learn the data-dependent effect separately. A major advantage of the TCF approach over previous collective matrix factorization (or bifactorization) methods is that we are able to capture the data-dependent effect when sharing the data-independent knowledge, so as to increase the over-all quality of knowledge transfer. Experimental results demonstrate the effectiveness of TCF at various sparsity levels as compared to several state-of-the-art methods.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["extends"]}, "@cite_11": {"mid": "2807855639", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["Differential privacy @cite_15 is a rigorous privacy standard that requires the output of a DP mechanism should not reveal information specific to any individuals.", "DP requires a trusted data curator who collects original data from users.", "Recently, a local version of DP has been proposed.", "In the local setting, each user perturbs his her data and sends perturbed data to the data curator.", "Since the original data never leave users' devices, LDP mechanisms have the benefit of not requiring trusted data curator.", "Accordingly, many companies attempt to adopt LDP to collect data from the clients privately @cite_5 @cite_20 @cite_2 @cite_10 ."], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "General descriptions of the topic", "Reference to current state of knowledge", "Reference to current state of knowledge", "General reference to previous research or scholarship: approaches taken"], "target_paper": "A point-of-interest (POI) recommendation system plays an important role in location-based services (LBS) because it can help people to explore new locations and promote advertisers to launch ads to target users. Exiting POI recommendation methods need users' raw check-in data, which can raise location privacy breaches. Even worse, several privacy-preserving recommendation systems could not utilize the transition pattern in the human movement. To address these problems, we propose Successive Point-of-Interest REcommendation with Local differential privacy (SPIREL) framework. SPIREL employs two types of sources from users' check-in history: a transition pattern between two POIs and visiting counts of POIs. We propose a novel objective function for learning the user-POI and POI-POI relationships simultaneously. We further propose two privacy-preserving mechanisms to train our recommendation system. Experiments using two public datasets demonstrate that SPIREL achieves better POI recommendation quality while preserving stronger privacy for check-in history.", "reference": {"@cite_2": {"mid": "2963559079", "abstract": "The collection and analysis of telemetry data from user's devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users' privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.", "ref_function": ["background", "background", "background", "background", "background", "objective", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "1981029888", "abstract": "Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2517104773", "abstract": "We continue a line of research initiated in [10, 11] on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user. Previous work focused on the case of noisy sums, in which f = \u03a3 i g(x i ), where x i denotes the ith row of the database and g maps database rows to [0,1]. We extend the study to general functions f, proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f. Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case. The first step is a very clean characterization of privacy in terms of indistinguishability of transcripts. Additionally, we obtain separation results showing the increased value of interactive sanitization mechanisms over non-interactive.", "ref_function": ["background", "background", "background", "background", "background", "method", "result", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2421389337", "abstract": "Organizations with a large user base, such as Samsung and Google, can potentially benefit from collecting and mining users' data. However, doing so raises privacy concerns, and risks accidental privacy breaches with serious consequences. Local differential privacy (LDP) techniques address this problem by only collecting randomized answers from each user, with guarantees of plausible deniability; meanwhile, the aggregator can still build accurate models and predictors by analyzing large amounts of such randomized data. So far, existing LDP solutions either have severely restricted functionality, or focus mainly on theoretical aspects such as asymptotical bounds rather than practical usability and performance. Motivated by this, we propose Harmony, a practical, accurate and efficient system for collecting and analyzing data from smart device users, while satisfying LDP. Harmony applies to multi-dimensional data containing both numerical and categorical attributes, and supports both basic statistics (e.g., mean and frequency estimates), and complex machine learning tasks (e.g., linear regression, logistic regression and SVM classification). Experiments using real data confirm Harmony's effectiveness.", "ref_function": ["background", "background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["There are several works applying DP LDP on the recommendation system @cite_1 @cite_7 @cite_13 .", "@cite_1 proposed an objective function perturbation method.", "In their work, a trusted data curator adds Laplace noises to the objective function so that the factorized item matrix satisfies DP.", "They also proposed a gradient perturbation method which can preserve the privacy of users' ratings from an untrusted data curator.", "@cite_7 proposed a probabilistic matrix factorization with personalized differential privacy.", "They used a random sampling method to satisfy different users' privacy requirements.", "Then, they applied the objective function perturbation method to obtain the perturbed item matrix.", "Finally, @cite_13 proposed a new recommendation system under LDP.", "Specifically, users update their profile vectors locally and submit perturbed gradients in the iterative factorization process.", "Further, to reduce the error incurred by perturbation, they adopted random projection for dimensionality reduction."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "A point-of-interest (POI) recommendation system plays an important role in location-based services (LBS) because it can help people to explore new locations and promote advertisers to launch ads to target users. Exiting POI recommendation methods need users' raw check-in data, which can raise location privacy breaches. Even worse, several privacy-preserving recommendation systems could not utilize the transition pattern in the human movement. To address these problems, we propose Successive Point-of-Interest REcommendation with Local differential privacy (SPIREL) framework. SPIREL employs two types of sources from users' check-in history: a transition pattern between two POIs and visiting counts of POIs. We propose a novel objective function for learning the user-POI and POI-POI relationships simultaneously. We further propose two privacy-preserving mechanisms to train our recommendation system. Experiments using two public datasets demonstrate that SPIREL achieves better POI recommendation quality while preserving stronger privacy for check-in history.", "reference": {"@cite_13": {"mid": "2789607830", "abstract": "Recommender systems are collecting and analyzing user data to provide better user experience. However, several privacy concerns have been raised when a recommender knows user's set of items or their ratings. A number of solutions have been suggested to improve privacy of legacy recommender systems, but the existing solutions in the literature can protect either items or ratings only. In this paper, we propose a recommender system that protects both user's items and ratings. For this, we develop novel matrix factorization algorithms under local differential privacy (LDP). In a recommender system with LDP, individual users randomize their data themselves to satisfy differential privacy and send the perturbed data to the recommender. Then, the recommender computes aggregates of the perturbed data. This framework ensures that both user's items and ratings remain private from the recommender. However, applying LDP to matrix factorization typically raises utility issues with i) high dimensionality due to a large number of items and ii) iterative estimation algorithms. To tackle these technical challenges, we adopt dimensionality reduction technique and a novel binary mechanism based on sampling. We additionally introduce a factor that stabilizes the perturbed gradients. With MovieLens and LibimSeTi datasets, we evaluate recommendation accuracy of our recommender system and demonstrate that our algorithm performs better than the existing differentially private gradient descent algorithm for matrix factorization under stronger privacy requirements.", "ref_function": ["background", "background", "background", "objective", "method", "method", "background", "background", "background", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_1": {"mid": "2296635479", "abstract": "Matrix factorization (MF) is a prevailing collaborative filtering method for building recommender systems. It requires users to upload their personal preferences to the recommender for performing MF, which raises serious privacy concerns. This paper proposes a differentially private MF mechanism that can prevent an untrusted recommender from learning any users' ratings or profiles. Our design decouples computations upon users' private data from the recommender to users, and makes the recommender aggregate local results in a privacy-preserving way. It uses the objective perturbation to make sure that the final item profiles satisfy differential privacy and solves the challenge to decompose the noise component for objective perturbation into small pieces that can be determined locally and independently by users. We also propose a third-party based mechanism to reduce noises added in each iteration and adapt our online algorithm to the dynamic setting that allows users to leave and join. The experiments show that our proposal is efficient and introduces acceptable side effects on the precision of results.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_7": {"mid": "2896723315", "abstract": "Probabilistic matrix factorization (PMF) plays a crucial role in recommendation systems. It requires a large amount of user data (such as user shopping records and movie ratings) to predict personal preferences, and thereby provides users high-quality recommendation services, which expose the risk of leakage of user privacy. Differential privacy, as a provable privacy protection framework, has been applied widely to recommendation systems. It is common that different individuals have different levels of privacy requirements on items. However, traditional differential privacy can only provide a uniform level of privacy protection for all users. In this paper, we mainly propose a probabilistic matrix factorization recommendation scheme with personalized differential privacy (PDP-PMF). It aims to meet users' privacy requirements specified at the item-level instead of giving the same level of privacy guarantees for all. We then develop a modified sampling mechanism (with bounded differential privacy) for achieving PDP. We also perform a theoretical analysis of the PDP-PMF scheme and demonstrate the privacy of the PDP-PMF scheme. In addition, we implement the probabilistic matrix factorization schemes both with traditional and with personalized differential privacy (DP-PMF, PDP-PMF) and compare them through a series of experiments. The results show that the PDP-PMF scheme performs well on protecting the privacy of each user and its recommendation quality is much better than the DP-PMF scheme.", "ref_function": ["background", "background", "background", "background", "result", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["Our work is inspired by @cite_0 @cite_35 .", "Unlike these methods, however, our work attempts to achieve a good tradeoff between system performance and the availability of the computational resource.", "In other words, our algorithm is optimized with some constraints from real applications.", "We notice that the recent DPC work @cite_25 is very related to ours.", "It addresses the dense image prediction problem via searching an efficient multi-scale architecture on the use of performance driven random search @cite_23 .", "Nevertheless, our work is different from @cite_25 .", "First of all, we have different objectives.", "Instead of targeting high-quality segmentation in @cite_25 , our solution is customizable to search for an optimized architecture which is constrained by the requirements of real applications.", "The generated architecture tries to keep a balance between the quality and limited computational resource.", "Secondly, our solution optimizes the architecture of the whole network including both backbone and multi-scale module, while @cite_25 focuses on multi-scale optimization.", "Finally, our method employs a lightweight network, which costs much less training time as compared to that of @cite_25 ."], "label": ["Explaining the method relationship between own work and references", "Describing the objective", "Describing used methods", "Explaining the method relationship between own work and references", "Reference to single investigations in the past: about method", "Explaining the method relationship between own work and references", "Describing the objective", "Describing used methods", "Describing used methods", "Describing used methods", "Describing used methods"], "target_paper": "In this paper, we propose a Customizable Architecture Search (CAS) approach to automatically generate a network architecture for semantic image segmentation. The generated network consists of a sequence of stacked computation cells. A computation cell is represented as a directed acyclic graph, in which each node is a hidden representation (i.e., feature map) and each edge is associated with an operation (e.g., convolution and pooling), which transforms data to a new layer. During the training, the CAS algorithm explores the search space for an optimized computation cell to build a network. The cells of the same type share one architecture but with different weights. In real applications, however, an optimization may need to be conducted under some constraints such as GPU time and model size. To this end, a cost corresponding to the constraint will be assigned to each operation. When an operation is selected during the search, its associated cost will be added to the objective. As a result, our CAS is able to search an optimized architecture with customized constraints. The approach has been thoroughly evaluated on Cityscapes and CamVid datasets, and demonstrates superior performance over several state-of-the-art techniques. More remarkably, our CAS achieves 72.3 mIoU on the Cityscapes dataset with speed of 108 FPS on an Nvidia TitanXp GPU.", "reference": {"@cite_0": {"mid": "2810075754", "abstract": "", "ref_function": [], "cite_purpose": ["uses"]}, "@cite_35": {"mid": "2964081807", "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4 error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7 top-1 and 96.2 top-5 on ImageNet. Our model is 1.2 better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28 in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74 top-1 accuracy, which is 3.1 better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0 achieving 43.1 mAP on the COCO dataset.", "ref_function": ["background", "method", "method", "objective", "method", "method", "background", "background", "method", "result", "result", "result", "result"], "cite_purpose": ["uses"]}, "@cite_25": {"mid": "2891778567", "abstract": "The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7 on Cityscapes (street scene parsing), 71.3 on PASCAL-Person-Part (person-part segmentation), and 87.9 on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.", "ref_function": ["background", "background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["similarities", "differences", "differences", "differences", "differences"]}, "@cite_23": {"mid": "2732547613", "abstract": "Any sufficiently complex system acts as a black box when it becomes easier to experiment with than to understand. Hence, black-box optimization has become increasingly important as systems have become more complex. In this paper we describe Google Vizier, a Google-internal service for performing black-box optimization that has become the de facto parameter tuning engine at Google. Google Vizier is used to optimize many of our machine learning models and other systems, and also provides core capabilities to Google's Cloud Machine Learning HyperTune subsystem. We discuss our requirements, infrastructure design, underlying algorithms, and advanced features such as transfer learning and automated early stopping that the service provides.", "ref_function": ["background", "background", "objective", "method", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["This optimization problem is NP-hard @cite_9 , and was first introduced for the design of vacuum systems @cite_5 .", "It has then be studied independently in several different contexts, mainly dealing with network design: computer networks @cite_1 , social networks @cite_2 (more precisely modeling the communication paradigm @cite_6 @cite_16 @cite_4 ), but also other fields, such as auction systems @cite_3 and structural biology @cite_17 @cite_7 .", "Finally, we can mention the issue of hypergraph drawing, where, in addition to the connectivity constraints, one usually looks for graphs with additional properties ( planarity, having a tree-like structure... ) @cite_18 @cite_10 @cite_8 @cite_0 .", "This plethora of applications explains why this problem is known under different names, such as , or .", "For a comprehensive survey of the theoretical work done on this problem, see @cite_11 and the references therein."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: research objective", "General reference to previous research or scholarship: research objective", "Reference to current state of knowledge", "Signalling Transition"], "target_paper": "Given a hypergraph @math , the Minimum Connectivity Inference problem asks for a graph on the same vertex set as @math with the minimum number of edges such that the subgraph induced by every hyperedge of @math is connected. This problem has received a lot of attention these recent years, both from a theoretical and practical perspective, leading to several implemented approximation, greedy and heuristic algorithms. Concerning exact algorithms, only Mixed Integer Linear Programming (MILP) formulations have been experimented, all representing connectivity constraints by the means of graph flows. In this work, we investigate the efficiency of a constraint generation algorithm, where we iteratively add cut constraints to a simple ILP until a feasible (and optimal) solution is found. It turns out that our method is faster than the previous best flow-based MILP algorithm on random generated instances, which suggests that a constraint generation approach might be also useful for other optimization problems dealing with connectivity constraints. At last, we present the results of an enumeration algorithm for the problem.", "reference": {"@cite_18": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2013504716", "abstract": "The NP-hard Subset Interconnection Design problem, also known as Minimum Topic-Connected Overlay, is motivated by numerous applications including the design of scalable overlay networks and vacuum systems. It has as input a finite set @math and a collection of subsets @math , and asks for a minimum-cardinality edge set @math such that for the graph @math all induced subgraphs @math are connected. We study Subset Interconnection Design in the context of polynomial-time data reduction rules that preserve the possibility of constructing optimal solutions. Our contribution is threefold: First, we show the incorrectness of earlier polynomial-time data reduction rules. Second, we show linear-time solvability in case of a constant number @math of subsets, implying fixed-parameter tractability for the parameter @math . Third, we provide a fixed-parameter tractability result for small subset sizes and tree-like output graphs. To achieve our results, we elaborate o...", "ref_function": ["background", "background", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2134774739", "abstract": "Designing an overlay network for publish subscribe communication in a system where nodes may subscribe to many different topics of interest is of fundamental importance. For scalability and efficiency, it is important to keep the degree of the nodes in the publish subscribe system low. It is only natural then to formalize the following problem: Given a collection of nodes and their topic subscriptions connect the nodes into a graph which has least possible maximum degree and in such a way that for each topic t, the graph induced by the nodes interested in t is connected. We present the first polynomial time logarithmic approximation algorithm for this problem and prove an almost tight lower bound on the approximation ratio. Our experimental results show that our algorithm drastically improves the maximum degree of publish subscribe overlay systems. We also propose a variation of the problem by enforcing that each topic-connected overlay network be of constant diameter, while keeping the average degree low. We present a heuristic for this problem which guarantees that each topic-connected overlay network will be of diameter 2 and which aims at keeping the overall average node degree low. Our experimental results validate our algorithm showing that our algorithm is able to achieve very low diameter without increasing the average degree by much.", "ref_function": ["background", "background", "method", "method", "result", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2130739699", "abstract": "Consider a set of oligomers listing the subunits involved in sub-complexes of a macro-molecular assembly, obtained e.g. using native mass spectrometry or affinity purification. Given these oligomers, connectivity inference (CI) consists of finding the most plausible contacts between these subunits, and minimum connectivity inference (MCI) is the variant consisting of finding a set of contacts of smallest cardinality. MCI problems avoid speculating on the total number of contacts, but yield a subset of all contacts and do not allow exploiting a priori information on the likelihood of individual contacts. In this context, we present two novel algorithms, MILP-W and MILP-WB. The former solves the minimum weight connectivity inference (MWCI), an optimization problem whose criterion mixes the number of contacts and their likelihood. The latter uses the former in a bootstrap fashion, to improve the sensitivity and the specificity of solution sets. Experiments on three systems (yeast exosome, yeast proteasome lid, human eiF3), for which reference contacts are known (crystal structure, cryo electron microscopy, cross-linking), show that our algorithms predict contacts with high specificity and sensitivity, yielding a very significant improvement over previous work, typically a twofold increase in sensitivity. The software accompanying this paper is made available, and should prove of ubiquitous interest whenever connectivity inference from oligomers is faced.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "381489330", "abstract": "In this paper we present an O(n 2(m + logn))-time algorithm for computing a minimum-weight tree support (if one exists) of a hypergraph H = (V,S) with n vertices and m hyperedges. This improves the previously best known algorithm with running time O(n 4 m 2). A support of H is a graph G on V such that each hyperedge in S induces a connected subgraph in G. If G is a tree, it is called a tree support and it is a minimum tree support if its edge weight is minimum for a given edge weight function. Tree supports of hypergraphs have several applications, from social network analysis and network design problems to the visualization of hypergraphs and Euler diagrams. We show in particular how a minimum-weight tree support can be used to generate an area-proportional Euler diagram that satisfies typical well-formedness conditions and additionally minimizes the number of concurrent curves of the set boundaries in the Euler diagram.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_9": {"mid": "2080022329", "abstract": "A problem arising in the design of vacuum systems and having applications to some natural problems of interconnection design is described as follows. (1) Given a set X and subsets @math of @math , satisfying @math , find a graph G with vertex set X and the minimum number of edges such that for any i, the subgraph induced by @math has a connected component containing @math .Two other problems related to this one are the following ones. (2) Given a set X and subsets @math such that @math , find a graph G with vertex set X and the minimum number of edges such that for any i the subgraph @math induced by @math in G is connected. (3) Given a set X and subsets @math such that @math , find a graph G with vertex set X, find a graph G with vertex set X and the minimum number of edges such that for any subset I of @math , the subgraph induced by @math is co...", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "22994886", "abstract": "The Interconnection Graph Problem (IGP) is to compute for a given hypergraph H= (V, R) a graph G= (V, E) with the minimum number of edges |E| such that for all hyperedges N? Rthe subgraph of Ginduced by Nis connected. Computing feasible interconnection graphs is basically motivated by the design of reconfigurable interconnection networks. This paper proves that IGP is NP-complete and hard to approximate even when all hyperedges of Hhave at most three vertices. Afterwards it presents a search tree based parameterized algorithm showing that the problem is fixed-parameter tractable when the hyperedge size of His bounded. Moreover, the paper gives a reduction based greedy algorithm and closes with its experimental justification.", "ref_function": ["background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "2049693545", "abstract": "We investigate the problem of designing a scalable overlay network to support decentralized topic-based pub sub communication. We introduce a new optimization problem, called Minimum Topic-Connected Overlay (Min-TCO), that captures the tradeoff between the scalability of the overlay (in terms of the nodes' fanout) and the message forwarding overhead incurred by the communicating parties. Roughly, the Min-TCO problem is as follows: Given a collection of nodes and their subscriptions, connect the nodes using the minimum possible number of edges so that for each topic t, a message published on t could reach all the nodes interested in t by being forwarded by onlythe nodes interested in t. We show that the decision version of Min-TCO is NP-complete, and present a polynomial algorithm that approximates the optimal solution within a logarithmic factor with respect to the number of edges in theconstructed overlay. We further prove that this approximation ratio is almost tight by showing that no polynomial algorithm can approximate Min-TCO within a constant factor (unless P=NP). We show experimentally that on typical inputs, the fanout of the overlay constructed by our approximation algorithm is significantly lower thanthat of the overlays built by the existing algorithms, and that its running time is just a small fraction of the analytical worst case bound. As Min-TCO can be shown to capture several important aspects of most known overlay-based pub sub implementations, our study sheds light on the inherent limitations of the existing systems as well asprovides an insight into the best possible feasible solution. Finally, we introduce a flexible framework that generalizes Min-TCO and formalizes most similar overlay design problems that occur in scalable pub sub systems. We also briefly discuss several examples of such problems, and show some results with respect to their complexity.", "ref_function": ["background", "background", "method", "method", "method", "result", "method", "result"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "1543455987", "abstract": "Combinatorial auctions (CAs) are important mechanisms for allocating interrelated items. Unfortunately, winner determination is NP-complete unless there is special structure. We study the setting where there is a graph (with some desired property), with the items as vertices, and every bid bids on a connected set of items. Two computational problems arise: 1) clearing the auction when given the item graph, and 2) constructing an item graph (if one exists) with the desired property. 1 was previously solved for the case of a tree or a cycle, and 2 for the case of a line graph or a cycle. We generalize the first result by showing that given an item graph with bounded treewidth, the clearing problem can be solved in polynomial time (and every CA instance has some treewidth; the complexity is exponential in only that parameter). We then give an algorithm for constructing an item tree (treewidth 1) if such a tree exists, thus closing a recognized open problem. We show why this algorithm does not work for treewidth greater than 1, but leave open whether item graphs of (say) treewidth 2 can be constructed in polynomial time. We show that finding the item graph with the fewest edges is NP-complete (even when a graph of treewidth 2 exists). Finally, we study how the results change if a bid is allowed to have more than one connected component. Even for line graphs, we show that clearing is hard even with 2 components, and constructing the line graph is hard even with 5.", "ref_function": ["background", "background", "method", "method", "result", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2050339304", "abstract": "We consider the following problem: Given a complete graph G=(V,E) with a cost on every edge and a given collection of subsets of V, we have to find a minimum cost spanning tree T such that each subset of the vertices in the collection induces a subtree in T. One motivation for this problem is to construct a minimum cost communication tree network for a collection of non-disjoint groups of customers such that the network will provide group fault tolerance'' and group privacy''. We model this problem as a matroid. We extend it to general matroids and call the new matroids clustering matroids''. We define three variations of the clustering tree problem and show that from an algorithmic point of view they are polynomially equivalent. We present a polynomial algorithm for one of the three variations, which implies that all of them can be solved polynomially. For the case where the cardinality of the subsets in the collection does not exceed three, we provide a greedy algorithm, a linear algorithm and also a polyhedron description of the convex hull of all the feasible solutions.", "ref_function": ["objective", "objective", "method", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "1787635289", "abstract": "We consider the problem of inferring the most likely social network given connectivity constraints imposed by observations of outbreaks within the network. Given a set of vertices (or agents) V and constraints (or observations) Si \u2286 V we seek to find a minimum log-likelihood cost (or maximum likelihood) set of edges (or connections) E such that each Si induces a connected subgraph of (V, E). For the offline version of the problem, we prove an \u03a9(log(n)) hardness of approximation result for uniform cost networks and give an algorithm that almost matches this bound, even for arbitrary costs. Then we consider the online problem, where the constraints are satisfied as they arrive. We give an O(n log(n))-competitive algorithm for the arbitrary cost online problem, which has an \u03a9(n)-competitive lower bound.We look at the uniform cost case as well and give an O(n2 3 log2 3(n))-competitive algorithm against an oblivious adversary, as well as an \u03a9(\u221an)-competitive lower bound against an adaptive adversary. We examine cases when the underlying network graph is known to be a star or a path, and prove matching upper and lower bounds of \u0398(log(n)) on the competitive ratio for them.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "1969940463", "abstract": "Given a setX and subsetsX1,...,Xm, we consider the problem of finding a graphG with vertex setX and the minimum number of edges such that fori=1,...,m, the subgraphGi; induced byXi is connected. Suppose that for any\u03b1 pointsx1,...,x\u03b1e X, there are at most\u03b2Xi 's containing the set x1,...,x\u03b1 . In the paper, we show that the problem is polynomial-time solvable for (\u03b1 \u2a7d 2,\u03b2 \u2a7d 2) and is NP-hard for (\u03b1\u2a7e3,\u03b2=1), (\u03b1=l,\u03b2\u2a7e6), and (\u03b1\u2a7e2,\u03b2\u2a7e3).", "ref_function": ["background", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2058702473", "abstract": "In the context of designing a scalable overlay network to support decentralized topic-based pub sub communication, the Minimum Topic-Connected Overlay problem (Min-TCO in short) has been investigated: given a set of t topics and a collection of n users together with the lists of topics they are interested in, the aim is to connect these users to a network by a minimum number of edges such that every graph induced by users interested in a common topic is connected. It is known that Min-TCO is NP-hard and approximable within O(logt) in polynomial time. In this paper, we further investigate the problem and some of its special instances. We give various hardness results for instances where the number of topics in which a user is interested in is bounded by a constant, and also for the instances where the number of users interested in a common topic is a constant. For the latter case, we present a first constant approximation algorithm. We also present some polynomial-time algorithms for very restricted instances of Min-TCO.", "ref_function": ["objective", "objective", "objective", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2153718046", "abstract": "We introduce two new notions of planarity for hypergraphs based on dual generalizations of the standard Venn diagram. These definitions are illustrated by results concerning the existence and nonexistence of such diagrams for certain classes of hypergraphs. We conclude by showing that the general problem of determining whether such diagrams exist is NP-complete.", "ref_function": ["background", "background", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "125066187", "abstract": "We consider the following Minimum Connectivity Inference problem (MCI), which arises in structural biology: given vertex sets V i \u2286 V, i \u2208 I, find a graph G = (V,E) minimizing the size of the edge set E, such that the sub-graph of G induced by each V i is connected. This problem arises in structural biology, when one aims at finding the pairwise contacts between the proteins of a protein assembly, given the lists of proteins involved in sub-complexes. We present four contributions.", "ref_function": ["background", "objective", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Concerning the implementation of algorithms, previous works mainly focused on approximation, greedy and other heuristic techniques @cite_4 .", "To the best of our knowledge, the first exact algorithm was designed by Agarwal al @cite_17 @cite_7 in the context of structural biology, where the sought graph represents the contact relations between proteins of a macro-molecule, which has to be inferred from a hypergraph constructed by chemical experiments and mass spectrometry.", "In this work, the authors define a Mixed Integer Linear Programming (MILP) formulation of the problem, representing the connectivity constraints by flows.", "They also provide an enumeration method using their algorithm as a black box, by iteratively adding constraints to the MILP in order to forbid already found solutions.", "Both their optimization and enumeration algorithms were tested on some real-life (from a structural biology perspective) instances for which the contact graph was already known."], "label": ["General descriptions of the topic", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Given a hypergraph @math , the Minimum Connectivity Inference problem asks for a graph on the same vertex set as @math with the minimum number of edges such that the subgraph induced by every hyperedge of @math is connected. This problem has received a lot of attention these recent years, both from a theoretical and practical perspective, leading to several implemented approximation, greedy and heuristic algorithms. Concerning exact algorithms, only Mixed Integer Linear Programming (MILP) formulations have been experimented, all representing connectivity constraints by the means of graph flows. In this work, we investigate the efficiency of a constraint generation algorithm, where we iteratively add cut constraints to a simple ILP until a feasible (and optimal) solution is found. It turns out that our method is faster than the previous best flow-based MILP algorithm on random generated instances, which suggests that a constraint generation approach might be also useful for other optimization problems dealing with connectivity constraints. At last, we present the results of an enumeration algorithm for the problem.", "reference": {"@cite_4": {"mid": "2134774739", "abstract": "Designing an overlay network for publish subscribe communication in a system where nodes may subscribe to many different topics of interest is of fundamental importance. For scalability and efficiency, it is important to keep the degree of the nodes in the publish subscribe system low. It is only natural then to formalize the following problem: Given a collection of nodes and their topic subscriptions connect the nodes into a graph which has least possible maximum degree and in such a way that for each topic t, the graph induced by the nodes interested in t is connected. We present the first polynomial time logarithmic approximation algorithm for this problem and prove an almost tight lower bound on the approximation ratio. Our experimental results show that our algorithm drastically improves the maximum degree of publish subscribe overlay systems. We also propose a variation of the problem by enforcing that each topic-connected overlay network be of constant diameter, while keeping the average degree low. We present a heuristic for this problem which guarantees that each topic-connected overlay network will be of diameter 2 and which aims at keeping the overall average node degree low. Our experimental results validate our algorithm showing that our algorithm is able to achieve very low diameter without increasing the average degree by much.", "ref_function": ["background", "background", "method", "method", "result", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_7": {"mid": "2130739699", "abstract": "Consider a set of oligomers listing the subunits involved in sub-complexes of a macro-molecular assembly, obtained e.g. using native mass spectrometry or affinity purification. Given these oligomers, connectivity inference (CI) consists of finding the most plausible contacts between these subunits, and minimum connectivity inference (MCI) is the variant consisting of finding a set of contacts of smallest cardinality. MCI problems avoid speculating on the total number of contacts, but yield a subset of all contacts and do not allow exploiting a priori information on the likelihood of individual contacts. In this context, we present two novel algorithms, MILP-W and MILP-WB. The former solves the minimum weight connectivity inference (MWCI), an optimization problem whose criterion mixes the number of contacts and their likelihood. The latter uses the former in a bootstrap fashion, to improve the sensitivity and the specificity of solution sets. Experiments on three systems (yeast exosome, yeast proteasome lid, human eiF3), for which reference contacts are known (crystal structure, cryo electron microscopy, cross-linking), show that our algorithms predict contacts with high specificity and sensitivity, yielding a very significant improvement over previous work, typically a twofold increase in sensitivity. The software accompanying this paper is made available, and should prove of ubiquitous interest whenever connectivity inference from oligomers is faced.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "125066187", "abstract": "We consider the following Minimum Connectivity Inference problem (MCI), which arises in structural biology: given vertex sets V i \u2286 V, i \u2208 I, find a graph G = (V,E) minimizing the size of the edge set E, such that the sub-graph of G induced by each V i is connected. This problem arises in structural biology, when one aims at finding the pairwise contacts between the proteins of a protein assembly, given the lists of proteins involved in sub-complexes. We present four contributions.", "ref_function": ["background", "objective", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["This MILP model was then improved recently by Dar al @cite_12 , who mainly reduced the number of variables and constraints of the formulation, but still representing the connectivity constraints by the means of flows.", "In addition, they also presented and implemented a number of (already known and new) reduction rules.", "This new MILP formulation together with the reduction rules were then compared to the algorithm of Agarwal al on randomly-generated instances.", "For every kind of tested hypergraphs (different number and sizes of hyperedges), they observed a drastic improvement of both the execution time and the maximum size of instances that could be solved."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Given a hypergraph @math , the Minimum Connectivity Inference problem asks for a graph on the same vertex set as @math with the minimum number of edges such that the subgraph induced by every hyperedge of @math is connected. This problem has received a lot of attention these recent years, both from a theoretical and practical perspective, leading to several implemented approximation, greedy and heuristic algorithms. Concerning exact algorithms, only Mixed Integer Linear Programming (MILP) formulations have been experimented, all representing connectivity constraints by the means of graph flows. In this work, we investigate the efficiency of a constraint generation algorithm, where we iteratively add cut constraints to a simple ILP until a feasible (and optimal) solution is found. It turns out that our method is faster than the previous best flow-based MILP algorithm on random generated instances, which suggests that a constraint generation approach might be also useful for other optimization problems dealing with connectivity constraints. At last, we present the results of an enumeration algorithm for the problem.", "reference": {"@cite_12": {"mid": "2800207446", "abstract": "AbstractThe Minimum Connectivity Inference (MCI) problem represents an NP -hard generalization of the well-known minimum spanning tree problem and has been studied in different fields of research i...", "ref_function": ["other"], "cite_purpose": ["background"]}}}
{"sentences": ["Shoulder surfing is a widely known attack in which the adversary tries to infer the victim's authentication secret by looking over his or her shoulder.", "There is a significant body of research into mitigating the impact of shoulder-surfing attacks.", "An in-depth survey conducted by @cite_8 considered the threat not only in the context of authentication, but also in the context of routine smartphone usage.", "The survey showed that 130 out of 174 participants indicated that shoulder-surfing attacks occurred on public transportation.", "Victims most commonly defended against such an attack by modifying their posture or cancelling the authentication.", "Furthermore, a study conducted by @cite_18 found the perceived risk of shoulder surfing to be high in only 11 of 3410 situations.", "This demonstrates that people are not actively defending themselves against shoulder surfing, and more work is needed to improve the shoulder-surfing resistance of authentication techniques."], "label": ["General descriptions of the topic", "General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Summarize the above references"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_18": {"mid": "2181155974", "abstract": "A lot of research is being conducted into improving the usability and security of phone-unlocking. There is however a severe lack of scientic data on users\u2019 current unlocking behavior and perceptions. We performed an online survey (n = 260) and a one-month eld study ( n = 52) to gain insights into real world (un)locking behavior of smartphone users. One of the main goals was to nd out how much overhead unlocking and authenticating adds to the overall phone usage and in how many unlock interactions security (i.e. authentication) was perceived as necessary. We also investigated why users do or do not use a lock screen and how they cope with smartphone-related risks, such as shouldersurng or unwanted accesses. Among other results, we found that on average, participants spent around 2.9 of their smartphone interaction time with authenticating (9 in the worst case). Participants that used a secure lock screen like PIN or Android unlock patterns considered it unnecessary in 24.1 of situations. Shoulder surng was perceived to be a relevant risk in only 11 of 3410 sampled situations.", "ref_function": ["background", "background", "method", "method", "objective", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "2611149039", "abstract": "Research has brought forth a variety of authentication systems to mitigate observation attacks. However, there is little work about shoulder surfing situations in the real world. We present the results of a user survey (N=174) in which we investigate actual stories about shoulder surfing on mobile devices from both users and observers. Our analysis indicates that shoulder surfing mainly occurs in an opportunistic, non-malicious way. It usually does not have serious consequences, but evokes negative feelings for both parties, resulting in a variety of coping strategies. Observed data was personal in most cases and ranged from information about interests and hobbies to login data and intimate details about third persons and relationships. Thus, our work contributes evidence for shoulder surfing in the real world and informs implications for the design of privacy protection mechanisms.", "ref_function": ["background", "background", "method", "result", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["PIN keypads and pattern locks are commonly used methods for phone authentication.", "Unfortunately, these techniques are vulnerable to smudge attacks, because the user leaves oily residues on the screen.", "Previous work has demonstrated that smudge attacks are especially effective on pattern locks as users drag their fingers over the screen.", "Smudge attacks can also be used to limit the input space for PIN locks.", "@cite_24 found that as long as the line of sight is not perpendicular, it is easy to observe entered patterns based on smudges.", "Under ideal conditions, 92 These results demonstrate that even if the adversary is not able to actively observe the process of authentication, he or she can still recover the password with considerable success.", "In our work, we leverage pre-touch information to limit the number of touches the user makes on the screen, mitigating the effect of smudge attacks."], "label": ["General descriptions of the topic", "Explaining the inadequacies of previous studies", "General descriptions of the topic", "Reference to current state of knowledge", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Describing used methods"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_24": {"mid": "1626992774", "abstract": "Touch screens are an increasingly common feature on personal computing devices, especially smartphones, where size and user interface advantages accrue from consolidating multiple hardware components (keyboard, number pad, etc.) into a single software definable user interface. Oily residues, or smudges, on the touch screen surface, are one side effect of touches from which frequently used patterns such as a graphical password might be inferred. In this paper we examine the feasibility of such smudge attacks on touch screens for smartphones, and focus our analysis on the Android password pattern. We first investigate the conditions (e.g., lighting and camera orientation) under which smudges are easily extracted. In the vast majority of settings, partial or complete patterns are easily retrieved. We also emulate usage situations that interfere with pattern identification, and show that pattern smudges continue to be recognizable. Finally, we provide a preliminary analysis of applying the information learned in a smudge attack to guessing an Android password pattern.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Now that smartphones are commonplace, traditional authentication techniques have been adapted to work on the small touchscreens of smartphones.", "@cite_25 compared speed and shoulder-surfing resistance of a scrambled PIN entry keypad and a normal PIN entry keypad.", "They found that the scrambled keypad was slower but more resistant to shoulder surfing."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_25": {"mid": "2538700141", "abstract": "PIN unlock is a popular screen locking mechanism used for protecting the sensitive private information on smart-phones. However, it is susceptible to a number of attacks such as guessing attacks, shoulder surfing attacks, smudge attacks, and side-channel attacks. Scramble keypad changes the keypad layout in each PIN-entry process to improve the security of PIN unlock. In this paper, the security and usability of scramble keypad for PIN unlock are studied. Our security analysis shows that scramble keypad can defend smudge attacks perfectly and greatly reduce the threats of side-channel attacks. A user study is conducted to demonstrate that scramble keypad has a better chance to defend shoulder surfing attacks than standard keypad. We also investigate how the usability of scramble keypad is compromised for improved security through a user study.", "ref_function": ["background", "background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Several works have examined the possibility of augmenting PIN keypads with gestures.", "SwiPIN, by von @cite_6 , divided the PIN keypad into two sections.", "Each number in each section corresponded to a different swipe gesture direction.", "Performing a swipe gesture on the correct section of the screen would insert the corresponding number.", "Their study demonstrated that this technique improved resistance against smudge attacks.", "introduced ForcePINs'' @cite_11 , with which each PIN digit could be entered with different levels of finger pressure on the screen, to add an additional layer of challenge for shoulder surfers.", "However, results showed that there was no statistically significant difference in shoulder-surfing resistance between regular PINs and ForcePINs, because when users pressed harder, they also pressed for a noticeably longer time."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_6": {"mid": "2159837114", "abstract": "In this paper, we present SwiPIN, a novel authentication system that allows input of traditional PINs using simple touch gestures like up or down and makes it secure against human observers. We present two user studies which evaluated different designs of SwiPIN and compared it against traditional PIN. The results show that SwiPIN performs adequately fast (3.7 s) to serve as an alternative input method for risky situations. Furthermore, SwiPIN is easy to use, significantly more secure against shoulder surfing attacks and switching between PIN and SwiPIN feels natural.", "ref_function": ["background", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2795614125", "abstract": "We evaluate the efficacy of shoulder surfing defenses for PIN-based authentication systems. We find tilting the device away from the observer, a widely adopted defense strategy, provides limited protection. We also evaluate a recently proposed defense incorporating an \"invisible pressure component\" into PIN entry. Contrary to earlier claims, our results show this provides little defense against malicious insider attacks. Observations during the study uncover successful attacker strategies for reconstructing a victim's PIN when faced with a tilt defense. Our evaluations identify common misconceptions regarding shoulder surfing defenses, and highlight the need to educate users on how to safeguard their credentials from these attacks.", "ref_function": ["background", "background", "method", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Other works have looked beyond purely visual representations of PINs by incorporating haptic and audio feedback.", "@cite_17 created an observation-resistant authentication technique by providing no visual clues to the user.", "The technique renders a wheel on the screen with identical sections.", "However, when users drag their fingers over the sections of the wheel, tactile feedback is presented with varying lengths and strengths.", "To select a section, users drag their fingers to the middle of the wheel.", "After each entry, the sections are shuffled to provide resistance against smudge attacks.", "Similarly, VibraInput @cite_1 used an on-screen, rotary wheel with two levels.", "The outer level contained the letters A through D, each corresponding to a fixed vibration pattern (that has to be remembered by user).", "The inner level corresponded to the PIN numbers 0 through 9.", "Upon starting PIN entry, the phone would vibrate the pattern of a letter.", "The user would then rotate the outer wheel to align the letter with the number to select on the inner wheel.", "By repeating this process, the technique could use process of elimination to ascertain the PIN number.", "The overall technique would repeat until the entire PIN was entered."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_1": {"mid": "2036616308", "abstract": "Current standard PIN entry systems for mobile devices are not safe to shoulder surfing. In this paper, we present VibraInput, a two-step PIN entry system based on the combination of vibration and visual information for mobile devices. This system only uses four vibration patterns, with which users enter a digit by two distinct selections. We believe that this design secures PIN entry, and allows users to easily remember and recognize the patterns. Moreover, it can be implemented on current off-the-shelf mobile devices. We designed two kinds of prototypes of VibraInput. The experiment shows that the mean failure rate is 4.0 ; moreover, the system shows good security properties.", "ref_function": ["background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2151027695", "abstract": "Tangible user interfaces are portals to digital information. In the future, securing access to such material will be an important concern. This paper describes the design, implementation and evaluation of a PIN entry system based on audio or haptic cues that is suitable for integration into such physical systems. The current implementation links movements on a mobile phone touch screen with the display of non-visual cues; selection of a sequence of these cues composes a password. Studies reveal the validity of this approach in terms of task times and error rates that improve over prior art. In sum, this paper demonstrates the potential of non-visual PINs as a mechanism for securing access to a range of systems, ultimately incorporating mobile, ubiquitous or tangible interfaces.", "ref_function": ["background", "background", "objective", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Two-Thumbs-Up (TTU) @cite_9 prevents shoulder-surfing attacks by requiring the user to cover the screen with their hands.", "This forms a handshield'' and enters a challenge mode.", "If users move their hands away from the screen, the authentication technique disappears.", "TTU randomly associates five response'' letters with two digits each, presenting the digits and letters on either side of the screen.", "The user then has to tap on the letter corresponding to the next PIN digit.", "After a certain number (dependent on PIN length) of correctly selected letters, the authentication process is complete."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_9": {"mid": "2803549391", "abstract": "Abstract We present a new Personal Identification Number (PIN) entry method for smartphones that can be used in security-critical applications, such as smartphone banking. The proposed \u201cTwo-Thumbs-Up\u201d (TTU) scheme is resilient against observation attacks such as shoulder-surfing and camera recording, and guides users to protect their PIN information from eavesdropping by shielding the challenge area on the touch screen. To demonstrate the feasibility of TTU, we conducted a user study for TTU, and compared it with existing authentication methods (Normal PIN, Black and White PIN, and ColorPIN) in terms of usability and security. The study results demonstrate that TTU is more secure than other PIN entry methods in the presence of an observer recording multiple authentication sessions.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Harbach at al.", "@cite_23 focused on comparing PIN locks and pattern locks.", "They were able to observe the behaviour of 134 smartphone users over one month, revealing differences between the two techniques.", "Results showed that although pattern locks are faster, users are six times as likely to make mistakes compared to PIN locks.", "When including failed attempts, there were no differences in authentication time between the two techniques.", "When a user made a mistake entering a PIN or pattern, subsequent successful attempts took more time, presumably because the user took more care when repeating the authentication.", "Visual feedback did not influence the error rate nor the entry time.", "Similarly, our 3D Pattern technique improves shoulder-surfing resistance by reducing visual feedback during authentication."], "label": ["Not sure", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result", "Describing used methods"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_23": {"mid": "2315247372", "abstract": "To prevent unauthorized parties from accessing data stored on their smartphones, users have the option of enabling a \"lock screen\" that requires a secret code (e.g., PIN, drawing a pattern, or biometric) to gain access to their devices. We present a detailed analysis of the smartphone locking mechanisms currently available to billions of smartphone users worldwide. Through a month-long field study, we logged events from a panel of users with instrumented smartphones (N=134). We are able to show how existing lock screen mechanisms provide users with distinct tradeoffs between usability (unlocking speed vs. unlocking frequency) and security. We find that PIN users take longer to enter their codes, but commit fewer errors than pattern users, who unlock more frequently and are very prone to errors. Overall, PIN and pattern users spent the same amount of time unlocking their devices on average. Additionally, unlock performance seemed unaffected for users enabling the stealth mode for patterns. Based on our results, we identify areas where device locking mechanisms can be improved to result in fewer human errors -- increasing usability -- while also maintaining security.", "ref_function": ["background", "background", "method", "method", "result", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Another category of PIN entry techniques uses pictures or other graphics.", "In SemanticLock @cite_10 , users arrange icons on the screen in a memorable way.", "The user is authenticated based on correct placement of the icons.", "In a similar work, Awase-E @cite_5 , Takada and Koike leverage photos taken on a user's smartphone.", "The lock screen breaks a user-chosen photograph up into smaller chunks, and shows nine chunks of various photographs all at once.", "The user then has to select the tile from the correct photograph four times in a row to unlock the phone."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_5": {"mid": "135685467", "abstract": "There is a trade-off between security and usability in user authentication for mobile phones. Since such devices have a poor input interfaces, 4-digit number passwords are widely used at present. Therefore, a more secure and user friendly authentication is needed. This paper proposes a novel authentication method called \u201cAwase-E\u201d. The system uses image passwords. It, moreover, integrates image registration and notification interfaces. Image registration enables users to use their favorite image instead of a text password. Notification gives users a trigger to take action against a threat when it happens. Awase-E is implemented so that it has a higher usability even when it is used through a mobile phone.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2809689775", "abstract": "We introduce SemanticLock, a single factor graphical authentication solution for mobile devices. SemanticLock uses a set of graphical images as password tokens that construct a semantically memorable story representing the user s password. A familiar and quick action of dragging or dropping the images into their respective positions either in a or in movements on the the touchscreen is what is required to use our solution. The authentication strength of the SemanticLock is based on the large number of possible semantic constructs derived from the positioning of the image tokens and the type of images selected. Semantic Lock has a high resistance to smudge attacks and it equally exhibits a higher level of memorability due to its graphical paradigm. In a three weeks user study with 21 participants comparing SemanticLock against other authentication systems, we discovered that SemanticLock outperformed the PIN and matched the PATTERN both on speed, memorability, user acceptance and usability. Furthermore, qualitative test also show that SemanticLock was rated more superior in like-ability. SemanticLock was also evaluated while participants walked unencumbered and walked encumbered carrying \"everyday\" items to analyze the effects of such activities on its usage.", "ref_function": ["background", "background", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["There is considerable research exploring whether or not lock screens are even necessary at all, by applying , also known as implicit authentication.", "Continuous authentication systems analyze an individual's regular patterns of touches on the screen, and build a model.", "A different user would have different patterns, and could be denied access by the system.", "With the Touchalytics project @cite_0 , were able to use continuous authentication to identify the user with an error rate below 4 , @cite_19 showed that an attacker, merely watching a video of the target using their phone, could bypass swipe-based continuous authentication at least 75"], "label": ["General descriptions of the topic", "Reference to current state of knowledge", "Reference to current state of knowledge", "Reference to single investigations in the past: about method"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_0": {"mid": "2151854612", "abstract": "We investigate whether a classifier can continuously authenticate users based on the way they interact with the touchscreen of a smart phone. We propose a set of 30 behavioral touch features that can be extracted from raw touchscreen logs and demonstrate that different users populate distinct subspaces of this feature space. In a systematic experiment designed to test how this behavioral pattern exhibits consistency over time, we collected touch data from users interacting with a smart phone using basic navigation maneuvers, i.e., up-down and left-right scrolling. We propose a classification framework that learns the touch behavior of a user during an enrollment phase and is able to accept or reject the current user by monitoring interaction with the touch screen. The classifier achieves a median equal error rate of 0 for intrasession authentication, 2 -3 for intersession authentication, and below 4 when the authentication test was carried out one week after the enrollment phase. While our experimental findings disqualify this method as a standalone authentication mechanism for long-term authentication, it could be implemented as a means to extend screen-lock time or as a part of a multimodal biometric authentication system.", "ref_function": ["background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2468988960", "abstract": "Touch input implicit authentication ( touch IA'') employs behavioural biometrics like touch location and pressure to continuously and transparently authenticate smartphone users. We provide the first ever evaluation of targeted mimicry attacks on touch IA and show that it fails against shoulder surfing and offline training attacks. Based on experiments with three diverse touch IA schemes and 256 unique attacker-victim pairs, we show that shoulder surfing attacks have a bypass success rate of 84 with the majority of successful attackers observing the victim's behaviour for less than two minutes. Therefore, the accepted assumption that shoulder surfing attacks on touch IA are infeasible due to the hidden nature of some features is incorrect. For offline training attacks, we created an open-source training app for attackers to train on their victims' touch data. With this training, attackers achieved bypass success rates of 86 , even with only partial knowledge of the underlying features used by the IA scheme. Previous work failed to find these severe vulnerabilities due to its focus on random, non-targeted attacks. Our work demonstrates the importance of considering targeted mimicry attacks to evaluate the security of an implicit authentication scheme. Based on our results, we conclude that touch IA is unsuitable from a security standpoint.", "ref_function": ["background", "background", "background", "background", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Some work has explored applying the principles of continuous authentication to augment traditional lock screen techniques.", "@cite_12 use spatial touch features in addition to previously used temporal touch features on keyboards to verify users based on their individual text entry behaviours.", "Examples of spatial touch features include touch offsets, angles, and pressures.", "By incorporating such spatial features, user recognition accuracy was improved."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_12": {"mid": "2064376060", "abstract": "Authentication methods can be improved by considering implicit, individual behavioural cues. In particular, verifying users based on typing behaviour has been widely studied with physical keyboards. On mobile touchscreens, the same concepts have been applied with little adaptations so far. This paper presents the first reported study on mobile keystroke biometrics which compares touch-specific features between three different hand postures and evaluation schemes. Based on 20.160 password entries from a study with 28 participants over two weeks, we show that including spatial touch features reduces implicit authentication equal error rates (EER) by 26.4 - 36.8 relative to the previously used temporal features. We also show that authentication works better for some hand postures than others. To improve applicability and usability, we further quantify the influence of common evaluation assumptions: known attacker data, training and testing on data from a single typing session, and fixed hand postures. We show that these practices can lead to overly optimistic evaluations. In consequence, we describe evaluation recommendations, a probabilistic framework to handle unknown hand postures, and ideas for further improvements.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Many recent works on touchscreen interactions have started exploring pre-touch information; that is, positional information about the user's hands or fingers before making contact with the screen.", "For example, with TouchCuts and TouchZoom, @cite_21 used pre-touch finger distance to expand nearby targets on screen, facilitating easier target selection.", "This general approach has not yet been explored in the context of authentication techniques resistant to shoulder surfing."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to current state of knowledge"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_21": {"mid": "2136328445", "abstract": "Although touch-screen laptops are increasing in popularity, users still do not comfortably rely on touch in these environments, as current software interfaces were not designed for being used by the finger. In this paper, we first demonstrate the benefits of using touch as a complementary input modality along with the keyboard and mouse or touchpad in a laptop setting. To alleviate the frustration users experience with touch, we then design two techniques, TouchCuts, a single target expansion technique, and ,i>TouchZoom, i>, a multiple target expansion technique. Both techniques facilitate the selection of small icons, by detecting the finger proximity above the display surface, and expanding the target as the finger approaches. In a controlled evaluation, we show that our techniques improve performance in comparison to both the computer mouse and a baseline touch-based target acquisition technique. We conclude by discussing other application scenarios that our techniques support.", "ref_function": ["background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Another common application of pre-touch information is for reducing the perceived latency of touchscreen interactions.", "employed this approach for tabletop displays @cite_4 , achieving a touch location prediction error of about 1 ,cm.", "The approach was implemented by tracking the user's index finger location using motion capture with fiducial markers, which are small retro-reflective spheres that can be precisely tracked by IR cameras.", "In the prototype of our 3D Pattern technique, we also use a motion capture system for finger position tracking."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Describing used methods"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_4": {"mid": "2078073494", "abstract": "A method of reducing the perceived latency of touch input by employing a model to predict touch events before the finger reaches the touch surface is proposed. A corpus of 3D finger movement data was collected, and used to develop a model capable of three granularities at different phases of movement: initial direction, final touch location, time of touchdown. The model is validated for target distances >= 25.5cm, and demonstrated to have a mean accuracy of 1.05cm 128ms before the user touches the screen. Preference study of different levels of latency reveals a strong preference for unperceived latency touchdown feedback. A form of 'soft' feedback, as well as other uses for this prediction to improve performance, is proposed.", "ref_function": ["background", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["We anticipate pre-touch sensing to become available on commodity smartphones in the near future.", "In 2016, @cite_2 explored how a smartphone with a self-capacitance touchscreen could enable pre-touch information to be sensed, and applied this information in various smartphone applications.", "We envision that our pre-touch PIN entry techniques will be able to be used on smartphones without additional motion tracking hardware."], "label": ["Other functional sentences", "Reference to single investigations in the past: about method", "Describing the results"], "target_paper": "Smartphones store a significant amount of personal and private information, and are playing an increasingly important role in people's lives. It is important for authentication techniques to be more resistant against two known attacks called shoulder surfing and smudge attacks. In this work, we propose a new technique called 3D Pattern. Our 3D Pattern technique takes advantage of a new input paradigm called pre-touch, which could soon allow smartphones to sense a user's finger position at some distance from the screen. We implement the technique and evaluate it in a pilot study (n=6) by comparing it to PIN and pattern locks. Our results show that although our prototype takes about 8 seconds to authenticate, it is immune to smudge attacks and promises to be more resistant to shoulder surfing.", "reference": {"@cite_2": {"mid": "2397886250", "abstract": "Touchscreens continue to advance including progress towards sensing fingers proximal to the display. We explore this emerging pre-touch modality via a self-capacitance touchscreen that can sense multiple fingers above a mobile device, as well as grip around the screen's edges. This capability opens up many possibilities for mobile interaction. For example, using pre-touch in an anticipatory role affords an \"ad-lib interface\" that fades in a different UI--appropriate to the context--as the user approaches one-handed with a thumb, two-handed with an index finger, or even with a pinch or two thumbs. Or we can interpret pre-touch in a retroactive manner that leverages the approach trajectory to discern whether the user made contact with a ballistic vs. a finely-targeted motion. Pre-touch also enables hybrid touch + hover gestures, such as selecting an icon with the thumb while bringing a second finger into range to invoke a context menu at a convenient location. Collectively these techniques illustrate how pre-touch sensing offers an intriguing new back-channel for mobile interaction.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In the first type, @cite_8 propose a framework for solving the problem of one-shot classification.", "They first build a fully convolutional siamese network based on verification loss, and then use this network to calculate the similarity between the image to be identified and other labeled samples.", "The image is then recognized as a sample of the category which the most similar labeled sample belongs to.", "@cite_1 propose matching network.", "During the training process, some samples are selected to form a support set and the remaining samples are used as training images.", "They construct different encoders for the support set and training pictures.", "The classfier's output is a weighted sum of the predicted values between the support set and the training images.", "During the test process, one-shot sample are used as support set to predict the category of new images.", "@cite_14 use meta-learning methods to learn multiple similar tasks, and build two encoders for the gallery and probe respectively.", "Based on these encoders, they get gallery images' embedding according to the characteristics of the remaining gallery images.", "They get probe images' embedding according to the characteristics of the gallery images.", "In this way they obtain a more discriminative feature representation."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "This paper mainly studies one-example and few-example video person re-identification. A multi-branch network PAM that jointly learns local and global features is proposed. PAM has high accuracy, few parameters and converges fast, which is suitable for few-example person re-identification. We iteratively estimates labels for unlabeled samples, incorporates them into training sets, and trains a more robust network. We propose the static relative distance sampling(SRD) strategy based on the relative distance between classes. For the problem that SRD can not use all unlabeled samples, we propose adaptive relative distance sampling (ARD) strategy. For one-example setting, We get 89.78 , 56.13 rank-1 accuracy on PRID2011 and iLIDS-VID respectively, and 85.16 , 45.36 mAP on DukeMTMC and MARS respectively, which exceeds the previous methods by large margin.", "reference": {"@cite_14": {"mid": "2886491726", "abstract": "In this paper, we investigate the challenging task of person re-identification from a new perspective and propose an end-to-end attention-based architecture for few-shot re-identification through meta-learning. The motivation for this task lies in the fact that humans, can usually identify another person after just seeing that given person a few times (or even once) by attending to their memory. On the other hand, the unique nature of the person re-identification problem, i.e., only few examples exist per identity and new identities always appearing during testing, calls for a few shot learning architecture with the capacity of handling new identities. Hence, we frame the problem within a meta-learning setting, where a neural network based meta-learner is trained to optimize a learner i.e., an attention-based matching function. Another challenge of the person re-identification problem is the small inter-class difference between different identities and large intra-class difference of the same identity. In order to increase the discriminative power of the model, we propose a new attention-based feature encoding scheme that takes into account the critical intra-view and cross-view relationship of images. We refer to the proposed Attention-based Re-identification Met alearning model as ARM. Extensive evaluations demonstrate the advantages of the ARM as compared to the state-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501 datasets.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2432717477", "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6 to 93.2 and from 88.0 to 93.8 on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["In the second type, @cite_9 establish a graph for each camera.", "They view the labeled sample as the node of the graph, and view the distance between the video sequence features as the path.", "Unlabeled sample are mapped into different graphs (namely estimating the labels) to minimize the objective function.", "The graphs are updated dynamically .", "They continually estimate labels, and train models until the algorithm converges.", "@cite_10 first initialize the model with labeled samples.", "Then they calculate k nearest neighbors of the probe with the gallery.", "They remove the suspect samples and then add the remaining samples to the training set.", "The procedure is iterated until the algorithm converges.", "@cite_2 initialize a CNN with labeled data firstly, and then linearly incorporate pseudo-label samples to the training set according to the distance to labeled samples.", "Then the CNN is retrained with the new training set.", "Finally all unlabeled samples have estimated label and are added into training set, then they use a validation set to select the best model."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "This paper mainly studies one-example and few-example video person re-identification. A multi-branch network PAM that jointly learns local and global features is proposed. PAM has high accuracy, few parameters and converges fast, which is suitable for few-example person re-identification. We iteratively estimates labels for unlabeled samples, incorporates them into training sets, and trains a more robust network. We propose the static relative distance sampling(SRD) strategy based on the relative distance between classes. For the problem that SRD can not use all unlabeled samples, we propose adaptive relative distance sampling (ARD) strategy. For one-example setting, We get 89.78 , 56.13 rank-1 accuracy on PRID2011 and iLIDS-VID respectively, and 85.16 , 45.36 mAP on DukeMTMC and MARS respectively, which exceeds the previous methods by large margin.", "reference": {"@cite_9": {"mid": "2963989829", "abstract": "Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant cross-camera variations, this paper propose a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods.1", "ref_function": ["background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2778652957", "abstract": "The intensive annotation cost and the rich but unlabeled data contained in videos motivate us to propose an unsupervised video-based person re-identification (re-ID) method. We start from two assumptions: 1) different video tracklets typically contain different persons, given that the tracklets are taken at distinct places or with long intervals; 2) within each tracklet, the frames are mostly of the same person. Based on these assumptions, this paper propose a stepwise metric promotion approach to estimate the identities of training tracklets, which iterates between cross-camera tracklet association and feature learning. Specifically, We use each training tracklet as a query, and perform retrieval in the cross-camera training set. Our method is built on reciprocal nearest neighbor search and can eliminate the hard negative label matches, i.e., the cross-camera nearest neighbors of the false matches in the initial rank list. The tracklet that passes the reciprocal nearest neighbor check is considered to have the same ID with the query. Experimental results on the PRID 2011, ILIDS-VID, and MARS datasets show that the proposed method achieves very competitive re-ID accuracy compared with its supervised counterparts.", "ref_function": ["background", "method", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2799185441", "abstract": "We focus on the one-shot learning for video-based person re-Identification (re-ID). Unlabeled tracklets for the person re-ID tasks can be easily obtained by preprocessing, such as pedestrian detection and tracking. In this paper, we propose an approach to exploiting unlabeled tracklets by gradually but steadily improving the discriminative capability of the Convolutional Neural Network (CNN) feature representation via stepwise learning. We first initialize a CNN model using one labeled tracklet for each identity. Then we update the CNN model by the following two steps iteratively: 1. sample a few candidates with most reliable pseudo labels from unlabeled tracklets; 2. update the CNN model according to the selected data. Instead of the static sampling strategy applied in existing works, we propose a progressive sampling method to increase the number of the selected pseudo-labeled candidates step by step. We systematically investigate the way how we should select pseudo-labeled tracklets into the training set to make the best use of them. Notably, the rank-1 accuracy of our method outperforms the state-of-the-art method by 21.46 points (absolute, i.e., 62.67 vs. 41.21 ) on the MARS dataset, and 16.53 points on the DukeMTMC-VideoReID dataset1.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["A monocular SLAM system, which leverages structural regularity in Manhattan world and contains three optimization strategies is proposed in @cite_17 .", "However, to reduce the estimation error of the rotation motion, multiple orthogonal planes must be visible throughout the entire motion estimation process.", "Unlike only using planes in @cite_17 , the rotation motion is estimated by joint lines and planes in @cite_28 .", "Once the rotation is found, the translational motion can be recovered by minimizing the de-rotated reprojection error.", "In @cite_24 , the accuracy of BA optimization is enhanced by incorporating feature scale constraints into it.", "Structural constraints between nearby planes (e.g.", "right angle) are added in the SLAM system to further recover the drift and distortion in @cite_2 .", "Since the structural regularity does not exist in all environments, the application scope of this category is limited."], "label": ["Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Accurate camera pose estimation result is essential for visual SLAM (VSLAM). This paper presents a novel pose correction method to improve the accuracy of the VSLAM system. Firstly, the relationship between the camera pose estimation error and bias values of map points is derived based on the optimized function in VSLAM. Secondly, the bias value of the map point is calculated by a statistical method. Finally, the camera pose estimation error is compensated according to the first derived relationship. After the pose correction, procedures of the original system, such as the bundle adjustment (BA) optimization, can be executed as before. Compared with existing methods, our algorithm is compact and effective and can be easily generalized to different VSLAM systems. Additionally, the robustness to system noise of our method is better than feature selection methods, due to all original system information is preserved in our algorithm while only a subset is employed in the latter. Experimental results on benchmark datasets show that our approach leads to considerable improvements over state-of-the-art algorithms for absolute pose estimation.", "reference": {"@cite_28": {"mid": "2892177182", "abstract": "We present a low-drift visual odometry algorithm that separately estimates rotational and translational motion from lines, planes, and points found in RGB-D images. Previous methods estimate drift-free rotational motion from structural regularities to reduce drift in the rotation estimate, which is the primary source of positioning inaccuracy in visual odometry. However, multiple orthogonal planes are required to be visible throughout the entire motion estimation process; otherwise, these VO approaches fail. We propose a new approach to estimate drift-free rotational motion jointly from both lines and planes by exploiting environmental regularities. We track the spatial regularities with an efficient SO(3)-manifold constrained mean shift algorithm. Once the drift-free rotation is found, we recover the translational motion from all tracked points with and without depth by minimizing the de-rotated reprojection error. We compare the proposed algorithm to other state-of-the-art visual odometry methods on a variety of RGB-D datasets (including especially challenging pure rotations) and demonstrate improved accuracy and lower drift error.", "ref_function": ["background", "method", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2892147056", "abstract": "In this work, we develop a novel dense planar-inertial SLAM (DPI-SLAM) system to reconstruct dense 3D models of large indoor environments using a hand-held RGB-D sensor and an inertial measurement unit (IMU). The preinte-grated IMU measurements are loosely-coupled with the dense visual odometry (VO) estimation and tightly-coupled with the planar measurements in a full SLAM framework. The poses, velocities, and IMU biases are optimized together with the planar landmarks in a global factor graph using incremental smoothing and mapping with the Bayes Tree (iSAM2). With odometry estimation using both RGB-D and IMU data, our system can keep track of the poses of the sensors even without sufficient planes or visual information (e.g. textureless walls) temporarily. Modeling planes and IMU states in the fully probabilistic global optimization reduces the drift that distorts the reconstruction results of other SLAM algorithms. Moreover, structural constraints between nearby planes (e.g. right angles) are added into the DPI-SLAM system, which further recovers the drift and distortion. We test our DPI-SLAM on large indoor datasets and demonstrate its state-of-the-art performance as the first planar-inertial SLAM system.", "ref_function": ["background", "method", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": [""]}, "@cite_24": {"mid": "2784319755", "abstract": "We propose to incorporate within bundle adjustment (BA) a new type of constraint that uses feature scale information, leveraging the scale invariance property of typical image feature detectors (e.g., SIFT). While feature scales play an important role in image matching, they have not been utilized thus far for estimation purposes in a BA framework. Our approach exploits the already-available feature scale information and uses it to enhance the accuracy of BA, especially along the optical axis of the camera in a monocular setup. Importantly, the mentioned feature scale constraints can be formulated on a frame to frame basis and do not require loop closures. We study our approach in synthetic environments and the real-imagery KITTI dataset, demonstrating significant improvement in positioning error.", "ref_function": ["background", "background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_17": {"mid": "2889958683", "abstract": "The structural features in Manhattan world encode useful geometric information of parallelism, orthogonality and or coplanarity in the scene. By fully exploiting these structural features, we propose a novel monocular SLAM system which provides accurate estimation of camera poses and 3D map. The foremost contribution of the proposed system is a structural feature-based optimization module which contains three novel optimization strategies. First, a rotation optimization strategy using the parallelism and orthogonality of 3D lines is presented. We propose a global binding method to compute an accurate estimation of the absolute rotation of the camera. Then we propose an approach for calculating the relative rotation to further refine the absolute rotation. Second, a translation optimization strategy leveraging coplanarity is proposed. Coplanar features are effectively identified, and we leverage them by a unified model handling both points and lines to calculate the relative translation, and then the optimal absolute translation. Third, a 3D line optimization strategy utilizing parallelism, orthogonality and coplanarity simultaneously is proposed to obtain an accurate 3D map consisting of structural line segments with low computational complexity. Experiments in man-made environments have demonstrated that the proposed system outperforms existing state-of-the-art monocular SLAM systems in terms of accuracy and robustness.", "ref_function": ["background", "objective", "objective", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background", "background"]}}}
{"sentences": ["On the side of BNNs, @cite_18 connect Bernoulli dropout with BNNs, and @cite_29 formalize Gaussian dropout as a Bayesian approach.", "In @cite_5 , novel BNNs are proposed, using RealNVP @cite_22 to implement a normalizing flow @cite_56 , auxiliary variables and local reparameterization .", "None of these approaches measure calibration performance explicitly on DNNs, as we do.", "For instance, @cite_5 and @cite_21 evaluate uncertainty by training on one dataset and use it on another, expecting a maximum entropy output distribution.", "More recently, @cite_2 propose a scalable inference algorithm that is also asymptotically accurate as MCMC algorithms and @cite_34 propose a deterministic way of computing the ELBO to reduce the variance of the estimator to 0, allowing for faster convergence.", "They also propose a hierarchical prior on the parameters."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Deep Neural Networks (DNNs) have achieved state-of-the-art accuracy performance in many tasks. However, recent works have pointed out that the outputs provided by these models are not well-calibrated, seriously limiting their use in critical decision scenarios. In this work, we propose to use a decoupled Bayesian stage, implemented with a Bayesian Neural Network (BNN), to map the uncalibrated probabilities provided by a DNN to calibrated ones, consistently improving calibration. Our results evidence that incorporating uncertainty provides more reliable probabilistic models, a critical condition for achieving good calibration. We report a generous collection of experimental results using high-accuracy DNNs in standardized image classification benchmarks, showing the good performance, flexibility and robust behavior of our approach with respect to several state-of-the-art calibration methods. Code for reproducibility is provided.", "reference": {"@cite_18": {"mid": "601603264", "abstract": "Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data -- as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2962695743", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "ref_function": ["background", "background", "method", "result"], "cite_purpose": ["background"]}, "@cite_29": {"mid": "1826234144", "abstract": "We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the mini-batch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2963238274", "abstract": "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_56": {"mid": "2963090522", "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_2": {"mid": "2886496274", "abstract": "Probabilistic modelling is a general and elegant framework to capture the uncertainty, ambiguity and diversity of data. Probabilistic inference is the core technique for developing training and simulation algorithms on probabilistic models. However, the classic inference methods, like Markov chain Monte Carlo (MCMC) methods and mean-field variational inference (VI), are not computationally scalable for the recent developed probabilistic models with neural networks (NNs). This motivates many recent works on improving classic inference methods using NNs, especially, NN empowered VI. However, even with powerful NNs, VI still suffers its fundamental limitations. In this work, we propose a novel computational scalable general inference framework. With the theoretical foundation in ergodic theory, the proposed methods are not only computationally scalable like NN-based VI methods but also asymptotically accurate like MCMC. We test our method on popular benchmark problems and the results suggest that our methods can outperform NN-based VI and MCMC on deep generative models and Bayesian neural networks.", "ref_function": ["background", "background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_5": {"mid": "2592505114", "abstract": "We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende & Mohamed, 2015) while still allowing for local reparametrizations (, 2015) and a tractable lower bound (, 2015; , 2016). In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.", "ref_function": ["background", "method", "result"], "cite_purpose": ["background", "background"]}, "@cite_34": {"mid": "2897001865", "abstract": "Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We fix VB and turn it into a robust inference tool for Bayesian neural networks. We achieve this with two innovations: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate strong predictive performance over alternative approaches.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Since the implementation of deep learning became practical, text detection techniques are based on neural networks.", "A deep learning based method @cite_2 uses fully convolutional network (FCN) to find a probability that pixels belong to a text area.", "After applying maximally stable extremal regions (MSER), a shortened FCN was utilized to acquire the character centroids and with the help of intensity and geometric criteria remove false candidates."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Text detection in natural images is a challenging but necessary task for many applications. Existing approaches utilize large deep convolutional neural networks making it difficult to use them in real-world tasks. We propose a small yet relatively precise text extraction method. The basic component of it is a convolutional neural network which works in a fully-convolutional manner and produces results at multiple scales. Each scale output predicts whether a pixel is a part of some word, its geometry, and its relation to neighbors at the same scale and between scales. The key factor of reducing the complexity of the model was the utilization of depthwise separable convolution, linear bottlenecks, and inverted residuals. Experiments on public datasets show that the proposed network can effectively detect text while keeping the number of parameters in the range of 1.58 to 10.59 million in different configurations.", "reference": {"@cite_2": {"mid": "2339589954", "abstract": "In this paper, we propose a novel approach for text detection in natural images. Both local and global cues are taken into account for localizing text lines in a coarse-to-fine procedure. First, a Fully Convolutional Network (FCN) model is trained to predict the salient map of text regions in a holistic manner. Then, text line hypotheses are estimated by combining the salient map and character components. Finally, another FCN classifier is used to predict the centroid of each character, in order to remove the false hypotheses. The framework is general for handling text in multiple orientations, languages and fonts. The proposed method consistently achieves the state-of-the-art performance on three text detection benchmarks: MSRA-TD500, ICDAR2015 and ICDAR2013.", "ref_function": ["objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Shi in @cite_4 proposed to find segments of words and connections between them.", "The whole detection process of segments and links was done in a single pass of a CNN named SegLink in a fully-convolutional manner with depth-first search (DFS) and bounding box creation postprocessing."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Text detection in natural images is a challenging but necessary task for many applications. Existing approaches utilize large deep convolutional neural networks making it difficult to use them in real-world tasks. We propose a small yet relatively precise text extraction method. The basic component of it is a convolutional neural network which works in a fully-convolutional manner and produces results at multiple scales. Each scale output predicts whether a pixel is a part of some word, its geometry, and its relation to neighbors at the same scale and between scales. The key factor of reducing the complexity of the model was the utilization of depthwise separable convolution, linear bottlenecks, and inverted residuals. Experiments on public datasets show that the proposed network can effectively detect text while keeping the number of parameters in the range of 1.58 to 10.59 million in different configurations.", "reference": {"@cite_4": {"mid": "2605076167", "abstract": "Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line, A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0 on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512x512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.", "ref_function": ["background", "method", "objective", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Zhou proposed a similar strategy in @cite_3 where a variety of postprocessing steps were eliminated by performing most of the calculations in a single U-Net-like @cite_12 FCN named EAST which outputs word box parameters by itself.", "Results of computations are filtered by non-maximum suppression (NMS) and thresholding.", "The length of the word to be detected is limited by a receptive field of output pixels."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Text detection in natural images is a challenging but necessary task for many applications. Existing approaches utilize large deep convolutional neural networks making it difficult to use them in real-world tasks. We propose a small yet relatively precise text extraction method. The basic component of it is a convolutional neural network which works in a fully-convolutional manner and produces results at multiple scales. Each scale output predicts whether a pixel is a part of some word, its geometry, and its relation to neighbors at the same scale and between scales. The key factor of reducing the complexity of the model was the utilization of depthwise separable convolution, linear bottlenecks, and inverted residuals. Experiments on public datasets show that the proposed network can effectively detect text while keeping the number of parameters in the range of 1.58 to 10.59 million in different configurations.", "reference": {"@cite_12": {"mid": "1901129140", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http: lmb.informatik.uni-freiburg.de people ronneber u-net .", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_3": {"mid": "2605982830", "abstract": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.", "ref_function": ["background", "background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["An ArbiText network @cite_8 based on the Single Shot Detector (SSD) applies the circle anchors to replace bounding boxes which should be more robust to orientation variations.", "Authors also applied pyramid pooling to preserve low-level features in deeper layers."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Text detection in natural images is a challenging but necessary task for many applications. Existing approaches utilize large deep convolutional neural networks making it difficult to use them in real-world tasks. We propose a small yet relatively precise text extraction method. The basic component of it is a convolutional neural network which works in a fully-convolutional manner and produces results at multiple scales. Each scale output predicts whether a pixel is a part of some word, its geometry, and its relation to neighbors at the same scale and between scales. The key factor of reducing the complexity of the model was the utilization of depthwise separable convolution, linear bottlenecks, and inverted residuals. Experiments on public datasets show that the proposed network can effectively detect text while keeping the number of parameters in the range of 1.58 to 10.59 million in different configurations.", "reference": {"@cite_8": {"mid": "2771082502", "abstract": "Arbitrary-oriented text detection in the wild is a very challenging task, due to the aspect ratio, scale, orientation, and illumination variations. In this paper, we propose a novel method, namely Arbitrary-oriented Text (or ArbText for short) detector, for efficient text detection in unconstrained natural scene images. Specifically, we first adopt the circle anchors rather than the rectangular ones to represent bounding boxes, which is more robust to orientation variations. Subsequently, we incorporate a pyramid pooling module into the Single Shot MultiBox Detector framework, in order to simultaneously explore the local and global visual information, which can, therefore, generate more confidential detection results. Experiments on established scene-text datasets, such as the ICDAR 2015 and MSRA-TD500 datasets, have demonstrated the supe rior performance of the proposed method, compared to the state-of-the-art approaches.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Liu in @cite_13 combined text detection and recognition parts in one end-to-end CNN.", "The backbone of the network is the Feature Pyramid Network which incorporates residual operations from ResNet-50 @cite_15 .", "The network, in text detection part, outputs text probability, bounding box distances in four directions, and a rotation angle of the bounding box.", "The smallest real-time version contains 29 million parameters."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Text detection in natural images is a challenging but necessary task for many applications. Existing approaches utilize large deep convolutional neural networks making it difficult to use them in real-world tasks. We propose a small yet relatively precise text extraction method. The basic component of it is a convolutional neural network which works in a fully-convolutional manner and produces results at multiple scales. Each scale output predicts whether a pixel is a part of some word, its geometry, and its relation to neighbors at the same scale and between scales. The key factor of reducing the complexity of the model was the utilization of depthwise separable convolution, linear bottlenecks, and inverted residuals. Experiments on public datasets show that the proposed network can effectively detect text while keeping the number of parameters in the range of 1.58 to 10.59 million in different configurations.", "reference": {"@cite_15": {"mid": "2194775991", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57 error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "ref_function": ["background", "background", "method", "result", "result", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["uses"]}, "@cite_13": {"mid": "2964018263", "abstract": "Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specifically, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method makes our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5 on ICDAR 2015 text spotting task while keeping 22.6 fps.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["One group of methods have considered confounding factors that are either singularly labeled or cannot be labeled.", "Ben-David et.", "al @cite_49 showed that a classifier trained to predict the sentiment of reviews can implicitly learn to predict the category of the products.", "The authors used an adversarial multi-task classifier to learn domain invariant sentiment representations.", "Shinohara @cite_38 used an adversarial approach to train noise-robust networks for automatic speech recognition.", "They used domain (i.e., background noise) as the adversarial task while training the model to obtain representations that are both senone-discriminative and domain-invariant.", "In emotion recognition applications, @cite_44 used domain adversarial networks to improve cross-corpus generalization for emotion recognition tasks."], "label": ["General descriptions of the topic", "Not sure", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Various psychological factors affect how individuals express emotions. Yet, when we collect data intended for use in building emotion recognition systems, we often try to do so by creating paradigms that are designed just with a focus on eliciting emotional behavior. Algorithms trained with these types of data are unlikely to function outside of controlled environments because our emotions naturally change as a function of these other factors. In this work, we study how the multimodal expressions of emotion change when an individual is under varying levels of stress. We hypothesize that stress produces modulations that can hide the true underlying emotions of individuals and that we can make emotion recognition algorithms more generalizable by controlling for variations in stress. To this end, we use adversarial networks to decorrelate stress modulations from emotion representations. We study how stress alters acoustic and lexical emotional predictions, paying special attention to how modulations due to stress affect the transferability of learned emotion recognition models across domains. Our results show that stress is indeed encoded in trained emotion classifiers and that this encoding varies across levels of emotions and across the lexical and acoustic modalities. Our results also show that emotion recognition models that control for stress during training have better generalizability when applied to new domains, compared to models that do not control for stress during training. We conclude that is is necessary to consider the effect of extraneous psychological factors when building and testing emotion recognition models.", "reference": {"@cite_44": {"mid": "2963447013", "abstract": "The performance of speech emotion recognition is affected by the differences in data distributions between train (source domain) and test (target domain) sets used to build and evaluate the models. This is a common problem, as multiple studies have shown that the performance of emotional classifiers drops when they are exposed to data that do not match the distribution used to build the emotion classifiers. The difference in data distributions becomes very clear when the training and testing data come from different domains, causing a large performance gap between development and testing performance. Due to the high cost of annotating new data and the abundance of unlabeled data, it is crucial to extract as much useful information as possible from the available unlabeled data. This study looks into the use of adversarial multitask training to extract a common representation between train and test domains. The primary task is to predict emotional-attribute-based descriptors for arousal, valence, or dominance. The secondary task is to learn a common representation, where the train and test domains cannot be distinguished. By using a gradient reversal layer, the gradients coming from the domain classifier are used to bring the source and target domain representations closer. We show that exploiting unlabeled data consistently leads to better emotion recognition performance across all emotional dimensions. We visualize the effect of adversarial training on the feature representation across the proposed deep learning architecture. The analysis shows that the data representations for the train and test domains converge as the data are passed to deeper layers of the network. We also evaluate the difference in performance when we use a shallow neural network versus a deep neural network and the effect of the number of shared layers used by the task and domain classifiers.", "ref_function": ["background", "background", "background", "background", "objective", "objective", "objective", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_38": {"mid": "2510867321", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_49": {"mid": "2104094955", "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time? We address the first question by bounding a classifier's target error in terms of its source error and the divergence between the two domains. We give a classifier-induced divergence measure that can be estimated from finite, unlabeled samples from the domains. Under the assumption that there exists some hypothesis that performs well in both domains, we show that this quantity together with the empirical source error characterize the target error of a source-trained classifier. We answer the second question by bounding the target error of a model which minimizes a convex combination of the empirical source and target errors. Previous theoretical work has considered minimizing just the source error, just the target error, or weighting instances from the two domains equally. We show how to choose the optimal combination of source and target error as a function of the divergence, the sample sizes of both domains, and the complexity of the hypothesis class. The resulting bound generalizes the previously studied cases and is always at least as tight as a bound which considers minimizing only the target error or an equal weighting of source and target errors.", "ref_function": ["background", "background", "objective", "objective", "objective", "method", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["The task of reconstructing a full classical description -- the density matrix @math -- of a @math -dimensional quantum system from experimental data is one of the most fundamental problems in quantum statistics, see e.g.", "@cite_52 @cite_13 @cite_12 @cite_31 and references therein.", "Sample-optimal protocols, i.e.", "estimation techniques that get by with a minimal number of measurement repetitions, have only been developed recently.", "Information-theoretic bounds assert that an order of @math state copies are necessary to fully reconstruct @math @cite_26 .", "Constructive protocols @cite_29 @cite_26 saturate this bound, but require entangled circuits and measurements that act on all state copies simultaneously.", "More tractable measurement procedures, where each copy of the state is measured independently, require an order of @math measurements @cite_26 .", "This more stringent bound is saturated by low rank matrix recovery @cite_4 @cite_44 @cite_40 and projected least squares estimation @cite_18 ."], "label": ["General reference to previous research or scholarship: research objective", "Not sure", "Not sure", "General descriptions of the topic", "Not sure", "General reference to previous research or scholarship: about results", "Not sure", "Not sure"], "target_paper": "Predicting features of complex, large-scale quantum systems is essential to the characterization and engineering of quantum architectures. We present an efficient approach for predicting a large number of linear features using classical shadows obtained from very few quantum measurements. This approach is guaranteed to accurately predict @math linear functions with bounded Hilbert-Schmidt norm from only @math measurement repetitions. This sampling rate is completely independent of the system size and saturates fundamental lower bounds from information theory. We support our theoretical findings with numerical experiments over a wide range of problem sizes (2 to 162 qubits). These highlight advantages compared to existing machine learning approaches.", "reference": {"@cite_18": {"mid": "2893608605", "abstract": "Projected least squares (PLS) is an intuitive and numerically cheap technique for quantum state tomography. The method first computes the least-squares estimator (or a linear inversion estimator) and then projects the initial estimate onto the space of states. The main result of this paper equips this point estimator with a rigorous, non-asymptotic confidence region expressed in terms of the trace distance. The analysis holds for a variety of measurements, including 2-designs and Pauli measurements. The sample complexity of the estimator is comparable to the strongest convergence guarantees available in the literature and---in the case of measuring the uniform POVM---saturates fundamental lower bounds.The results are derived by reinterpreting the least-squares estimator as a sum of random matrices and applying a matrix-valued concentration inequality. The theory is supported by numerical simulations for mutually unbiased bases, Pauli observables, and Pauli basis measurements.", "ref_function": ["background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_26": {"mid": "2649051464", "abstract": "It is a fundamental problem to decide how many copies of an unknown mixed quantum state are necessary and sufficient to determine the state. Previously, it was known only that estimating states to error @math in trace distance required @math copies for a @math -dimensional density matrix of rank @math . Here, we give a theoretical measurement scheme (POVM) that requires @math copies to estimate @math to error @math in infidelity, and a matching lower bound up to logarithmic factors. This implies @math copies suffice to achieve error @math in trace distance. We also prove that for independent (product) measurements, @math copies are necessary in order to achieve error @math in infidelity. For fixed @math , our measurement can be implemented on a quantum computer in time polynomial in @math .", "ref_function": ["background", "background", "method", "method", "method", "result"], "cite_purpose": ["background", "motivation", "background", "background"]}, "@cite_4": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_29": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["motivation", "background"]}, "@cite_52": {"mid": "1965471276", "abstract": "An algorithm for quantum-state estimation based on the maximum-likelihood estimation is proposed. Existing techniques for state reconstruction based on the inversion of measured data are shown to be overestimated since they do not guarantee the positive definiteness of the reconstructed density matrix.", "ref_function": ["background", "method"], "cite_purpose": ["background"]}, "@cite_44": {"mid": "2963583445", "abstract": "Abstract We study the recovery of Hermitian low rank matrices X \u2208 C n \u00d7 n from undersampled measurements via nuclear norm minimization. We consider the particular scenario where the measurements are Frobenius inner products with random rank-one matrices of the form a j a j \u204e for some measurement vectors a 1 , \u2026 , a m , i.e., the measurements are given by b j = tr ( X a j a j \u204e ) . The case where the matrix X = x x \u204e to be recovered is of rank one reduces to the problem of phaseless estimation (from measurements b j = | \u3008 x , a j \u3009 | 2 ) via the PhaseLift approach, which has been introduced recently. We derive bounds for the number m of measurements that guarantee successful uniform recovery of Hermitian rank r matrices, either for the vectors a j , j = 1 , \u2026 , m , being chosen independently at random according to a standard Gaussian distribution, or a j being sampled independently from an (approximate) complex projective t-design with t = 4 . In the Gaussian case, we require m \u2265 C r n measurements, while in the case of 4-designs we need m \u2265 Cr n log \u2061 ( n ) . Our results are uniform in the sense that one random choice of the measurement vectors a j guarantees recovery of all rank r-matrices simultaneously with high probability. Moreover, we prove robustness of recovery under perturbation of the measurements by noise. The result for approximate 4-designs generalizes and improves a recent bound on phase retrieval due to Gross, Krahmer and Kueng. In addition, it has applications in quantum state tomography. Our proofs employ the so-called bowling scheme which is based on recent ideas by Mendelson and Koltchinskii.", "ref_function": ["background", "background", "background", "method", "method", "result", "result", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_40": {"mid": "2539873326", "abstract": "We prove that low-rank matrices can be recovered efficiently from a small number of measurements that are sampled from orbits of a certain matrix group. As a special case, our theory makes statements about the phase retrieval problem. Here, the task is to recover a vector given only the amplitudes of its inner product with a small number of vectors from an orbit. Variants of the group in question have appeared under different names in many areas of mathematics. In coding theory and quantum information, it is the complex Clifford group; in time-frequency analysis the oscillator group; and in mathematical physics the metaplectic group. It affords one particularly small and highly structured orbit that includes and generalizes the discrete Fourier basis: While the Fourier vectors have coefficients of constant modulus and phases that depend linearly on their index, the vectors in said orbit have phases with a quadratic dependence. In quantum information, the orbit is used extensively and is known as the set of stabilizer states. We argue that due to their rich geometric structure and their near-optimal recovery properties, stabilizer states form an ideal model for structured measurements for phase retrieval. Our results hold for @math measurements, where the oversampling factor k varies between @math and @math depending on the orbit. The reconstruction is stable towards both additive noise and deviations from the assumption of low rank. If the matrices of interest are in addition positive semidefinite, reconstruction may be performed by a simple constrained least squares regression. Our proof methods could be adapted to cover orbits of other groups.", "ref_function": ["background", "background", "objective", "background", "background", "result", "background", "background", "result", "method", "method", "method"], "cite_purpose": ["background"]}, "@cite_31": {"mid": "2063059850", "abstract": "Quantum tomography has come a long way from early reconstructions of Wigner functions from projections along quadratures to the full characterization of multipartite systems. Now, it is routinely carried out in a wide variety of systems. And yet, many fundamental questions remain unanswered. In recent years, a spate of radical new experimental, theoretical and mathematical developments have occurred. The appeal of the subject lies largely in the breadth of techniques that must be brought together in order to fully understand the problem. This \u2018focus on\u2019 collection provides a platform for facilitating the exchange of ideas between the different communities involved in this process.", "ref_function": ["background", "background", "background", "background", "background", "result"], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2098271372", "abstract": "Accurately inferring the state of a quantum device from the results of measurements is a crucial task in building quantum information processing hardware. The predominant state estimation procedure, maximum likelihood estimation (MLE), generally reports an estimate with zero eigenvalues. These cannot be justified. Furthermore, the MLE estimate is incompatible with error bars, so conclusions drawn from it are suspect. I propose an alternative procedure, Bayesian mean estimation (BME). BME never yields zero eigenvalues, its eigenvalues provide a bound on their own uncertainties, and under certain circumstances it is provably the most accurate procedure possible. I show how to implement BME numerically, and how to obtain natural error bars that are compatible with the estimate. Finally, I briefly discuss the differences between Bayesian and frequentist estimation techniques.", "ref_function": ["background", "background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_12": {"mid": "1529624360", "abstract": "We establish methods for quantum state tomography based on compressed sensing. These methods are specialized for quantum states that are fairly pure, and they offer a significant performance improvement on large quantum systems. In particular, they are able to reconstruct an unknown density matrix of dimension d and rank r using O(rdlog^2d) measurement settings, compared to standard methods that require d^2 settings. Our methods have several features that make them amenable to experimental implementation: they require only simple Pauli measurements, use fast convex optimization, are stable against noise, and can be applied to states that are only approximately low rank. The acquired data can be used to certify that the state is indeed close to pure, so no a priori assumptions are needed.", "ref_function": ["method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Restricting attention to highly structured subsets of quantum states sometimes allows for overcoming the exponential bottleneck that plagues general tomography.", "Matrix product state (MPS) tomography @cite_41 is the most prominent example for such an approach.", "It only requires a polynomial number of samples, provided that the underlying quantum state is well approximated by a MPS with low bond dimension.", "In quantum many body physics this assumption is often justifiable @cite_54 .", "However, MPS representations of general states have exponentially large bond dimension.", "In this case, MPS tomography offers no advantage over general tomography."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "Predicting features of complex, large-scale quantum systems is essential to the characterization and engineering of quantum architectures. We present an efficient approach for predicting a large number of linear features using classical shadows obtained from very few quantum measurements. This approach is guaranteed to accurately predict @math linear functions with bounded Hilbert-Schmidt norm from only @math measurement repetitions. This sampling rate is completely independent of the system size and saturates fundamental lower bounds from information theory. We support our theoretical findings with numerical experiments over a wide range of problem sizes (2 to 162 qubits). These highlight advantages compared to existing machine learning approaches.", "reference": {"@cite_41": {"mid": "1971384536", "abstract": "Direct quantum state tomography\u2014deducing the state of a system from measurements\u2014is mostly unfeasible due to the exponential scaling of measurement number with system size. The authors present two new schemes, which scale linearly in this respect, and can be applied to a wide range of quantum states.", "ref_function": ["background", "method"], "cite_purpose": ["background"]}, "@cite_54": {"mid": "2565722603", "abstract": "Traditionally quantum state tomography is used to characterize a quantum state, but it becomes exponentially hard with the system size. An alternative technique, matrix product state tomography, is shown to work well in practical situations.", "ref_function": ["background", "method"], "cite_purpose": ["background"]}}}
{"sentences": ["Direct fidelity estimation is a procedure that allows for predicting a single pure target fidelity @math up to accuracy @math .", "The best-known technique is based on few Pauli measurements that are selected randomly using importance sampling @cite_49 .", "The required number of samples depends on the target: it can range from a dimension-independent order of @math (if @math is a stablizer state) to roughly @math in the worst case."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Predicting features of complex, large-scale quantum systems is essential to the characterization and engineering of quantum architectures. We present an efficient approach for predicting a large number of linear features using classical shadows obtained from very few quantum measurements. This approach is guaranteed to accurately predict @math linear functions with bounded Hilbert-Schmidt norm from only @math measurement repetitions. This sampling rate is completely independent of the system size and saturates fundamental lower bounds from information theory. We support our theoretical findings with numerical experiments over a wide range of problem sizes (2 to 162 qubits). These highlight advantages compared to existing machine learning approaches.", "reference": {"@cite_49": {"mid": "2090368878", "abstract": "We describe a simple method for certifying that an experimental device prepares a desired quantum state \u03c1. Our method is applicable to any pure state \u03c1, and it provides an estimate of the fidelity between \u03c1 and the actual (arbitrary) state in the lab, up to a constant additive error. The method requires measuring only a constant number of Pauli expectation values, selected at random according to an importance-weighting rule. Our method is faster than full tomography by a factor of d, the dimension of the state space, and extends easily and naturally to quantum channels.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Shadow tomography aims at simultaneously estimating the probability associated with @math 2-outcome measurements up to accuaracy @math : @math , where each @math is a positive semidefinite matrix whose with operator norm at most one @cite_15 @cite_5 @cite_47 .", "This may be viewed as a generalization of direct fidelity estimation.", "The best existing result is due to Aaronson @cite_47 who showed that copies of the unknown state The scaling symbol @math suppresses logarithmic expressions in other problem-specific parameters.", "suffice to achieve this task.", "In a nutshell, his protocol is based on gently measuring the 2-outcome measurements one-by-one and subsequently (partially) reverting the perturbative effects a measurement exerts on quantum states.", "This task is achieved by explicit quantum circuits of exponential size that act on all copies of the unknown state simultaneously.", "This rather intricate procedure bypasses the no-go result advertised in Theorem and results in a sampling rate that is independent of the measurement in question -- only their cardinality @math matters."], "label": ["General reference to previous research or scholarship: approaches taken", "Reference to current state of knowledge", "Reference to single investigations in the past: about method", "Not sure", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result"], "target_paper": "Predicting features of complex, large-scale quantum systems is essential to the characterization and engineering of quantum architectures. We present an efficient approach for predicting a large number of linear features using classical shadows obtained from very few quantum measurements. This approach is guaranteed to accurately predict @math linear functions with bounded Hilbert-Schmidt norm from only @math measurement repetitions. This sampling rate is completely independent of the system size and saturates fundamental lower bounds from information theory. We support our theoretical findings with numerical experiments over a wide range of problem sizes (2 to 162 qubits). These highlight advantages compared to existing machine learning approaches.", "reference": {"@cite_5": {"mid": "2797355014", "abstract": "We give two new quantum algorithms for solving semidefinite programs (SDPs) providing quantum speed-ups. We consider SDP instances with @math constraint matrices, each of dimension @math , rank @math , and sparsity @math . The first algorithm assumes an input model where one is given access to entries of the matrices at unit cost. We show that it has run time @math , where @math is the error. This gives an optimal dependence in terms of @math and quadratic improvement over previous quantum algorithms when @math . The second algorithm assumes a fully quantum input model in which the matrices are given as quantum states. We show that its run time is @math , with @math an upper bound on the trace-norm of all input matrices. In particular the complexity depends only poly-logarithmically in @math and polynomially in @math . We apply the second SDP solver to the problem of learning a good description of a quantum state with respect to a set of measurements: Given @math measurements and copies of an unknown state @math , we show we can find in time @math a description of the state as a quantum circuit preparing a density matrix which has the same expectation values as @math on the @math measurements, up to error @math . The density matrix obtained is an approximation to the maximum entropy state consistent with the measurement data considered in Jaynes' principle from statistical mechanics. As in previous work, we obtain our algorithm by \"quantizing\" classical SDP solvers based on the matrix multiplicative weight method. One of our main technical contributions is a quantum Gibbs state sampler for low-rank Hamiltonians with a poly-logarithmic dependence on its dimension, which could be of independent interest.", "ref_function": ["background", "background", "method", "method", "method", "result", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "2963956175", "abstract": "We introduce the problem of *shadow tomography*: given an unknown D-dimensional quantum mixed state \u03c1, as well as known two-outcome measurements E1,\u2026,EM, estimate the probability that Ei accepts \u03c1, to within additive error e, for each of the M measurements. How many copies of \u03c1 are needed to achieve this, with high probability? Surprisingly, we give a procedure that solves the problem by measuring only O( e\u22125\u00b7log4 M\u00b7logD) copies. This means, for example, that we can learn the behavior of an arbitrary n-qubit state, on *all* accepting rejecting circuits of some fixed polynomial size, by measuring only nO( 1) copies of the state. This resolves an open problem of the author, which arose from his work on private-key quantum money schemes, but which also has applications to quantum copy-protected software, quantum advice, and quantum one-way communication. Recently, building on this work, have given a different approach to shadow tomography using semidefinite programming, which achieves a savings in computation time.", "ref_function": ["background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_47": {"mid": "2939719762", "abstract": "", "ref_function": [], "cite_purpose": ["background", "background"]}}}
{"sentences": ["The first and second approaches solve a different problem (of feature importance across all the training data), and we will ignore them for the most part.", "Notice that the rest are solving the attribution problem, @cite_13 unifies several of these methods under a common framework based on conditional expectations.", "and they all apply the Shapley value, but they differ in how they switch a feature off', and consequently give different results.", "In this paper, we attempt to pick between these methods using the lens of axiomatization."], "label": ["Explaining the objective relationship between own work and references", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Describing used methods"], "target_paper": "The Shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. The Shapley value [1] is known to be the unique method that satisfies certain desirable properties, and this motivates its use. Unfortunately, despite this uniqueness result, there are a multiplicity of Shapley values used in explaining a model's prediction. This is because there are many ways to apply the Shapley value that differ in how they reference the model, the training data, and the explanation context. In this paper, we study an approach that applies the Shapley value to conditional expectations (CES) of sets of features (cf. [2]) that subsumes several prior approaches within a common framework. We provide the first algorithm for the general version of CES. We show that CES can result in counterintuitive attributions in theory and in practice (we study a diabetes prediction task); for instance, CES can assign non-zero attributions to features that are not referenced by the model. In contrast, we show that an approach called the Baseline Shapley (BS) does not exhibit counterintuitive attributions; we support this claim with a uniqueness (axiomatic) result. We show that BS is a special case of CES, and CES with an independent feature distribution coincides with a randomized version of BS. Thus, BS fits into the CES framework, but does not suffer from many of CES's deficiencies.", "reference": {"@cite_13": {"mid": "2962862931", "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and or better consistency with human intuition than previous approaches.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Crowd Counting: Numerous deep learning based methods @cite_33 @cite_13 @cite_20 @cite_40 @cite_46 @cite_6 @cite_10 have been proposed for crowd counting.", "These methods have various network structures and the mainstream is a multiscale architecture, which extracts multiple features from different columns branches of networks to handle the scale variation of people.", "For instance, @cite_21 combined a deep network and a shallow network to learn scale-robust features.", "@cite_0 developed a multi-column CNN to generate density maps.", "HydraCNN @cite_45 fed a pyramid of image patches into networks to estimate the count.", "CP-CNN @cite_3 proposed a Contextual Pyramid CNN to incorporate the global and local contextual information for crowd counting.", "@cite_4 built an encoder-decoder network with multiple scale aggregation modules.", "However, the issue of the huge variation of people's scales is still far from being fully solved."], "label": ["General reference to previous research or scholarship: approaches taken", "General reference to previous research or scholarship: approaches taken", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies"], "target_paper": "Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. In this module, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. Specifically, our DSSINet achieves improvements of 9.5 error reduction on Shanghaitech dataset and 24.9 on UCF-QNRF dataset against the state-of-the-art methods.", "reference": {"@cite_4": {"mid": "2895051362", "abstract": "In this paper, we propose a novel encoder-decoder network, called Scale Aggregation Network (SANet), for accurate and efficient crowd counting. The encoder extracts multi-scale features with scale aggregation modules and the decoder generates high-resolution density maps by using a set of transposed convolutions. Moreover, we find that most existing works use only Euclidean loss which assumes independence among each pixel but ignores the local correlation in density maps. Therefore, we propose a novel training loss, combining of Euclidean loss and local pattern consistency loss, which improves the performance of the model in our experiments. In addition, we use normalization layers to ease the training process and apply a patch-based test scheme to reduce the impact of statistic shift problem. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on four major crowd counting datasets and our method achieves superior performance to state-of-the-art methods while with much less parameters.", "ref_function": ["objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_33": {"mid": "2520826941", "abstract": "In this paper, we address the task of object counting in images. We follow modern learning approaches in which a density map is estimated directly from the input image. We employ CNNs and incorporate two significant improvements to the state of the art methods: layered boosting and selective sampling. As a result, we manage both to increase the counting accuracy and to reduce processing time. Moreover, we show that the proposed method is effective, even in the presence of labeling errors. Extensive experiments on five different datasets demonstrate the efficacy and robustness of our approach. Mean Absolute error was reduced by 20 to 35 . At the same time, the training time of each CNN has been reduced by 50 .", "ref_function": ["objective", "method", "method", "method", "method", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_10": {"mid": "2951955299", "abstract": "Crowd counting from a single image is a challenging task due to high appearance similarity, perspective changes and severe congestion. Many methods only focus on the local appearance features and they cannot handle the aforementioned challenges. In order to tackle them, we propose a Perspective Crowd Counting Network (PCC Net), which consists of three parts: 1) Density Map Estimation (DME) focuses on learning very local features for density map estimation; 2) Random High-level Density Classification (R-HDC) extracts global features to predict the coarse density labels of random patches in images; 3) Fore- Background Segmentation (FBS) encodes mid-level features to segments the foreground and background. Besides, the DULR module is embedded in PCC Net to encode the perspective changes on four directions (Down, Up, Left and Right). The proposed PCC Net is verified on five mainstream datasets, which achieves the state-of-the-art performance on the one and attains the competitive results on the other four datasets. The source code is available at this https URL.", "ref_function": ["background", "background", "method", "method", "result", "other"], "cite_purpose": ["background"]}, "@cite_21": {"mid": "2517615595", "abstract": "Our work proposes a novel deep learning framework for estimating crowd density from static images of highly dense crowds. We use a combination of deep and shallow, fully convolutional networks to predict the density map for a given crowd image. Such a combination is used for effectively capturing both the high-level semantic information (face body detectors) and the low-level features (blob detectors), that are necessary for crowd counting under large scale variations. As most crowd datasets have limited training samples (", "ref_function": ["objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_6": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_3": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_0": {"mid": "2463631526", "abstract": "This paper aims to develop a method than can accurately estimate the crowd count from an individual image with arbitrary crowd density and arbitrary perspective. To this end, we have proposed a simple but effective Multi-column Convolutional Neural Network (MCNN) architecture to map the image to its crowd density map. The proposed MCNN allows the input image to be of arbitrary size or resolution. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people head size due to perspective effect or image resolution. Furthermore, the true density map is computed accurately based on geometry-adaptive kernels which do not need knowing the perspective map of the input image. Since exiting crowd counting datasets do not adequately cover all the challenging situations considered in our work, we have collected and labelled a large new dataset that includes 1198 images with about 330,000 heads annotated. On this challenging new dataset, as well as all existing datasets, we conduct extensive experiments to verify the effectiveness of the proposed model and method. In particular, with the proposed simple MCNN model, our method outperforms all existing methods. In addition, experiments show that our model, once trained on one dataset, can be readily transferred to a new dataset.", "ref_function": ["objective", "method", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_40": {"mid": "2964203052", "abstract": "In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.", "ref_function": ["background", "background", "method", "background", "result", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_45": {"mid": "2519281173", "abstract": "In this paper we address the problem of counting objects instances in images. Our models are able to precisely estimate the number of vehicles in a traffic congestion, or to count the humans in a very crowded scene. Our first contribution is the proposal of a novel convolutional neural network solution, named Counting CNN (CCNN). Essentially, the CCNN is formulated as a regression model where the network learns how to map the appearance of the image patches to their corresponding object density maps. Our second contribution consists in a scale-aware counting model, the Hydra CNN, able to estimate object densities in different very crowded scenarios where no geometric information of the scene can be provided. Hydra CNN learns a multiscale non-linear regression model which uses a pyramid of image patches extracted at multiple scales to perform the final density prediction. We report an extensive experimental evaluation, using up to three different object counting benchmarks, where we show how our solutions achieve a state-of-the-art performance.", "ref_function": ["background", "background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_46": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_13": {"mid": "2741077351", "abstract": "We propose a novel crowd counting model that maps a given crowd scene to its density. Crowd analysis is compounded by myriad of factors like inter-occlusion between people due to extreme crowding, high similarity of appearance between people and background elements, and large variability of camera view-points. Current state-of-the art approaches tackle these factors by using multi-scale CNN architectures, recurrent networks and late fusion of features from multi-column CNN with different receptive fields. We propose switching convolutional neural network that leverages variation of crowd density within an image to improve the accuracy and localization of the predicted crowd count. Patches from a grid within a crowd scene are relayed to independent CNN regressors based on crowd count prediction quality of the CNN established during training. The independent CNN regressors are designed to have different receptive fields and a switch classifier is trained to relay the crowd scene patch to the best CNN regressor. We perform extensive experiments on all major crowd counting datasets and evidence better performance compared to current state-of-the-art methods. We provide interpretable representations of the multichotomy of space of crowd scene patches inferred from the switch. It is observed that the switch relays an image patch to a particular CNN column based on density of crowd.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2964018834", "abstract": "Region of Interest (ROI) crowd counting can be formulated as a regression problem of learning a mapping from an image or a video frame to a crowd density map. Recently, convolutional neural network (CNN) models have achieved promising results for crowd counting. However, even when dealing with video data, CNN-based methods still consider each video frame independently, ignoring the strong temporal correlation between neighboring frames. To exploit the otherwise very useful temporal information in video sequences, we propose a variant of a recent deep learning model called convolutional LSTM (ConvLSTM) for crowd counting. Unlike the previous CNN-based methods, our method fully captures both spatial and temporal dependencies. Furthermore, we extend the ConvLSTM model to a bidirectional ConvLSTM model which can access long-range information in both directions. Extensive experiments using four publicly available datasets demonstrate the reliability of our approach and the effectiveness of incorporating temporal information to boost the accuracy of crowd counting. In addition, we also conduct some transfer learning experiments to show that once our model is trained on one dataset, its learning experience can be transferred easily to a new dataset which consists of only very few video frames for model adaptation.", "ref_function": ["background", "background", "background", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Conditional Random Fields: In the field of computer vision, CRFs have been exploited to refine the features and outputs of convolutional neural networks (CNN) with a message passing mechanism @cite_25 .", "For instance, @cite_29 used CRFs to refine the semantic segmentation maps of CNN by modeling the relationship among pixels.", "@cite_44 fused multiple features with Attention-Gated CRFs to produce richer representations for contour prediction.", "@cite_28 introduced an inter-view message passing module based on CRFs to enhance the view-specific features for action recognition."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. In this module, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. Specifically, our DSSINet achieves improvements of 9.5 error reduction on Shanghaitech dataset and 24.9 on UCF-QNRF dataset against the state-of-the-art methods.", "reference": {"@cite_44": {"mid": "2964088293", "abstract": "Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.", "ref_function": ["background", "objective", "objective", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_29": {"mid": "2124592697", "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_28": {"mid": "2894942405", "abstract": "In this paper, we propose a new Dividing and Aggregating Network (DA-Net) for multi-view action recognition. In our DA-Net, we learn view-independent representations shared by all views at lower layers, while we learn one view-specific representation for each view at higher layers. We then train view-specific action classifiers based on the view-specific representation for each view and a view classifier based on the shared representation at lower layers. The view classifier is used to predict how likely each video belongs to each view. Finally, the predicted view probabilities from multiple views are used as the weights when fusing the prediction scores of view-specific action classifiers. We also propose a new approach based on the conditional random field (CRF) formulation to pass message among view-specific representations from different branches to help each other. Comprehensive experiments on two benchmark datasets clearly demonstrate the effectiveness of our proposed DA-Net for multi-view action recognition.", "ref_function": ["objective", "method", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_25": {"mid": "2161236525", "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.", "ref_function": ["background", "background", "objective", "background", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Multiscale Structural Similarity: MS-SSIM @cite_37 is a widely used metric for image quality assessment.", "Its formula is based on the luminance, contrast and structure comparisons between the multiscale regions of two images.", "In @cite_22 , MS-SSIM loss has been successfully applied in image restoration tasks (e.g., image denoising and super-resolution), but its effectiveness has not been verified in high-level tasks (e.g, crowd counting).", "Recently, @cite_4 combined Euclidean loss and SSIM loss @cite_30 to optimize their network for crowd counting, but they can only capture the local correlation in regions with a fixed size."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies"], "target_paper": "Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. In this module, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. Specifically, our DSSINet achieves improvements of 9.5 error reduction on Shanghaitech dataset and 24.9 on UCF-QNRF dataset against the state-of-the-art methods.", "reference": {"@cite_30": {"mid": "2133665775", "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http: www.cns.nyu.edu spl sim lcv ssim .", "ref_function": ["background", "method", "method", "other"], "cite_purpose": ["motivation"]}, "@cite_37": {"mid": "1580389772", "abstract": "The structural similarity image quality paradigm is based on the assumption that the human visual system is highly adapted for extracting structural information from the scene, and therefore a measure of structural similarity can provide a good approximation to perceived image quality. This paper proposes a multiscale structural similarity method, which supplies more flexibility than previous single-scale methods in incorporating the variations of viewing conditions. We develop an image synthesis method to calibrate the parameters that define the relative importance of different scales. Experimental comparisons demonstrate the effectiveness of the proposed method.", "ref_function": ["background", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_4": {"mid": "2895051362", "abstract": "In this paper, we propose a novel encoder-decoder network, called Scale Aggregation Network (SANet), for accurate and efficient crowd counting. The encoder extracts multi-scale features with scale aggregation modules and the decoder generates high-resolution density maps by using a set of transposed convolutions. Moreover, we find that most existing works use only Euclidean loss which assumes independence among each pixel but ignores the local correlation in density maps. Therefore, we propose a novel training loss, combining of Euclidean loss and local pattern consistency loss, which improves the performance of the model in our experiments. In addition, we use normalization layers to ease the training process and apply a patch-based test scheme to reduce the impact of statistic shift problem. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on four major crowd counting datasets and our method achieves superior performance to state-of-the-art methods while with much less parameters.", "ref_function": ["objective", "method", "method", "method", "method", "result"], "cite_purpose": ["motivation"]}, "@cite_22": {"mid": "2562637781", "abstract": "Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is @math . In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.", "ref_function": ["background", "background", "objective", "method", "method", "result"], "cite_purpose": ["motivation", "background"]}}}
{"sentences": ["The whole concept of adversarial attacks is quite simple: let us slightly change the input to a classifying neural net so that the recognized class will change from correct to some other class (first adversarial attacks were made only on classifiers).", "The pioneering work @cite_27 formulates the task as follows:"], "label": ["General descriptions of the topic", "Not sure"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_27": {"mid": "1673923490", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In @cite_27 the authors propose to use a quasi-newton L-BFGS-B method to solve the task formulated above.", "Simpler and more efficient method called Fast Gradient-Sign Method (FGSM) is proposed in @cite_39 .", "This method suggests using the gradients with respect to the input and constructing an adversarial image using the following formula: @math (or @math in case of targeted attack).", "Here @math is a loss function (e.g.", "cross-entropy) which depends on the weights of the model @math , input @math , and label @math .", "Note that usually one step is not enough and we need to do a number of iterations described above each time using the projection to the initial input space (e.g.", "@math ).", "It is called projected gradient descent (PGD) @cite_20 ."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Other functional sentences", "Other functional sentences", "Other functional sentences", "Other functional sentences", "Reference to single investigations in the past: about method"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_27": {"mid": "1673923490", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_20": {"mid": "2640329709", "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.", "ref_function": ["background", "background", "objective", "objective", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_39": {"mid": "1945616565", "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "ref_function": ["background", "background", "background", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["It turns out that using momentum for the iterative procedure of an adversarial example construction is a good way to increase the robustness of the adversarial attack @cite_17 ."], "label": ["Reference to single investigations in the past: about result"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_17": {"mid": "2950906520", "abstract": "Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of the existing adversarial attacks can only fool a black-box model with a low success rate because of the coupling of the attack ability and the transferability. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. We won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["All the aforementioned adversarial attacks suggest that we restrict the maximum per-pixel perturbation (in case of image as an input) i.e.", "use @math norm.", "Another interesting case is when we do not concentrate on the maximum perturbation but we strive to achieve the fewest possible number of pixels to be attacked ( @math norm).", "One of the first examples of such attack is the Jacobian-based Saliency Map Attack (JSMA) @cite_6 , where the saliency maps are constructed of the pixels that are the most prone to cause the misclassification."], "label": ["Summarize the above references", "Other functional sentences", "Other functional sentences", "Other functional sentences"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_6": {"mid": "2949152835", "abstract": "Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97 adversarial success rate while only modifying on average 4.02 of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.", "ref_function": ["background", "background", "objective", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Another extreme case of attack for the @math norm is a one-pixel attack @cite_0 .", "The authors use differential evolution for this specific case, the algorithm which lies in the class of evolutionary algorithms.", "It should be mentioned that not only classification neural nets are prone to adversarial attacks.", "There are also attacks for detection and segmentation @cite_11 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "Reference to current state of knowledge", "Reference to single investigations in the past:  about objective"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_0": {"mid": "2765424254", "abstract": "Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97 of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47 confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.", "ref_function": ["background", "objective", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_11": {"mid": "2950774971", "abstract": "It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Another interesting property of the adversarial attacks is that they are transferable between different neural networks @cite_27 .", "An attack prepared using one model can successfully confuse another model with different architecture and training dataset."], "label": ["Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_27": {"mid": "1673923490", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Usually, the adversarial attacks which are constructed using the specific architecture and even the weights of the attacked model are called white-box attacks.", "If the attack has no access to model weights then it is called a black-box attack @cite_36 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past:  about objective"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_36": {"mid": "2603766943", "abstract": "Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24 of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19 and 88.94 . We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.", "ref_function": ["background", "background", "background", "method", "method", "method", "background", "background", "background", "method", "result", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Our approach is most similar to that of fine-tuning with co-training @cite_14 .", "That method begins by using low-level features to identify images within a source dataset having similar textures to a target dataset, and concludes by using a multi-task objective to fine-tune the target task using these images.", "A related approach has been used to enhance performance and reduce training time in document classification @cite_25 and to identify examples to supplement training data @cite_16 @cite_14 .", "Our goal is to extend this approach to high-level features, and to domains outside computer vision to construct a more complete map of the feature space of a trained network.", "In this way our approach has some parallels with \"learning to transfer\" approaches @cite_34 , which attempt to train a source model optimized for transfer rather than target accuracy."], "label": ["Explaining the method relationship between own work and references", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken", "Describing the objective", "Explaining the method relationship between own work and references"], "target_paper": "Transfer learning enhances learning across tasks, by leveraging previously learned representations -- if they are properly chosen. We describe an efficient method to accurately estimate the appropriateness of a previously trained model for use in a new learning task. We use this measure, which we call \"Predict To Learn\" (\"P2L\"), in the two very different domains of images and semantic relations, where it predicts, from a set of \"source\" models, the one model most likely to produce effective transfer for training a given \"target\" model. We validate our approach thoroughly, by assembling a collection of candidate source models, then fine-tuning each candidate to perform each of a collection of target tasks, and finally measuring how well transfer has been enhanced. Across 95 tasks within multiple domains (images classification and semantic relations), the P2L approach was able to select the best transfer learning model on average, while the heuristic of choosing model trained with the largest data set selected the best model in only 55 cases. These results suggest that P2L captures important information in common between source and target tasks, and that this shared informational structure contributes to successful transfer learning more than simple data size.", "reference": {"@cite_34": {"mid": "2745420784", "abstract": "Transfer learning borrows knowledge from a source domain to facilitate learning in a target domain. Two primary issues to be addressed in transfer learning are what and how to transfer. For a pair of domains, adopting different transfer learning algorithms results in different knowledge transferred between them. To discover the optimal transfer learning algorithm that maximally improves the learning performance in the target domain, researchers have to exhaustively explore all existing transfer learning algorithms, which is computationally intractable. As a trade-off, a sub-optimal algorithm is selected, which requires considerable expertise in an ad-hoc way. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we first learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer for a newly arrived pair of domains by optimizing the reflection function. Extensive experiments demonstrate the L2T's superiority over several state-of-the-art transfer learning algorithms and its effectiveness on discovering more transferable knowledge.", "ref_function": ["background", "background", "background", "background", "method", "background", "objective", "method", "result"], "cite_purpose": ["similarities"]}, "@cite_14": {"mid": "2149933564", "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", "ref_function": ["background", "background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["similarities", "background"]}, "@cite_25": {"mid": "2963336383", "abstract": "In this article, a region-based Deep Convolutional Neural Network framework is presented for document structure learning. The contribution of this work involves efficient training of region based classifiers and effective ensembling for document image classification. A primary level of \u2018inter-domain\u2019 transfer learning is used by exporting weights from a pre-trained VGG16 architecture on the ImageNet dataset to train a document classifier on whole document images. Exploiting the nature of region based influence modelling, a secondary level of \u2018intra-domain\u2019 transfer learning is used for rapid training of deep learning models for image segments. Finally, a stacked generalization based ensembling is utilized for combining the predictions of the base deep neural network models. The proposed method achieves state-of-the-art accuracy of 92.21 on the popular RVL-CDIP document image dataset, exceeding the benchmarks set by the existing algorithms.", "ref_function": ["background", "objective", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_16": {"mid": "2591924527", "abstract": "Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a deep transfer learning scheme, called selective joint fine-tuning, for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task. Experiments demonstrate that our deep transfer learning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, and fine-grained classification problems (Oxford Flowers 102 and Stanford Dogs 120). In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2 - 10 using a single model. Codes and models are available at https: github.com ZYYSzj Selective-Joint-Fine-tuning.", "ref_function": ["background", "background", "objective", "method", "method", "objective", "method", "method", "result", "result", "other"], "cite_purpose": ["background"]}}}
{"sentences": ["Although adversarial attacks are quite successful in the digital domain (where we can change the image on the pixel level before feeding it to a classifier), in the physical (i.e.", "real) world the efficiency of adversarial attacks is still questionable.", "Kurakin demonstrate the potential for further research in this domain @cite_5 .", "They discovered that if an adversarial image is printed on the paper and then shot by a camera phone it still can successfully fool classification network."], "label": ["Explaining the inadequacies of previous studies", "Explaining the inadequacies of previous studies", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about result"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_5": {"mid": "2460937040", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.", "ref_function": ["background", "background", "background", "background", "background", "background", "objective", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["It turns out that the most successful paradigm to construct the real-world adversarial examples is an Expectation Over Transformation (EOT) algorithm @cite_28 .", "This approach takes into account that in the real world the object usually undergoes a set of transformations (scaling, jittering, brightness and contrast changes, etc).", "The task is to find an adversarial example which is robust under this set of transformations @math and can be formulated as follows:"], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past:  about objective", "Reference to single investigations in the past:  about objective"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_28": {"mid": "2736899637", "abstract": "Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.", "ref_function": ["background", "method", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["Another work with the usage of @math -limited attacks proposes to attack facial recognition neural nets with the adversarial eyeglasses @cite_21 .", "The authors propose a method to print adversarial perturbation on the eyeglasses frame with the help of Total Variation (TV) loss and non-printability score (NPS).", "TV loss is designed to make the image more smooth.", "Thus it makes an attack more stable for different image interpolation methods on the devices and makes it more inconspicuousness for human.", "NPS is designed to deal with the difference in digital RGB-values and the ability of real printers to reproduce these values."], "label": ["Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about result", "Reference to single investigations in the past: about method"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_21": {"mid": "2535873859", "abstract": "Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.", "ref_function": ["background", "background", "objective", "objective", "method", "method", "result"], "cite_purpose": ["background"]}}}
{"sentences": ["In general, most of the subsequent works for the real-world attack use the concepts of @math -limited perturbation, EOT, TV loss, and NPS.", "Let us briefly list them.", "In @cite_40 the authors construct the physical attack for the traffic sign recognition model using EOT and NPS for making either adversarial posters (attacking the whole traffic sign area) or adversarial stickers (black and white stickers on the real traffic sign).", "The works of @cite_1 @cite_15 use some form of EOT to attack traffic sign recognition model too."], "label": ["General descriptions of the topic", "Other functional sentences", "Reference to single investigations in the past: about method", "Reference to single investigations in the past: about method"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_40": {"mid": "2759471388", "abstract": "Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100 of the images obtained in lab settings, and in 84.8 of the captured video frames obtained on a moving vehicle(field test) for the target classifier.", "ref_function": ["background", "background", "method", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_1": {"mid": "2783882201", "abstract": "We propose a new real-world attack against the computer vision based systems of autonomous vehicles (AVs). Our novel Sign Embedding attack exploits the concept of adversarial examples to modify innocuous signs and advertisements in the environment such that they are classified as the adversary's desired traffic sign with high confidence. Our attack greatly expands the scope of the threat posed to AVs since adversaries are no longer restricted to just modifying existing traffic signs as in previous work. Our attack pipeline generates adversarial samples which are robust to the environmental conditions and noisy image transformations present in the physical world. We ensure this by including a variety of possible image transformations in the optimization problem used to generate adversarial samples. We verify the robustness of the adversarial samples by printing them out and carrying out drive-by tests simulating the conditions under which image capture would occur in a real-world scenario. We experimented with physical attack samples for different distances, lighting conditions, and camera angles. In addition, extensive evaluations were carried out in the virtual setting for a variety of image transformations. The adversarial samples generated using our method have adversarial success rates in excess of 95 in the physical as well as virtual settings.", "ref_function": ["background", "background", "method", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_15": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}}}
{"sentences": ["A number of works are devoted to adversarial attacks on traffic sign detectors in the real world.", "One of the first works @cite_22 proposes an adversarial attack on Faster R-CNN @cite_35 stop sign detector using a sort of EOT (handcrafted estimation of a viewing map).", "Several works used EOT, NPS, and TV loss to attack Faster R-CNN, YOLOv2 @cite_42 based traffic sign recognition models @cite_19 @cite_45 @cite_8 ."], "label": ["General descriptions of the topic", "Reference to single investigations in the past: about method", "General reference to previous research or scholarship: approaches taken"], "target_paper": "In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.", "reference": {"@cite_35": {"mid": "2953106684", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "ref_function": ["background", "background", "objective", "method", "method", "method", "method", "result", "result"], "cite_purpose": ["background"]}, "@cite_22": {"mid": "2775467454", "abstract": "An adversarial example is an example that has been adjusted to produce a wrong label when presented to a system at test time. To date, adversarial example constructions have been demonstrated for classifiers, but not for detectors. If adversarial examples that could fool a detector exist, they could be used to (for example) maliciously create security hazards on roads populated with smart vehicles. In this paper, we demonstrate a construction that successfully fools two standard detectors, Faster RCNN and YOLO. The existence of such examples is surprising, as attacking a classifier is very different from attacking a detector, and that the structure of detectors - which must search for their own bounding box, and which cannot estimate that box very accurately - makes it quite likely that adversarial patterns are strongly disrupted. We show that our construction produces adversarial examples that generalize well across sequences digitally, even though large perturbations are needed. We also show that our construction yields physical objects that are adversarial.", "ref_function": ["background", "background", "background", "method", "method", "method", "result"], "cite_purpose": ["background"]}, "@cite_8": {"mid": "", "abstract": "", "ref_function": [], "cite_purpose": ["background"]}, "@cite_42": {"mid": "2951433694", "abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.", "ref_function": ["background", "method", "method", "method", "method", "method", "background", "background", "method", "result", "result", "result", "result"], "cite_purpose": ["background"]}, "@cite_19": {"mid": "2890883923", "abstract": "Given the ability to directly manipulate image pixels in the digital input space, an adversary can easily generate imperceptible perturbations to fool a Deep Neural Network (DNN) image classifier, as demonstrated in prior work. In this work, we propose ShapeShifter, an attack that tackles the more challenging problem of crafting physical adversarial perturbations to fool image-based object detectors like Faster R-CNN. Attacking an object detector is more difficult than attacking an image classifier, as it needs to mislead the classification results in multiple bounding boxes with different scales. Extending the digital attack to the physical world adds another layer of difficulty, because it requires the perturbation to be robust enough to survive real-world distortions due to different viewing distances and angles, lighting conditions, and camera limitations. We show that the Expectation over Transformation technique, which was originally proposed to enhance the robustness of adversarial perturbations in image classification, can be successfully adapted to the object detection setting. ShapeShifter can generate adversarially perturbed stop signs that are consistently mis-detected by Faster R-CNN as other objects, posing a potential threat to autonomous vehicles and other safety-critical computer vision systems. Code related to this paper is available at: https: github.com shangtse robust-physical-attack.", "ref_function": ["background", "objective", "background", "method", "method", "method", "other"], "cite_purpose": ["background"]}, "@cite_45": {"mid": "2778115935", "abstract": "Deep learning has proven to be a powerful tool for computer vision and has seen widespread adoption for numerous tasks. However, deep learning algorithms are known to be vulnerable to adversarial examples. These adversarial inputs are created such that, when provided to a deep learning algorithm, they are very likely to be mislabeled. This can be problematic when deep learning is used to assist in safety critical decisions. Recent research has shown that classifiers can be attacked by physical adversarial examples under various physical conditions. Given the fact that state-of-the-art objection detection algorithms are harder to be fooled by the same set of adversarial examples, here we show that these detectors can also be attacked by physical adversarial examples. In this note, we briefly show both static and dynamic test results. We design an algorithm that produces physical adversarial inputs, which can fool the YOLO object detector and can also attack Faster-RCNN with relatively high success rate based on transferability. Furthermore, our algorithm can compress the size of the adversarial inputs to stickers that, when attached to the targeted object, result in the detector either mislabeling or not detecting the object a high percentage of the time. This note provides a small set of results. Our upcoming paper will contain a thorough evaluation on other object detectors, and will present the algorithm.", "ref_function": ["background", "background", "background", "background", "result", "background", "background", "method", "method", "result", "result"], "cite_purpose": ["background"]}}}
